{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, cifar_noniid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# from sympy import * \n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 40  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep=1 #\"the number of local epochs: E\"\n",
    "    local_bs=100 #\"local batch size: B\"\n",
    "    bs=100 #\"test batch size\"\n",
    "    lr=0.03 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    weight_decay = 5e-4\n",
    "    opt = 'SGD' #'ADAM'\n",
    "    loss = 'Cross'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# load dataset and split users\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "\n",
    "dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "\n",
    "img_size = dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40700\n",
      "tensor(7) tensor(8) tensor(8)\n"
     ]
    }
   ],
   "source": [
    "print(dict_users[0][0])\n",
    "\n",
    "print(dataset_train.train_labels[dict_users[0][0]],dataset_train.train_labels[dict_users[0][797]],dataset_train.train_labels[dict_users[0][1499]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62346\n"
     ]
    }
   ],
   "source": [
    "from models.Nets import *\n",
    "from utils.functions import *\n",
    "\n",
    "net_glob = CNNMnist3(args)\n",
    "net_glob = net_glob.cuda()\n",
    "\n",
    "net_glob.train()\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "d = tensor_dim(w_glob)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate = 0.03\n",
      "\n",
      "CNNMnist2(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "[[36.20980613]] [[35.66808174]] [[73.155904]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 3.1105 \n",
      "Accuracy: 958/10000 (9.58%)\n",
      "\n",
      "Round   0, Train average loss 0.471 Test accuracy 9.580\n",
      "[[36.31609106]] [[35.71383303]] [[73.38286083]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.3948 \n",
      "Accuracy: 1707/10000 (17.07%)\n",
      "\n",
      "Round   1, Train average loss 0.731 Test accuracy 17.070\n",
      "[[36.45059104]] [[35.57830136]] [[72.93069868]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.3381 \n",
      "Accuracy: 1809/10000 (18.09%)\n",
      "\n",
      "Round   2, Train average loss 1.008 Test accuracy 18.090\n",
      "[[36.45453752]] [[35.63510922]] [[73.01970622]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.2930 \n",
      "Accuracy: 1654/10000 (16.54%)\n",
      "\n",
      "Round   3, Train average loss 1.526 Test accuracy 16.540\n",
      "[[36.32020343]] [[35.50079901]] [[71.80600178]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.3067 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Round   4, Train average loss 1.748 Test accuracy 9.740\n",
      "[[36.33143607]] [[35.34311362]] [[71.4873213]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.2460 \n",
      "Accuracy: 1859/10000 (18.59%)\n",
      "\n",
      "Round   5, Train average loss 1.816 Test accuracy 18.590\n",
      "[[36.30307885]] [[35.49027132]] [[71.59044209]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.2684 \n",
      "Accuracy: 1538/10000 (15.38%)\n",
      "\n",
      "Round   6, Train average loss 1.675 Test accuracy 15.380\n",
      "[[36.43214457]] [[35.6694038]] [[71.91218019]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.9318 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Round   7, Train average loss 1.446 Test accuracy 10.280\n",
      "[[36.46939899]] [[35.81004833]] [[71.99201952]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.9517 \n",
      "Accuracy: 982/10000 (9.82%)\n",
      "\n",
      "Round   8, Train average loss 0.900 Test accuracy 9.820\n",
      "[[36.63562343]] [[36.4009475]] [[72.71271462]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.1853 \n",
      "Accuracy: 3576/10000 (35.76%)\n",
      "\n",
      "Round   9, Train average loss 0.831 Test accuracy 35.760\n",
      "[[36.57564186]] [[36.75370962]] [[72.92127242]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.0982 \n",
      "Accuracy: 3873/10000 (38.73%)\n",
      "\n",
      "Round  10, Train average loss 0.718 Test accuracy 38.730\n",
      "[[36.55079278]] [[37.36359322]] [[73.32447982]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.1105 \n",
      "Accuracy: 2603/10000 (26.03%)\n",
      "\n",
      "Round  11, Train average loss 0.887 Test accuracy 26.030\n",
      "[[36.79958732]] [[37.90461166]] [[74.23749585]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 2.1845 \n",
      "Accuracy: 3378/10000 (33.78%)\n",
      "\n",
      "Round  12, Train average loss 0.894 Test accuracy 33.780\n",
      "[[36.91485808]] [[38.39517243]] [[74.88542769]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.9290 \n",
      "Accuracy: 3112/10000 (31.12%)\n",
      "\n",
      "Round  13, Train average loss 0.891 Test accuracy 31.120\n",
      "[[37.02356704]] [[38.71942156]] [[75.19188447]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.7378 \n",
      "Accuracy: 4637/10000 (46.37%)\n",
      "\n",
      "Round  14, Train average loss 0.565 Test accuracy 46.370\n",
      "[[37.29163029]] [[39.27399221]] [[76.14807397]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.9602 \n",
      "Accuracy: 2198/10000 (21.98%)\n",
      "\n",
      "Round  15, Train average loss 0.413 Test accuracy 21.980\n",
      "[[37.41798005]] [[39.68169929]] [[76.7539431]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 3.1447 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Round  16, Train average loss 0.408 Test accuracy 9.740\n",
      "[[37.55903523]] [[40.00866494]] [[77.11050516]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.9220 \n",
      "Accuracy: 3906/10000 (39.06%)\n",
      "\n",
      "Round  17, Train average loss 0.553 Test accuracy 39.060\n",
      "[[37.56794331]] [[40.28149909]] [[77.07629666]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.6831 \n",
      "Accuracy: 5801/10000 (58.01%)\n",
      "\n",
      "Round  18, Train average loss 0.562 Test accuracy 58.010\n",
      "[[37.82038129]] [[40.48626796]] [[77.71199944]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.5531 \n",
      "Accuracy: 5362/10000 (53.62%)\n",
      "\n",
      "Round  19, Train average loss 0.560 Test accuracy 53.620\n",
      "[[37.95092921]] [[40.77348959]] [[78.0367589]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.7150 \n",
      "Accuracy: 4304/10000 (43.04%)\n",
      "\n",
      "Round  20, Train average loss 0.405 Test accuracy 43.040\n",
      "[[38.35538939]] [[41.06479319]] [[78.84481894]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.3834 \n",
      "Accuracy: 6050/10000 (60.50%)\n",
      "\n",
      "Round  21, Train average loss 0.411 Test accuracy 60.500\n",
      "[[38.4675139]] [[41.29349322]] [[79.14561575]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.5468 \n",
      "Accuracy: 4533/10000 (45.33%)\n",
      "\n",
      "Round  22, Train average loss 0.341 Test accuracy 45.330\n",
      "[[38.70825336]] [[41.66503649]] [[79.78592513]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.4256 \n",
      "Accuracy: 5699/10000 (56.99%)\n",
      "\n",
      "Round  23, Train average loss 0.351 Test accuracy 56.990\n",
      "[[38.98131844]] [[41.97259388]] [[80.3727067]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.2882 \n",
      "Accuracy: 6002/10000 (60.02%)\n",
      "\n",
      "Round  24, Train average loss 0.346 Test accuracy 60.020\n",
      "[[39.18344199]] [[42.15271976]] [[80.86246369]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.2851 \n",
      "Accuracy: 5984/10000 (59.84%)\n",
      "\n",
      "Round  25, Train average loss 0.337 Test accuracy 59.840\n",
      "[[39.37320978]] [[42.29908615]] [[81.19384869]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.9417 \n",
      "Accuracy: 4273/10000 (42.73%)\n",
      "\n",
      "Round  26, Train average loss 0.320 Test accuracy 42.730\n",
      "[[39.55695774]] [[42.47631707]] [[81.51619822]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.1988 \n",
      "Accuracy: 6629/10000 (66.29%)\n",
      "\n",
      "Round  27, Train average loss 0.377 Test accuracy 66.290\n",
      "[[39.4412715]] [[42.67154672]] [[81.47403995]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8792 \n",
      "Accuracy: 7641/10000 (76.41%)\n",
      "\n",
      "Round  28, Train average loss 0.331 Test accuracy 76.410\n",
      "[[39.85513847]] [[42.84880096]] [[82.26950759]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9914 \n",
      "Accuracy: 6473/10000 (64.73%)\n",
      "\n",
      "Round  29, Train average loss 0.291 Test accuracy 64.730\n",
      "[[39.99903631]] [[43.08107595]] [[82.69402916]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9953 \n",
      "Accuracy: 6959/10000 (69.59%)\n",
      "\n",
      "Round  30, Train average loss 0.302 Test accuracy 69.590\n",
      "[[40.05643077]] [[43.13680198]] [[82.67659567]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8489 \n",
      "Accuracy: 7385/10000 (73.85%)\n",
      "\n",
      "Round  31, Train average loss 0.299 Test accuracy 73.850\n",
      "[[40.24775784]] [[43.26829259]] [[83.04626351]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7671 \n",
      "Accuracy: 7853/10000 (78.53%)\n",
      "\n",
      "Round  32, Train average loss 0.288 Test accuracy 78.530\n",
      "[[40.40006938]] [[43.42181785]] [[83.42675656]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9106 \n",
      "Accuracy: 7276/10000 (72.76%)\n",
      "\n",
      "Round  33, Train average loss 0.282 Test accuracy 72.760\n",
      "[[40.53780663]] [[43.57222212]] [[83.75332489]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8500 \n",
      "Accuracy: 7015/10000 (70.15%)\n",
      "\n",
      "Round  34, Train average loss 0.290 Test accuracy 70.150\n",
      "[[40.44697394]] [[43.68142037]] [[83.71577497]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8203 \n",
      "Accuracy: 7657/10000 (76.57%)\n",
      "\n",
      "Round  35, Train average loss 0.289 Test accuracy 76.570\n",
      "[[40.68196589]] [[43.75924208]] [[84.03983516]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8096 \n",
      "Accuracy: 7266/10000 (72.66%)\n",
      "\n",
      "Round  36, Train average loss 0.285 Test accuracy 72.660\n",
      "[[40.74809459]] [[43.8380491]] [[84.21378015]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6882 \n",
      "Accuracy: 7815/10000 (78.15%)\n",
      "\n",
      "Round  37, Train average loss 0.282 Test accuracy 78.150\n",
      "[[40.7817095]] [[43.87434315]] [[84.20415872]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7826 \n",
      "Accuracy: 7157/10000 (71.57%)\n",
      "\n",
      "Round  38, Train average loss 0.278 Test accuracy 71.570\n",
      "[[40.84910073]] [[43.92250375]] [[84.32667222]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6815 \n",
      "Accuracy: 7980/10000 (79.80%)\n",
      "\n",
      "Round  39, Train average loss 0.284 Test accuracy 79.800\n",
      "[[40.92895653]] [[44.05812792]] [[84.5578087]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6915 \n",
      "Accuracy: 7831/10000 (78.31%)\n",
      "\n",
      "Round  40, Train average loss 0.275 Test accuracy 78.310\n",
      "[[41.17960196]] [[44.15737269]] [[84.93496282]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6009 \n",
      "Accuracy: 8161/10000 (81.61%)\n",
      "\n",
      "Round  41, Train average loss 0.275 Test accuracy 81.610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41.26288557]] [[44.25636817]] [[85.15895193]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6134 \n",
      "Accuracy: 8070/10000 (80.70%)\n",
      "\n",
      "Round  42, Train average loss 0.270 Test accuracy 80.700\n",
      "[[41.36455894]] [[44.31604496]] [[85.33890804]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6461 \n",
      "Accuracy: 7934/10000 (79.34%)\n",
      "\n",
      "Round  43, Train average loss 0.269 Test accuracy 79.340\n",
      "[[41.3440692]] [[44.37808482]] [[85.40466334]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7246 \n",
      "Accuracy: 7336/10000 (73.36%)\n",
      "\n",
      "Round  44, Train average loss 0.272 Test accuracy 73.360\n",
      "[[41.466522]] [[44.44315094]] [[85.59609062]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6006 \n",
      "Accuracy: 8031/10000 (80.31%)\n",
      "\n",
      "Round  45, Train average loss 0.277 Test accuracy 80.310\n",
      "[[41.44857792]] [[44.55532386]] [[85.68293843]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6567 \n",
      "Accuracy: 7769/10000 (77.69%)\n",
      "\n",
      "Round  46, Train average loss 0.268 Test accuracy 77.690\n",
      "[[41.48753627]] [[44.65549208]] [[85.83880471]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6731 \n",
      "Accuracy: 7965/10000 (79.65%)\n",
      "\n",
      "Round  47, Train average loss 0.272 Test accuracy 79.650\n",
      "[[41.47495488]] [[44.72049897]] [[85.8661603]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7676 \n",
      "Accuracy: 7165/10000 (71.65%)\n",
      "\n",
      "Round  48, Train average loss 0.272 Test accuracy 71.650\n",
      "[[41.54079391]] [[44.78960052]] [[85.99260949]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5976 \n",
      "Accuracy: 8150/10000 (81.50%)\n",
      "\n",
      "Round  49, Train average loss 0.283 Test accuracy 81.500\n",
      "[[41.69884125]] [[44.84841311]] [[86.22521439]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6522 \n",
      "Accuracy: 7802/10000 (78.02%)\n",
      "\n",
      "Round  50, Train average loss 0.267 Test accuracy 78.020\n",
      "[[41.84252192]] [[44.91836381]] [[86.43688163]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6713 \n",
      "Accuracy: 7664/10000 (76.64%)\n",
      "\n",
      "Round  51, Train average loss 0.271 Test accuracy 76.640\n",
      "[[41.91354068]] [[45.01997705]] [[86.63630134]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6892 \n",
      "Accuracy: 7577/10000 (75.77%)\n",
      "\n",
      "Round  52, Train average loss 0.271 Test accuracy 75.770\n",
      "[[41.95039367]] [[45.06719677]] [[86.67645792]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8012 \n",
      "Accuracy: 7120/10000 (71.20%)\n",
      "\n",
      "Round  53, Train average loss 0.273 Test accuracy 71.200\n",
      "[[42.01761102]] [[45.11013012]] [[86.79254934]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7306 \n",
      "Accuracy: 7359/10000 (73.59%)\n",
      "\n",
      "Round  54, Train average loss 0.279 Test accuracy 73.590\n",
      "[[42.05049732]] [[45.18006464]] [[86.90386927]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6919 \n",
      "Accuracy: 7699/10000 (76.99%)\n",
      "\n",
      "Round  55, Train average loss 0.280 Test accuracy 76.990\n",
      "[[42.1099382]] [[45.2601655]] [[86.99543793]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5078 \n",
      "Accuracy: 8335/10000 (83.35%)\n",
      "\n",
      "Round  56, Train average loss 0.275 Test accuracy 83.350\n",
      "[[42.30789711]] [[45.31963539]] [[87.29803298]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5193 \n",
      "Accuracy: 8234/10000 (82.34%)\n",
      "\n",
      "Round  57, Train average loss 0.263 Test accuracy 82.340\n",
      "[[42.3604014]] [[45.38931777]] [[87.43932211]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4808 \n",
      "Accuracy: 8380/10000 (83.80%)\n",
      "\n",
      "Round  58, Train average loss 0.262 Test accuracy 83.800\n",
      "[[42.40122453]] [[45.44824112]] [[87.5476253]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7302 \n",
      "Accuracy: 7249/10000 (72.49%)\n",
      "\n",
      "Round  59, Train average loss 0.260 Test accuracy 72.490\n",
      "[[42.49117665]] [[45.49450434]] [[87.68249619]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5924 \n",
      "Accuracy: 7937/10000 (79.37%)\n",
      "\n",
      "Round  60, Train average loss 0.281 Test accuracy 79.370\n",
      "[[42.50599587]] [[45.53900376]] [[87.77289308]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5060 \n",
      "Accuracy: 8252/10000 (82.52%)\n",
      "\n",
      "Round  61, Train average loss 0.268 Test accuracy 82.520\n",
      "[[42.50729844]] [[45.61200744]] [[87.82335527]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5007 \n",
      "Accuracy: 8345/10000 (83.45%)\n",
      "\n",
      "Round  62, Train average loss 0.261 Test accuracy 83.450\n",
      "[[42.57453514]] [[45.68517126]] [[88.01152693]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5810 \n",
      "Accuracy: 7925/10000 (79.25%)\n",
      "\n",
      "Round  63, Train average loss 0.259 Test accuracy 79.250\n",
      "[[42.62760836]] [[45.72479382]] [[88.0727348]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5270 \n",
      "Accuracy: 8191/10000 (81.91%)\n",
      "\n",
      "Round  64, Train average loss 0.266 Test accuracy 81.910\n",
      "[[42.66988063]] [[45.80476234]] [[88.20884963]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5181 \n",
      "Accuracy: 8258/10000 (82.58%)\n",
      "\n",
      "Round  65, Train average loss 0.262 Test accuracy 82.580\n",
      "[[42.71603597]] [[45.8648981]] [[88.31970773]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4961 \n",
      "Accuracy: 8336/10000 (83.36%)\n",
      "\n",
      "Round  66, Train average loss 0.257 Test accuracy 83.360\n",
      "[[42.80261365]] [[45.89945825]] [[88.41896373]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4460 \n",
      "Accuracy: 8432/10000 (84.32%)\n",
      "\n",
      "Round  67, Train average loss 0.257 Test accuracy 84.320\n",
      "[[42.86322746]] [[45.9736707]] [[88.55879025]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5299 \n",
      "Accuracy: 8067/10000 (80.67%)\n",
      "\n",
      "Round  68, Train average loss 0.254 Test accuracy 80.670\n",
      "[[42.92196004]] [[46.0215462]] [[88.68508706]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4901 \n",
      "Accuracy: 8326/10000 (83.26%)\n",
      "\n",
      "Round  69, Train average loss 0.261 Test accuracy 83.260\n",
      "[[42.85928494]] [[46.08648065]] [[88.66080545]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5125 \n",
      "Accuracy: 8229/10000 (82.29%)\n",
      "\n",
      "Round  70, Train average loss 0.257 Test accuracy 82.290\n",
      "[[42.89032148]] [[46.13715728]] [[88.77929888]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6030 \n",
      "Accuracy: 7866/10000 (78.66%)\n",
      "\n",
      "Round  71, Train average loss 0.259 Test accuracy 78.660\n",
      "[[42.93738772]] [[46.17643717]] [[88.83909612]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5667 \n",
      "Accuracy: 7994/10000 (79.94%)\n",
      "\n",
      "Round  72, Train average loss 0.264 Test accuracy 79.940\n",
      "[[42.98764129]] [[46.23907185]] [[88.93278354]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5087 \n",
      "Accuracy: 8210/10000 (82.10%)\n",
      "\n",
      "Round  73, Train average loss 0.263 Test accuracy 82.100\n",
      "[[43.04176082]] [[46.29829102]] [[89.05772346]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5191 \n",
      "Accuracy: 8215/10000 (82.15%)\n",
      "\n",
      "Round  74, Train average loss 0.259 Test accuracy 82.150\n",
      "[[43.07850665]] [[46.36003656]] [[89.16246756]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5128 \n",
      "Accuracy: 8273/10000 (82.73%)\n",
      "\n",
      "Round  75, Train average loss 0.258 Test accuracy 82.730\n",
      "[[43.07363931]] [[46.41632096]] [[89.2114372]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5089 \n",
      "Accuracy: 8190/10000 (81.90%)\n",
      "\n",
      "Round  76, Train average loss 0.258 Test accuracy 81.900\n",
      "[[43.12628648]] [[46.45106862]] [[89.30781695]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5261 \n",
      "Accuracy: 8195/10000 (81.95%)\n",
      "\n",
      "Round  77, Train average loss 0.259 Test accuracy 81.950\n",
      "[[43.17359178]] [[46.49267926]] [[89.3946526]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5160 \n",
      "Accuracy: 8091/10000 (80.91%)\n",
      "\n",
      "Round  78, Train average loss 0.259 Test accuracy 80.910\n",
      "[[43.23974162]] [[46.53621952]] [[89.53531966]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5013 \n",
      "Accuracy: 8172/10000 (81.72%)\n",
      "\n",
      "Round  79, Train average loss 0.257 Test accuracy 81.720\n",
      "[[43.25013178]] [[46.57759026]] [[89.5926618]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4951 \n",
      "Accuracy: 8215/10000 (82.15%)\n",
      "\n",
      "Round  80, Train average loss 0.256 Test accuracy 82.150\n",
      "[[43.24650447]] [[46.62535405]] [[89.62396146]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4414 \n",
      "Accuracy: 8457/10000 (84.57%)\n",
      "\n",
      "Round  81, Train average loss 0.256 Test accuracy 84.570\n",
      "[[43.24744534]] [[46.65768702]] [[89.63966946]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4383 \n",
      "Accuracy: 8486/10000 (84.86%)\n",
      "\n",
      "Round  82, Train average loss 0.253 Test accuracy 84.860\n",
      "[[43.29937935]] [[46.70577308]] [[89.74070581]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4021 \n",
      "Accuracy: 8582/10000 (85.82%)\n",
      "\n",
      "Round  83, Train average loss 0.252 Test accuracy 85.820\n",
      "[[43.31521639]] [[46.76730864]] [[89.8236063]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4718 \n",
      "Accuracy: 8333/10000 (83.33%)\n",
      "\n",
      "Round  84, Train average loss 0.250 Test accuracy 83.330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43.31620631]] [[46.80976324]] [[89.86279075]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4185 \n",
      "Accuracy: 8554/10000 (85.54%)\n",
      "\n",
      "Round  85, Train average loss 0.254 Test accuracy 85.540\n",
      "[[43.3371378]] [[46.88053749]] [[89.94833898]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4350 \n",
      "Accuracy: 8451/10000 (84.51%)\n",
      "\n",
      "Round  86, Train average loss 0.250 Test accuracy 84.510\n",
      "[[43.39366675]] [[46.91969594]] [[90.0597274]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4238 \n",
      "Accuracy: 8520/10000 (85.20%)\n",
      "\n",
      "Round  87, Train average loss 0.252 Test accuracy 85.200\n",
      "[[43.45841486]] [[46.98132163]] [[90.1967039]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4781 \n",
      "Accuracy: 8355/10000 (83.55%)\n",
      "\n",
      "Round  88, Train average loss 0.250 Test accuracy 83.550\n",
      "[[43.46495648]] [[47.0436987]] [[90.27608984]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3897 \n",
      "Accuracy: 8614/10000 (86.14%)\n",
      "\n",
      "Round  89, Train average loss 0.254 Test accuracy 86.140\n",
      "[[43.48129398]] [[47.08594278]] [[90.37127479]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4100 \n",
      "Accuracy: 8509/10000 (85.09%)\n",
      "\n",
      "Round  90, Train average loss 0.248 Test accuracy 85.090\n",
      "[[43.5183804]] [[47.11618529]] [[90.45775292]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4179 \n",
      "Accuracy: 8531/10000 (85.31%)\n",
      "\n",
      "Round  91, Train average loss 0.249 Test accuracy 85.310\n",
      "[[43.4921644]] [[47.17047139]] [[90.46403996]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4500 \n",
      "Accuracy: 8368/10000 (83.68%)\n",
      "\n",
      "Round  92, Train average loss 0.250 Test accuracy 83.680\n",
      "[[43.5111105]] [[47.21010035]] [[90.53793259]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4381 \n",
      "Accuracy: 8412/10000 (84.12%)\n",
      "\n",
      "Round  93, Train average loss 0.251 Test accuracy 84.120\n",
      "[[43.54971744]] [[47.23945126]] [[90.6102052]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4897 \n",
      "Accuracy: 8196/10000 (81.96%)\n",
      "\n",
      "Round  94, Train average loss 0.250 Test accuracy 81.960\n",
      "[[43.53732223]] [[47.27042324]] [[90.6486491]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5418 \n",
      "Accuracy: 8000/10000 (80.00%)\n",
      "\n",
      "Round  95, Train average loss 0.253 Test accuracy 80.000\n",
      "[[43.59002054]] [[47.31525901]] [[90.73182421]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6872 \n",
      "Accuracy: 7574/10000 (75.74%)\n",
      "\n",
      "Round  96, Train average loss 0.264 Test accuracy 75.740\n",
      "[[43.65853983]] [[47.34152391]] [[90.79637931]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8725 \n",
      "Accuracy: 7288/10000 (72.88%)\n",
      "\n",
      "Round  97, Train average loss 0.269 Test accuracy 72.880\n",
      "[[43.85338163]] [[47.35630561]] [[90.91110897]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7628 \n",
      "Accuracy: 7606/10000 (76.06%)\n",
      "\n",
      "Round  98, Train average loss 0.289 Test accuracy 76.060\n",
      "[[43.87121063]] [[47.37141596]] [[90.90987481]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7319 \n",
      "Accuracy: 7770/10000 (77.70%)\n",
      "\n",
      "Round  99, Train average loss 0.280 Test accuracy 77.700\n",
      "[[43.90968938]] [[47.41177289]] [[90.98492567]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5001 \n",
      "Accuracy: 8292/10000 (82.92%)\n",
      "\n",
      "Round 100, Train average loss 0.277 Test accuracy 82.920\n",
      "[[43.84313628]] [[47.44940453]] [[90.94766166]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4351 \n",
      "Accuracy: 8542/10000 (85.42%)\n",
      "\n",
      "Round 101, Train average loss 0.258 Test accuracy 85.420\n",
      "[[43.91430012]] [[47.47404879]] [[91.04401296]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4012 \n",
      "Accuracy: 8595/10000 (85.95%)\n",
      "\n",
      "Round 102, Train average loss 0.253 Test accuracy 85.950\n",
      "[[43.98403727]] [[47.51882634]] [[91.16026814]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3934 \n",
      "Accuracy: 8609/10000 (86.09%)\n",
      "\n",
      "Round 103, Train average loss 0.251 Test accuracy 86.090\n",
      "[[44.06883841]] [[47.5485001]] [[91.29835693]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4273 \n",
      "Accuracy: 8469/10000 (84.69%)\n",
      "\n",
      "Round 104, Train average loss 0.250 Test accuracy 84.690\n",
      "[[44.10587037]] [[47.58406653]] [[91.39158811]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4154 \n",
      "Accuracy: 8542/10000 (85.42%)\n",
      "\n",
      "Round 105, Train average loss 0.252 Test accuracy 85.420\n",
      "[[44.07831992]] [[47.62766578]] [[91.42394162]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4481 \n",
      "Accuracy: 8412/10000 (84.12%)\n",
      "\n",
      "Round 106, Train average loss 0.250 Test accuracy 84.120\n",
      "[[44.08267895]] [[47.65441134]] [[91.45001033]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4070 \n",
      "Accuracy: 8529/10000 (85.29%)\n",
      "\n",
      "Round 107, Train average loss 0.253 Test accuracy 85.290\n",
      "[[44.14531349]] [[47.70624057]] [[91.57482853]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3942 \n",
      "Accuracy: 8584/10000 (85.84%)\n",
      "\n",
      "Round 108, Train average loss 0.250 Test accuracy 85.840\n",
      "[[44.16918795]] [[47.74291103]] [[91.66649972]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4175 \n",
      "Accuracy: 8508/10000 (85.08%)\n",
      "\n",
      "Round 109, Train average loss 0.249 Test accuracy 85.080\n",
      "[[44.16523808]] [[47.79458838]] [[91.70736845]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4184 \n",
      "Accuracy: 8440/10000 (84.40%)\n",
      "\n",
      "Round 110, Train average loss 0.251 Test accuracy 84.400\n",
      "[[44.21801031]] [[47.82024431]] [[91.7857892]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4409 \n",
      "Accuracy: 8356/10000 (83.56%)\n",
      "\n",
      "Round 111, Train average loss 0.249 Test accuracy 83.560\n",
      "[[44.24153459]] [[47.84887217]] [[91.84274267]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3795 \n",
      "Accuracy: 8624/10000 (86.24%)\n",
      "\n",
      "Round 112, Train average loss 0.250 Test accuracy 86.240\n",
      "[[44.26624366]] [[47.88495542]] [[91.88011415]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3950 \n",
      "Accuracy: 8589/10000 (85.89%)\n",
      "\n",
      "Round 113, Train average loss 0.247 Test accuracy 85.890\n",
      "[[44.26223392]] [[47.90864905]] [[91.92429974]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3893 \n",
      "Accuracy: 8610/10000 (86.10%)\n",
      "\n",
      "Round 114, Train average loss 0.248 Test accuracy 86.100\n",
      "[[44.27459908]] [[47.95628696]] [[92.01666144]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4167 \n",
      "Accuracy: 8472/10000 (84.72%)\n",
      "\n",
      "Round 115, Train average loss 0.247 Test accuracy 84.720\n",
      "[[44.24882611]] [[48.00754581]] [[92.02938655]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3702 \n",
      "Accuracy: 8657/10000 (86.57%)\n",
      "\n",
      "Round 116, Train average loss 0.249 Test accuracy 86.570\n",
      "[[44.27875071]] [[48.05080784]] [[92.11120877]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3881 \n",
      "Accuracy: 8594/10000 (85.94%)\n",
      "\n",
      "Round 117, Train average loss 0.245 Test accuracy 85.940\n",
      "[[44.29609058]] [[48.08702935]] [[92.15563014]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4540 \n",
      "Accuracy: 8347/10000 (83.47%)\n",
      "\n",
      "Round 118, Train average loss 0.247 Test accuracy 83.470\n",
      "[[44.27410077]] [[48.11512485]] [[92.15814588]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4257 \n",
      "Accuracy: 8500/10000 (85.00%)\n",
      "\n",
      "Round 119, Train average loss 0.253 Test accuracy 85.000\n",
      "[[44.30618756]] [[48.13066068]] [[92.19655642]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4324 \n",
      "Accuracy: 8454/10000 (84.54%)\n",
      "\n",
      "Round 120, Train average loss 0.249 Test accuracy 84.540\n",
      "[[44.35500401]] [[48.15700693]] [[92.2970451]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3898 \n",
      "Accuracy: 8580/10000 (85.80%)\n",
      "\n",
      "Round 121, Train average loss 0.249 Test accuracy 85.800\n",
      "[[44.37469806]] [[48.1941104]] [[92.36465951]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4545 \n",
      "Accuracy: 8312/10000 (83.12%)\n",
      "\n",
      "Round 122, Train average loss 0.248 Test accuracy 83.120\n",
      "[[44.38315231]] [[48.21907474]] [[92.40448025]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4696 \n",
      "Accuracy: 8268/10000 (82.68%)\n",
      "\n",
      "Round 123, Train average loss 0.252 Test accuracy 82.680\n",
      "[[44.44314021]] [[48.24572112]] [[92.48774318]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4497 \n",
      "Accuracy: 8308/10000 (83.08%)\n",
      "\n",
      "Round 124, Train average loss 0.252 Test accuracy 83.080\n",
      "[[44.45576915]] [[48.29187407]] [[92.5510147]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5914 \n",
      "Accuracy: 7857/10000 (78.57%)\n",
      "\n",
      "Round 125, Train average loss 0.251 Test accuracy 78.570\n",
      "[[44.56179352]] [[48.32450958]] [[92.67688873]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4098 \n",
      "Accuracy: 8518/10000 (85.18%)\n",
      "\n",
      "Round 126, Train average loss 0.259 Test accuracy 85.180\n",
      "[[44.56070374]] [[48.35806677]] [[92.70191799]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3752 \n",
      "Accuracy: 8651/10000 (86.51%)\n",
      "\n",
      "Round 127, Train average loss 0.249 Test accuracy 86.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.58112968]] [[48.38100436]] [[92.72326283]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3457 \n",
      "Accuracy: 8724/10000 (87.24%)\n",
      "\n",
      "Round 128, Train average loss 0.247 Test accuracy 87.240\n",
      "[[44.66334771]] [[48.40519351]] [[92.84310222]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3792 \n",
      "Accuracy: 8604/10000 (86.04%)\n",
      "\n",
      "Round 129, Train average loss 0.245 Test accuracy 86.040\n",
      "[[44.69380019]] [[48.43414909]] [[92.93656189]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3669 \n",
      "Accuracy: 8647/10000 (86.47%)\n",
      "\n",
      "Round 130, Train average loss 0.246 Test accuracy 86.470\n",
      "[[44.72074382]] [[48.46112052]] [[92.98255616]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3751 \n",
      "Accuracy: 8634/10000 (86.34%)\n",
      "\n",
      "Round 131, Train average loss 0.245 Test accuracy 86.340\n",
      "[[44.66728563]] [[48.48087997]] [[92.95804107]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3679 \n",
      "Accuracy: 8620/10000 (86.20%)\n",
      "\n",
      "Round 132, Train average loss 0.246 Test accuracy 86.200\n",
      "[[44.69760666]] [[48.51144692]] [[93.02223734]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3531 \n",
      "Accuracy: 8700/10000 (87.00%)\n",
      "\n",
      "Round 133, Train average loss 0.245 Test accuracy 87.000\n",
      "[[44.66975507]] [[48.54206745]] [[93.02834185]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3741 \n",
      "Accuracy: 8616/10000 (86.16%)\n",
      "\n",
      "Round 134, Train average loss 0.244 Test accuracy 86.160\n",
      "[[44.68161444]] [[48.5552659]] [[93.04244586]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3764 \n",
      "Accuracy: 8620/10000 (86.20%)\n",
      "\n",
      "Round 135, Train average loss 0.245 Test accuracy 86.200\n",
      "[[44.68419624]] [[48.58485025]] [[93.0926853]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4427 \n",
      "Accuracy: 8324/10000 (83.24%)\n",
      "\n",
      "Round 136, Train average loss 0.245 Test accuracy 83.240\n",
      "[[44.72404721]] [[48.61570427]] [[93.19059162]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3944 \n",
      "Accuracy: 8507/10000 (85.07%)\n",
      "\n",
      "Round 137, Train average loss 0.248 Test accuracy 85.070\n",
      "[[44.74617784]] [[48.64647735]] [[93.23030601]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3626 \n",
      "Accuracy: 8657/10000 (86.57%)\n",
      "\n",
      "Round 138, Train average loss 0.247 Test accuracy 86.570\n",
      "[[44.73865114]] [[48.66775669]] [[93.24439255]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3632 \n",
      "Accuracy: 8680/10000 (86.80%)\n",
      "\n",
      "Round 139, Train average loss 0.244 Test accuracy 86.800\n",
      "[[44.68287697]] [[48.66263474]] [[93.17399676]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3740 \n",
      "Accuracy: 8639/10000 (86.39%)\n",
      "\n",
      "Round 140, Train average loss 0.244 Test accuracy 86.390\n",
      "[[44.70557246]] [[48.68184969]] [[93.20758566]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3610 \n",
      "Accuracy: 8658/10000 (86.58%)\n",
      "\n",
      "Round 141, Train average loss 0.245 Test accuracy 86.580\n",
      "[[44.75020806]] [[48.70841274]] [[93.298552]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3458 \n",
      "Accuracy: 8725/10000 (87.25%)\n",
      "\n",
      "Round 142, Train average loss 0.244 Test accuracy 87.250\n",
      "[[44.75230418]] [[48.73097632]] [[93.32671257]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3471 \n",
      "Accuracy: 8717/10000 (87.17%)\n",
      "\n",
      "Round 143, Train average loss 0.243 Test accuracy 87.170\n",
      "[[44.75875054]] [[48.74729968]] [[93.37828205]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3440 \n",
      "Accuracy: 8718/10000 (87.18%)\n",
      "\n",
      "Round 144, Train average loss 0.243 Test accuracy 87.180\n",
      "[[44.77101895]] [[48.77925959]] [[93.41619399]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3483 \n",
      "Accuracy: 8689/10000 (86.89%)\n",
      "\n",
      "Round 145, Train average loss 0.243 Test accuracy 86.890\n",
      "[[44.79672529]] [[48.78807994]] [[93.46408272]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3546 \n",
      "Accuracy: 8678/10000 (86.78%)\n",
      "\n",
      "Round 146, Train average loss 0.242 Test accuracy 86.780\n",
      "[[44.77185421]] [[48.81837484]] [[93.48512694]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3663 \n",
      "Accuracy: 8652/10000 (86.52%)\n",
      "\n",
      "Round 147, Train average loss 0.243 Test accuracy 86.520\n",
      "[[44.80251915]] [[48.84580552]] [[93.5574186]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3761 \n",
      "Accuracy: 8612/10000 (86.12%)\n",
      "\n",
      "Round 148, Train average loss 0.243 Test accuracy 86.120\n",
      "[[44.81334273]] [[48.87413631]] [[93.58255712]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3404 \n",
      "Accuracy: 8732/10000 (87.32%)\n",
      "\n",
      "Round 149, Train average loss 0.244 Test accuracy 87.320\n",
      "[[44.80929669]] [[48.88517269]] [[93.587512]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3752 \n",
      "Accuracy: 8611/10000 (86.11%)\n",
      "\n",
      "Round 150, Train average loss 0.242 Test accuracy 86.110\n",
      "[[44.82370348]] [[48.91072338]] [[93.628316]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3965 \n",
      "Accuracy: 8530/10000 (85.30%)\n",
      "\n",
      "Round 151, Train average loss 0.243 Test accuracy 85.300\n",
      "[[44.81329911]] [[48.92002602]] [[93.64459097]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5087 \n",
      "Accuracy: 8109/10000 (81.09%)\n",
      "\n",
      "Round 152, Train average loss 0.244 Test accuracy 81.090\n",
      "[[44.86290495]] [[48.95107811]] [[93.69940829]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6108 \n",
      "Accuracy: 7844/10000 (78.44%)\n",
      "\n",
      "Round 153, Train average loss 0.260 Test accuracy 78.440\n",
      "[[44.91361561]] [[48.97435848]] [[93.73549611]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5323 \n",
      "Accuracy: 8008/10000 (80.08%)\n",
      "\n",
      "Round 154, Train average loss 0.257 Test accuracy 80.080\n",
      "[[44.86823359]] [[48.98947493]] [[93.72553025]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3583 \n",
      "Accuracy: 8648/10000 (86.48%)\n",
      "\n",
      "Round 155, Train average loss 0.253 Test accuracy 86.480\n",
      "[[44.86295664]] [[49.02109984]] [[93.72173298]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3470 \n",
      "Accuracy: 8714/10000 (87.14%)\n",
      "\n",
      "Round 156, Train average loss 0.245 Test accuracy 87.140\n",
      "[[44.88615995]] [[49.0562302]] [[93.79501526]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3278 \n",
      "Accuracy: 8759/10000 (87.59%)\n",
      "\n",
      "Round 157, Train average loss 0.244 Test accuracy 87.590\n",
      "[[44.86722924]] [[49.06349366]] [[93.78328141]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3475 \n",
      "Accuracy: 8670/10000 (86.70%)\n",
      "\n",
      "Round 158, Train average loss 0.242 Test accuracy 86.700\n",
      "[[44.88954152]] [[49.09126445]] [[93.85124897]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3697 \n",
      "Accuracy: 8626/10000 (86.26%)\n",
      "\n",
      "Round 159, Train average loss 0.243 Test accuracy 86.260\n",
      "[[44.89929233]] [[49.09944514]] [[93.88016359]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3681 \n",
      "Accuracy: 8622/10000 (86.22%)\n",
      "\n",
      "Round 160, Train average loss 0.244 Test accuracy 86.220\n",
      "[[44.87876073]] [[49.12020641]] [[93.8949723]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3692 \n",
      "Accuracy: 8630/10000 (86.30%)\n",
      "\n",
      "Round 161, Train average loss 0.243 Test accuracy 86.300\n",
      "[[44.90182534]] [[49.13756335]] [[93.94293639]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4064 \n",
      "Accuracy: 8495/10000 (84.95%)\n",
      "\n",
      "Round 162, Train average loss 0.243 Test accuracy 84.950\n",
      "[[44.89302694]] [[49.14706344]] [[93.94824045]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3972 \n",
      "Accuracy: 8556/10000 (85.56%)\n",
      "\n",
      "Round 163, Train average loss 0.245 Test accuracy 85.560\n",
      "[[44.89385009]] [[49.15928707]] [[93.93638661]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3559 \n",
      "Accuracy: 8674/10000 (86.74%)\n",
      "\n",
      "Round 164, Train average loss 0.245 Test accuracy 86.740\n",
      "[[44.89704365]] [[49.18491671]] [[93.98700243]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3452 \n",
      "Accuracy: 8716/10000 (87.16%)\n",
      "\n",
      "Round 165, Train average loss 0.243 Test accuracy 87.160\n",
      "[[44.92631096]] [[49.20510883]] [[94.03654553]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3364 \n",
      "Accuracy: 8729/10000 (87.29%)\n",
      "\n",
      "Round 166, Train average loss 0.242 Test accuracy 87.290\n",
      "[[44.96458207]] [[49.22541251]] [[94.0952643]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3232 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round 167, Train average loss 0.241 Test accuracy 87.920\n",
      "[[44.9421194]] [[49.22742463]] [[94.08619609]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3389 \n",
      "Accuracy: 8725/10000 (87.25%)\n",
      "\n",
      "Round 168, Train average loss 0.241 Test accuracy 87.250\n",
      "[[44.95652154]] [[49.25254123]] [[94.13304723]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3391 \n",
      "Accuracy: 8712/10000 (87.12%)\n",
      "\n",
      "Round 169, Train average loss 0.241 Test accuracy 87.120\n",
      "[[44.97446196]] [[49.2706016]] [[94.1863601]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3482 \n",
      "Accuracy: 8699/10000 (86.99%)\n",
      "\n",
      "Round 170, Train average loss 0.241 Test accuracy 86.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.95304526]] [[49.29145801]] [[94.18311783]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3442 \n",
      "Accuracy: 8710/10000 (87.10%)\n",
      "\n",
      "Round 171, Train average loss 0.242 Test accuracy 87.100\n",
      "[[44.93933595]] [[49.33234887]] [[94.1904807]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3506 \n",
      "Accuracy: 8658/10000 (86.58%)\n",
      "\n",
      "Round 172, Train average loss 0.242 Test accuracy 86.580\n",
      "[[44.95766324]] [[49.34881152]] [[94.22202215]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3412 \n",
      "Accuracy: 8716/10000 (87.16%)\n",
      "\n",
      "Round 173, Train average loss 0.241 Test accuracy 87.160\n",
      "[[44.97107985]] [[49.39583503]] [[94.28249038]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3700 \n",
      "Accuracy: 8588/10000 (85.88%)\n",
      "\n",
      "Round 174, Train average loss 0.241 Test accuracy 85.880\n",
      "[[44.99003883]] [[49.42358387]] [[94.3470647]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3531 \n",
      "Accuracy: 8653/10000 (86.53%)\n",
      "\n",
      "Round 175, Train average loss 0.242 Test accuracy 86.530\n",
      "[[45.00085802]] [[49.44484007]] [[94.37606284]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3408 \n",
      "Accuracy: 8718/10000 (87.18%)\n",
      "\n",
      "Round 176, Train average loss 0.241 Test accuracy 87.180\n",
      "[[44.99352179]] [[49.49302494]] [[94.39159407]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3360 \n",
      "Accuracy: 8756/10000 (87.56%)\n",
      "\n",
      "Round 177, Train average loss 0.241 Test accuracy 87.560\n",
      "[[44.96760012]] [[49.50029253]] [[94.37166761]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3325 \n",
      "Accuracy: 8740/10000 (87.40%)\n",
      "\n",
      "Round 178, Train average loss 0.241 Test accuracy 87.400\n",
      "[[44.9932702]] [[49.51162308]] [[94.4262675]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3298 \n",
      "Accuracy: 8730/10000 (87.30%)\n",
      "\n",
      "Round 179, Train average loss 0.240 Test accuracy 87.300\n",
      "[[45.01071025]] [[49.52550778]] [[94.48384576]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3225 \n",
      "Accuracy: 8765/10000 (87.65%)\n",
      "\n",
      "Round 180, Train average loss 0.240 Test accuracy 87.650\n",
      "[[44.99075491]] [[49.55102259]] [[94.49402334]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3431 \n",
      "Accuracy: 8730/10000 (87.30%)\n",
      "\n",
      "Round 181, Train average loss 0.240 Test accuracy 87.300\n",
      "[[44.9688776]] [[49.57408597]] [[94.49167456]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3388 \n",
      "Accuracy: 8711/10000 (87.11%)\n",
      "\n",
      "Round 182, Train average loss 0.241 Test accuracy 87.110\n",
      "[[44.98932396]] [[49.57536668]] [[94.49167019]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3480 \n",
      "Accuracy: 8704/10000 (87.04%)\n",
      "\n",
      "Round 183, Train average loss 0.240 Test accuracy 87.040\n",
      "[[44.96350211]] [[49.5948769]] [[94.48741266]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3955 \n",
      "Accuracy: 8534/10000 (85.34%)\n",
      "\n",
      "Round 184, Train average loss 0.241 Test accuracy 85.340\n",
      "[[44.95892167]] [[49.61619144]] [[94.52611973]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3410 \n",
      "Accuracy: 8673/10000 (86.73%)\n",
      "\n",
      "Round 185, Train average loss 0.244 Test accuracy 86.730\n",
      "[[44.99488591]] [[49.62492235]] [[94.56537846]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3379 \n",
      "Accuracy: 8720/10000 (87.20%)\n",
      "\n",
      "Round 186, Train average loss 0.240 Test accuracy 87.200\n",
      "[[44.9972735]] [[49.64293758]] [[94.59261024]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3526 \n",
      "Accuracy: 8653/10000 (86.53%)\n",
      "\n",
      "Round 187, Train average loss 0.240 Test accuracy 86.530\n",
      "[[44.98718493]] [[49.65725402]] [[94.6118103]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3211 \n",
      "Accuracy: 8774/10000 (87.74%)\n",
      "\n",
      "Round 188, Train average loss 0.241 Test accuracy 87.740\n",
      "[[44.98295631]] [[49.69479669]] [[94.62645309]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3611 \n",
      "Accuracy: 8625/10000 (86.25%)\n",
      "\n",
      "Round 189, Train average loss 0.239 Test accuracy 86.250\n",
      "[[44.97462564]] [[49.72010342]] [[94.64428938]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3823 \n",
      "Accuracy: 8558/10000 (85.58%)\n",
      "\n",
      "Round 190, Train average loss 0.241 Test accuracy 85.580\n",
      "[[44.96358818]] [[49.7628108]] [[94.69989419]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9407 \n",
      "Accuracy: 6936/10000 (69.36%)\n",
      "\n",
      "Round 191, Train average loss 0.242 Test accuracy 69.360\n",
      "[[45.04906228]] [[49.78427187]] [[94.77739438]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5315 \n",
      "Accuracy: 8169/10000 (81.69%)\n",
      "\n",
      "Round 192, Train average loss 0.327 Test accuracy 81.690\n",
      "[[45.03400617]] [[49.78870571]] [[94.71586712]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3472 \n",
      "Accuracy: 8725/10000 (87.25%)\n",
      "\n",
      "Round 193, Train average loss 0.253 Test accuracy 87.250\n",
      "[[45.04194568]] [[49.82378429]] [[94.77695707]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3514 \n",
      "Accuracy: 8722/10000 (87.22%)\n",
      "\n",
      "Round 194, Train average loss 0.242 Test accuracy 87.220\n",
      "[[45.03899732]] [[49.84692017]] [[94.80247874]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3425 \n",
      "Accuracy: 8703/10000 (87.03%)\n",
      "\n",
      "Round 195, Train average loss 0.242 Test accuracy 87.030\n",
      "[[45.06403784]] [[49.86065114]] [[94.82909892]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3258 \n",
      "Accuracy: 8755/10000 (87.55%)\n",
      "\n",
      "Round 196, Train average loss 0.242 Test accuracy 87.550\n",
      "[[45.07396868]] [[49.86769576]] [[94.84481876]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3347 \n",
      "Accuracy: 8723/10000 (87.23%)\n",
      "\n",
      "Round 197, Train average loss 0.241 Test accuracy 87.230\n",
      "[[45.07352378]] [[49.87480488]] [[94.85971123]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3756 \n",
      "Accuracy: 8603/10000 (86.03%)\n",
      "\n",
      "Round 198, Train average loss 0.241 Test accuracy 86.030\n",
      "[[45.06264492]] [[49.87336249]] [[94.85108862]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3243 \n",
      "Accuracy: 8773/10000 (87.73%)\n",
      "\n",
      "Round 199, Train average loss 0.243 Test accuracy 87.730\n",
      "[[45.09035491]] [[49.88436519]] [[94.88920856]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3376 \n",
      "Accuracy: 8716/10000 (87.16%)\n",
      "\n",
      "Round 200, Train average loss 0.241 Test accuracy 87.160\n",
      "[[45.10174772]] [[49.90046422]] [[94.91773637]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3477 \n",
      "Accuracy: 8690/10000 (86.90%)\n",
      "\n",
      "Round 201, Train average loss 0.241 Test accuracy 86.900\n",
      "[[45.11138772]] [[36.31609106]] [[12.09459904]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3659 \n",
      "Accuracy: 8631/10000 (86.31%)\n",
      "\n",
      "Round 202, Train average loss 0.242 Test accuracy 86.310\n",
      "[[45.11296424]] [[36.45059104]] [[11.29441393]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3315 \n",
      "Accuracy: 8752/10000 (87.52%)\n",
      "\n",
      "Round 203, Train average loss 0.242 Test accuracy 87.520\n",
      "[[45.11021286]] [[36.45453752]] [[11.15672235]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3273 \n",
      "Accuracy: 8743/10000 (87.43%)\n",
      "\n",
      "Round 204, Train average loss 0.241 Test accuracy 87.430\n",
      "[[45.11995568]] [[36.32020343]] [[11.09746702]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3265 \n",
      "Accuracy: 8757/10000 (87.57%)\n",
      "\n",
      "Round 205, Train average loss 0.240 Test accuracy 87.570\n",
      "[[45.11893538]] [[36.33143607]] [[11.12644153]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3232 \n",
      "Accuracy: 8760/10000 (87.60%)\n",
      "\n",
      "Round 206, Train average loss 0.240 Test accuracy 87.600\n",
      "[[45.15105336]] [[36.30307885]] [[11.08976064]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3862 \n",
      "Accuracy: 8559/10000 (85.59%)\n",
      "\n",
      "Round 207, Train average loss 0.240 Test accuracy 85.590\n",
      "[[45.14707142]] [[36.43214457]] [[11.09162577]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3511 \n",
      "Accuracy: 8687/10000 (86.87%)\n",
      "\n",
      "Round 208, Train average loss 0.242 Test accuracy 86.870\n",
      "[[45.14451001]] [[36.46939899]] [[11.06525929]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3271 \n",
      "Accuracy: 8769/10000 (87.69%)\n",
      "\n",
      "Round 209, Train average loss 0.242 Test accuracy 87.690\n",
      "[[45.13988648]] [[36.63562343]] [[10.89590216]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3372 \n",
      "Accuracy: 8748/10000 (87.48%)\n",
      "\n",
      "Round 210, Train average loss 0.240 Test accuracy 87.480\n",
      "[[45.1197931]] [[36.57564186]] [[10.57023613]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3249 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "Round 211, Train average loss 0.240 Test accuracy 88.000\n",
      "[[45.13329505]] [[36.55079278]] [[10.26308407]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3134 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round 212, Train average loss 0.240 Test accuracy 88.010\n",
      "[[45.16570351]] [[36.79958732]] [[10.03742623]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3442 \n",
      "Accuracy: 8700/10000 (87.00%)\n",
      "\n",
      "Round 213, Train average loss 0.239 Test accuracy 87.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.17070192]] [[36.91485808]] [[9.82400144]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3228 \n",
      "Accuracy: 8767/10000 (87.67%)\n",
      "\n",
      "Round 214, Train average loss 0.240 Test accuracy 87.670\n",
      "[[45.15254765]] [[37.02356704]] [[9.46445558]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3155 \n",
      "Accuracy: 8814/10000 (88.14%)\n",
      "\n",
      "Round 215, Train average loss 0.239 Test accuracy 88.140\n",
      "[[45.15232725]] [[37.29163029]] [[9.20387689]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3297 \n",
      "Accuracy: 8729/10000 (87.29%)\n",
      "\n",
      "Round 216, Train average loss 0.239 Test accuracy 87.290\n",
      "[[45.15171735]] [[37.41798005]] [[9.03137222]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3199 \n",
      "Accuracy: 8789/10000 (87.89%)\n",
      "\n",
      "Round 217, Train average loss 0.240 Test accuracy 87.890\n",
      "[[45.1567826]] [[37.55903523]] [[9.05523406]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3188 \n",
      "Accuracy: 8772/10000 (87.72%)\n",
      "\n",
      "Round 218, Train average loss 0.239 Test accuracy 87.720\n",
      "[[45.15567768]] [[37.56794331]] [[8.68304562]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3353 \n",
      "Accuracy: 8722/10000 (87.22%)\n",
      "\n",
      "Round 219, Train average loss 0.239 Test accuracy 87.220\n",
      "[[45.16802443]] [[37.82038129]] [[8.37847012]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3313 \n",
      "Accuracy: 8741/10000 (87.41%)\n",
      "\n",
      "Round 220, Train average loss 0.239 Test accuracy 87.410\n",
      "[[45.14967444]] [[37.95092921]] [[8.13523433]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3481 \n",
      "Accuracy: 8686/10000 (86.86%)\n",
      "\n",
      "Round 221, Train average loss 0.239 Test accuracy 86.860\n",
      "[[45.18390573]] [[38.35538939]] [[7.7992924]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round 222, Train average loss 0.240 Test accuracy 88.080\n",
      "[[45.18891895]] [[38.4675139]] [[7.49965871]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3486 \n",
      "Accuracy: 8670/10000 (86.70%)\n",
      "\n",
      "Round 223, Train average loss 0.238 Test accuracy 86.700\n",
      "[[45.20132602]] [[38.70825336]] [[7.23267043]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4129 \n",
      "Accuracy: 8415/10000 (84.15%)\n",
      "\n",
      "Round 224, Train average loss 0.240 Test accuracy 84.150\n",
      "[[45.2151373]] [[38.98131844]] [[7.0182301]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3505 \n",
      "Accuracy: 8643/10000 (86.43%)\n",
      "\n",
      "Round 225, Train average loss 0.248 Test accuracy 86.430\n",
      "[[45.22998283]] [[39.18344199]] [[6.78688004]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3873 \n",
      "Accuracy: 8517/10000 (85.17%)\n",
      "\n",
      "Round 226, Train average loss 0.241 Test accuracy 85.170\n",
      "[[45.24483988]] [[39.37320978]] [[6.6285205]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4282 \n",
      "Accuracy: 8416/10000 (84.16%)\n",
      "\n",
      "Round 227, Train average loss 0.244 Test accuracy 84.160\n",
      "[[45.26107422]] [[39.55695774]] [[6.52518787]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3255 \n",
      "Accuracy: 8759/10000 (87.59%)\n",
      "\n",
      "Round 228, Train average loss 0.244 Test accuracy 87.590\n",
      "[[45.23313507]] [[39.4412715]] [[6.41696919]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3393 \n",
      "Accuracy: 8684/10000 (86.84%)\n",
      "\n",
      "Round 229, Train average loss 0.240 Test accuracy 86.840\n",
      "[[45.21509054]] [[39.85513847]] [[6.09252942]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3243 \n",
      "Accuracy: 8762/10000 (87.62%)\n",
      "\n",
      "Round 230, Train average loss 0.240 Test accuracy 87.620\n",
      "[[45.22719988]] [[39.99903631]] [[5.95184888]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3128 \n",
      "Accuracy: 8770/10000 (87.70%)\n",
      "\n",
      "Round 231, Train average loss 0.239 Test accuracy 87.700\n",
      "[[45.21952426]] [[40.05643077]] [[5.83170773]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3147 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round 232, Train average loss 0.239 Test accuracy 87.920\n",
      "[[45.19929062]] [[40.24775784]] [[5.67134009]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3582 \n",
      "Accuracy: 8658/10000 (86.58%)\n",
      "\n",
      "Round 233, Train average loss 0.238 Test accuracy 86.580\n",
      "[[45.19973012]] [[40.40006938]] [[5.57200118]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3759 \n",
      "Accuracy: 8574/10000 (85.74%)\n",
      "\n",
      "Round 234, Train average loss 0.241 Test accuracy 85.740\n",
      "[[45.23294019]] [[40.53780663]] [[5.47873584]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3158 \n",
      "Accuracy: 8786/10000 (87.86%)\n",
      "\n",
      "Round 235, Train average loss 0.243 Test accuracy 87.860\n",
      "[[45.21007502]] [[40.44697394]] [[5.37680517]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3357 \n",
      "Accuracy: 8712/10000 (87.12%)\n",
      "\n",
      "Round 236, Train average loss 0.239 Test accuracy 87.120\n",
      "[[45.18242293]] [[40.68196589]] [[5.2500601]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3273 \n",
      "Accuracy: 8736/10000 (87.36%)\n",
      "\n",
      "Round 237, Train average loss 0.240 Test accuracy 87.360\n",
      "[[45.18081083]] [[40.74809459]] [[5.13515987]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3295 \n",
      "Accuracy: 8749/10000 (87.49%)\n",
      "\n",
      "Round 238, Train average loss 0.239 Test accuracy 87.490\n",
      "[[45.20399104]] [[40.7817095]] [[5.03450014]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3400 \n",
      "Accuracy: 8702/10000 (87.02%)\n",
      "\n",
      "Round 239, Train average loss 0.239 Test accuracy 87.020\n",
      "[[45.20507092]] [[40.84910073]] [[4.92420558]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3190 \n",
      "Accuracy: 8752/10000 (87.52%)\n",
      "\n",
      "Round 240, Train average loss 0.239 Test accuracy 87.520\n",
      "[[45.20715423]] [[40.92895653]] [[4.83684924]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3476 \n",
      "Accuracy: 8694/10000 (86.94%)\n",
      "\n",
      "Round 241, Train average loss 0.239 Test accuracy 86.940\n",
      "[[45.20374513]] [[41.17960196]] [[4.72147977]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3177 \n",
      "Accuracy: 8778/10000 (87.78%)\n",
      "\n",
      "Round 242, Train average loss 0.240 Test accuracy 87.780\n",
      "[[45.18762997]] [[41.26288557]] [[4.62089488]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3433 \n",
      "Accuracy: 8702/10000 (87.02%)\n",
      "\n",
      "Round 243, Train average loss 0.238 Test accuracy 87.020\n",
      "[[45.16809366]] [[41.36455894]] [[4.53430945]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3945 \n",
      "Accuracy: 8541/10000 (85.41%)\n",
      "\n",
      "Round 244, Train average loss 0.239 Test accuracy 85.410\n",
      "[[45.16896257]] [[41.3440692]] [[4.44657769]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3489 \n",
      "Accuracy: 8701/10000 (87.01%)\n",
      "\n",
      "Round 245, Train average loss 0.242 Test accuracy 87.010\n",
      "[[45.14757166]] [[41.466522]] [[4.37947674]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3395 \n",
      "Accuracy: 8697/10000 (86.97%)\n",
      "\n",
      "Round 246, Train average loss 0.240 Test accuracy 86.970\n",
      "[[45.14502994]] [[41.44857792]] [[4.29941651]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3468 \n",
      "Accuracy: 8684/10000 (86.84%)\n",
      "\n",
      "Round 247, Train average loss 0.240 Test accuracy 86.840\n",
      "[[45.14270269]] [[41.48753627]] [[4.24062644]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3138 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "Round 248, Train average loss 0.240 Test accuracy 87.900\n",
      "[[45.14800838]] [[41.47495488]] [[4.18756942]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3079 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round 249, Train average loss 0.238 Test accuracy 88.260\n",
      "[[45.16066492]] [[41.54079391]] [[4.14234823]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3310 \n",
      "Accuracy: 8724/10000 (87.24%)\n",
      "\n",
      "Round 250, Train average loss 0.237 Test accuracy 87.240\n",
      "[[45.15963431]] [[41.69884125]] [[4.04907732]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round 251, Train average loss 0.239 Test accuracy 87.990\n",
      "[[45.17403744]] [[41.84252192]] [[3.97840008]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3017 \n",
      "Accuracy: 8817/10000 (88.17%)\n",
      "\n",
      "Round 252, Train average loss 0.237 Test accuracy 88.170\n",
      "[[45.15158072]] [[41.91354068]] [[3.91160387]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3055 \n",
      "Accuracy: 8819/10000 (88.19%)\n",
      "\n",
      "Round 253, Train average loss 0.237 Test accuracy 88.190\n",
      "[[45.14000539]] [[41.95039367]] [[3.87796175]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3165 \n",
      "Accuracy: 8756/10000 (87.56%)\n",
      "\n",
      "Round 254, Train average loss 0.237 Test accuracy 87.560\n",
      "[[45.11210106]] [[42.01761102]] [[3.85433857]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3463 \n",
      "Accuracy: 8669/10000 (86.69%)\n",
      "\n",
      "Round 255, Train average loss 0.238 Test accuracy 86.690\n",
      "[[45.09627299]] [[42.05049732]] [[3.81186956]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3290 \n",
      "Accuracy: 8742/10000 (87.42%)\n",
      "\n",
      "Round 256, Train average loss 0.240 Test accuracy 87.420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.09354044]] [[42.1099382]] [[3.75950367]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3220 \n",
      "Accuracy: 8752/10000 (87.52%)\n",
      "\n",
      "Round 257, Train average loss 0.238 Test accuracy 87.520\n",
      "[[45.05362415]] [[42.30789711]] [[3.63607918]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3596 \n",
      "Accuracy: 8618/10000 (86.18%)\n",
      "\n",
      "Round 258, Train average loss 0.238 Test accuracy 86.180\n",
      "[[45.04399285]] [[42.3604014]] [[3.57779268]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3123 \n",
      "Accuracy: 8785/10000 (87.85%)\n",
      "\n",
      "Round 259, Train average loss 0.241 Test accuracy 87.850\n",
      "[[45.05570744]] [[42.40122453]] [[3.50850748]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3222 \n",
      "Accuracy: 8775/10000 (87.75%)\n",
      "\n",
      "Round 260, Train average loss 0.238 Test accuracy 87.750\n",
      "[[45.06253548]] [[42.49117665]] [[3.44618751]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3042 \n",
      "Accuracy: 8835/10000 (88.35%)\n",
      "\n",
      "Round 261, Train average loss 0.238 Test accuracy 88.350\n",
      "[[45.0488187]] [[42.50599587]] [[3.39904769]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3119 \n",
      "Accuracy: 8810/10000 (88.10%)\n",
      "\n",
      "Round 262, Train average loss 0.237 Test accuracy 88.100\n",
      "[[45.04649829]] [[42.50729844]] [[3.33112204]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3217 \n",
      "Accuracy: 8774/10000 (87.74%)\n",
      "\n",
      "Round 263, Train average loss 0.237 Test accuracy 87.740\n",
      "[[45.03995836]] [[42.57453514]] [[3.29490459]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3043 \n",
      "Accuracy: 8836/10000 (88.36%)\n",
      "\n",
      "Round 264, Train average loss 0.237 Test accuracy 88.360\n",
      "[[45.04512941]] [[42.62760836]] [[3.24943581]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2929 \n",
      "Accuracy: 8846/10000 (88.46%)\n",
      "\n",
      "Round 265, Train average loss 0.237 Test accuracy 88.460\n",
      "[[45.03827218]] [[42.66988063]] [[3.19167086]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3516 \n",
      "Accuracy: 8620/10000 (86.20%)\n",
      "\n",
      "Round 266, Train average loss 0.236 Test accuracy 86.200\n",
      "[[45.02783445]] [[42.71603597]] [[3.15903116]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3116 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round 267, Train average loss 0.238 Test accuracy 88.090\n",
      "[[45.02887699]] [[42.80261365]] [[3.10387873]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3006 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round 268, Train average loss 0.237 Test accuracy 88.300\n",
      "[[45.02733049]] [[42.86322746]] [[3.05596688]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3182 \n",
      "Accuracy: 8761/10000 (87.61%)\n",
      "\n",
      "Round 269, Train average loss 0.237 Test accuracy 87.610\n",
      "[[45.04127949]] [[42.92196004]] [[3.01566922]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3290 \n",
      "Accuracy: 8734/10000 (87.34%)\n",
      "\n",
      "Round 270, Train average loss 0.238 Test accuracy 87.340\n",
      "[[45.03936615]] [[42.85928494]] [[2.97493988]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3203 \n",
      "Accuracy: 8762/10000 (87.62%)\n",
      "\n",
      "Round 271, Train average loss 0.238 Test accuracy 87.620\n",
      "[[45.05026106]] [[42.89032148]] [[2.96282525]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3138 \n",
      "Accuracy: 8774/10000 (87.74%)\n",
      "\n",
      "Round 272, Train average loss 0.238 Test accuracy 87.740\n",
      "[[45.04394201]] [[42.93738772]] [[2.94301942]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3114 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round 273, Train average loss 0.237 Test accuracy 87.950\n",
      "[[45.05630484]] [[42.98764129]] [[2.90340523]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3259 \n",
      "Accuracy: 8740/10000 (87.40%)\n",
      "\n",
      "Round 274, Train average loss 0.237 Test accuracy 87.400\n",
      "[[45.05008241]] [[43.04176082]] [[2.8668918]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3255 \n",
      "Accuracy: 8743/10000 (87.43%)\n",
      "\n",
      "Round 275, Train average loss 0.238 Test accuracy 87.430\n",
      "[[45.03690192]] [[43.07850665]] [[2.81778185]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3162 \n",
      "Accuracy: 8746/10000 (87.46%)\n",
      "\n",
      "Round 276, Train average loss 0.237 Test accuracy 87.460\n",
      "[[45.04642454]] [[43.07363931]] [[2.80803344]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3001 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round 277, Train average loss 0.237 Test accuracy 88.080\n",
      "[[45.026097]] [[43.12628648]] [[2.76178943]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2971 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round 278, Train average loss 0.237 Test accuracy 88.380\n",
      "[[45.02133879]] [[43.17359178]] [[2.7620326]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3070 \n",
      "Accuracy: 8816/10000 (88.16%)\n",
      "\n",
      "Round 279, Train average loss 0.236 Test accuracy 88.160\n",
      "[[45.03219725]] [[43.23974162]] [[2.74692893]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3035 \n",
      "Accuracy: 8814/10000 (88.14%)\n",
      "\n",
      "Round 280, Train average loss 0.236 Test accuracy 88.140\n",
      "[[45.02707664]] [[43.25013178]] [[2.69759528]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2952 \n",
      "Accuracy: 8835/10000 (88.35%)\n",
      "\n",
      "Round 281, Train average loss 0.236 Test accuracy 88.350\n",
      "[[45.02035445]] [[43.24650447]] [[2.67339408]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8761/10000 (87.61%)\n",
      "\n",
      "Round 282, Train average loss 0.236 Test accuracy 87.610\n",
      "[[45.00631441]] [[43.24744534]] [[2.62589853]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3144 \n",
      "Accuracy: 8777/10000 (87.77%)\n",
      "\n",
      "Round 283, Train average loss 0.237 Test accuracy 87.770\n",
      "[[45.00700909]] [[43.29937935]] [[2.59209659]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3919 \n",
      "Accuracy: 8506/10000 (85.06%)\n",
      "\n",
      "Round 284, Train average loss 0.237 Test accuracy 85.060\n",
      "[[45.01548238]] [[43.31521639]] [[2.56746948]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3589 \n",
      "Accuracy: 8597/10000 (85.97%)\n",
      "\n",
      "Round 285, Train average loss 0.247 Test accuracy 85.970\n",
      "[[45.02369288]] [[43.31620631]] [[2.5435305]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3905 \n",
      "Accuracy: 8507/10000 (85.07%)\n",
      "\n",
      "Round 286, Train average loss 0.239 Test accuracy 85.070\n",
      "[[45.01338863]] [[43.3371378]] [[2.5120276]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 8089/10000 (80.89%)\n",
      "\n",
      "Round 287, Train average loss 0.241 Test accuracy 80.890\n",
      "[[45.0702154]] [[43.39366675]] [[2.50436086]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4882 \n",
      "Accuracy: 8297/10000 (82.97%)\n",
      "\n",
      "Round 288, Train average loss 0.288 Test accuracy 82.970\n",
      "[[45.05704443]] [[43.45841486]] [[2.46704132]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6237 \n",
      "Accuracy: 7726/10000 (77.26%)\n",
      "\n",
      "Round 289, Train average loss 0.277 Test accuracy 77.260\n",
      "[[44.92318131]] [[43.46495648]] [[2.46794279]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4663 \n",
      "Accuracy: 8307/10000 (83.07%)\n",
      "\n",
      "Round 290, Train average loss 0.257 Test accuracy 83.070\n",
      "[[44.92945814]] [[43.48129398]] [[2.42459829]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4705 \n",
      "Accuracy: 8317/10000 (83.17%)\n",
      "\n",
      "Round 291, Train average loss 0.249 Test accuracy 83.170\n",
      "[[44.95277161]] [[43.5183804]] [[2.41756332]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4435 \n",
      "Accuracy: 8399/10000 (83.99%)\n",
      "\n",
      "Round 292, Train average loss 0.250 Test accuracy 83.990\n",
      "[[44.9339762]] [[43.4921644]] [[2.3771422]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6326 \n",
      "Accuracy: 7928/10000 (79.28%)\n",
      "\n",
      "Round 293, Train average loss 0.248 Test accuracy 79.280\n",
      "[[45.01836513]] [[43.5111105]] [[2.44904345]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4086 \n",
      "Accuracy: 8524/10000 (85.24%)\n",
      "\n",
      "Round 294, Train average loss 0.260 Test accuracy 85.240\n",
      "[[44.99467951]] [[43.54971744]] [[2.41109723]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3664 \n",
      "Accuracy: 8686/10000 (86.86%)\n",
      "\n",
      "Round 295, Train average loss 0.246 Test accuracy 86.860\n",
      "[[45.03042642]] [[43.53732223]] [[2.38459925]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3968 \n",
      "Accuracy: 8564/10000 (85.64%)\n",
      "\n",
      "Round 296, Train average loss 0.244 Test accuracy 85.640\n",
      "[[45.0757259]] [[43.59002054]] [[2.39071613]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3521 \n",
      "Accuracy: 8724/10000 (87.24%)\n",
      "\n",
      "Round 297, Train average loss 0.245 Test accuracy 87.240\n",
      "[[45.11874868]] [[43.65853983]] [[2.39548751]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3315 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round 298, Train average loss 0.243 Test accuracy 87.970\n",
      "[[45.16075711]] [[43.85338163]] [[2.39990849]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3256 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round 299, Train average loss 0.241 Test accuracy 88.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.17436554]] [[43.87121063]] [[2.36344118]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3109 \n",
      "Accuracy: 8819/10000 (88.19%)\n",
      "\n",
      "Round 300, Train average loss 0.241 Test accuracy 88.190\n",
      "[[45.18588197]] [[43.90968938]] [[2.31872318]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3049 \n",
      "Accuracy: 8840/10000 (88.40%)\n",
      "\n",
      "Round 301, Train average loss 0.239 Test accuracy 88.400\n",
      "[[45.20619188]] [[43.84313628]] [[2.25703014]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3140 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round 302, Train average loss 0.239 Test accuracy 87.970\n",
      "[[45.22958113]] [[43.91430012]] [[2.2154594]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3076 \n",
      "Accuracy: 8811/10000 (88.11%)\n",
      "\n",
      "Round 303, Train average loss 0.239 Test accuracy 88.110\n",
      "[[45.2521202]] [[43.98403727]] [[2.18926909]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3171 \n",
      "Accuracy: 8772/10000 (87.72%)\n",
      "\n",
      "Round 304, Train average loss 0.239 Test accuracy 87.720\n",
      "[[45.25419019]] [[44.06883841]] [[2.16147526]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3027 \n",
      "Accuracy: 8815/10000 (88.15%)\n",
      "\n",
      "Round 305, Train average loss 0.239 Test accuracy 88.150\n",
      "[[45.23770833]] [[44.10587037]] [[2.13887243]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3320 \n",
      "Accuracy: 8727/10000 (87.27%)\n",
      "\n",
      "Round 306, Train average loss 0.238 Test accuracy 87.270\n",
      "[[45.22982979]] [[44.07831992]] [[2.12262952]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round 307, Train average loss 0.239 Test accuracy 88.010\n",
      "[[45.22031253]] [[44.08267895]] [[2.106413]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3464 \n",
      "Accuracy: 8694/10000 (86.94%)\n",
      "\n",
      "Round 308, Train average loss 0.238 Test accuracy 86.940\n",
      "[[45.21429869]] [[44.14531349]] [[2.08959731]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3300 \n",
      "Accuracy: 8723/10000 (87.23%)\n",
      "\n",
      "Round 309, Train average loss 0.240 Test accuracy 87.230\n",
      "[[45.22531511]] [[44.16918795]] [[2.06789713]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3183 \n",
      "Accuracy: 8768/10000 (87.68%)\n",
      "\n",
      "Round 310, Train average loss 0.239 Test accuracy 87.680\n",
      "[[45.20226341]] [[44.16523808]] [[2.04866089]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3036 \n",
      "Accuracy: 8834/10000 (88.34%)\n",
      "\n",
      "Round 311, Train average loss 0.239 Test accuracy 88.340\n",
      "[[45.19992271]] [[44.21801031]] [[2.03822239]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2915 \n",
      "Accuracy: 8863/10000 (88.63%)\n",
      "\n",
      "Round 312, Train average loss 0.238 Test accuracy 88.630\n",
      "[[45.19877593]] [[44.24153459]] [[2.02105057]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2970 \n",
      "Accuracy: 8832/10000 (88.32%)\n",
      "\n",
      "Round 313, Train average loss 0.237 Test accuracy 88.320\n",
      "[[45.20196674]] [[44.26624366]] [[2.00081155]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3334 \n",
      "Accuracy: 8706/10000 (87.06%)\n",
      "\n",
      "Round 314, Train average loss 0.237 Test accuracy 87.060\n",
      "[[45.2054544]] [[44.26223392]] [[1.98571302]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3195 \n",
      "Accuracy: 8777/10000 (87.77%)\n",
      "\n",
      "Round 315, Train average loss 0.239 Test accuracy 87.770\n",
      "[[45.22847626]] [[44.27459908]] [[1.96405516]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3161 \n",
      "Accuracy: 8779/10000 (87.79%)\n",
      "\n",
      "Round 316, Train average loss 0.238 Test accuracy 87.790\n",
      "[[45.22252979]] [[44.24882611]] [[1.94452855]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2926 \n",
      "Accuracy: 8848/10000 (88.48%)\n",
      "\n",
      "Round 317, Train average loss 0.238 Test accuracy 88.480\n",
      "[[45.2069082]] [[44.27875071]] [[1.92239483]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3016 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 318, Train average loss 0.237 Test accuracy 88.310\n",
      "[[45.20530362]] [[44.29609058]] [[1.90464228]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3122 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round 319, Train average loss 0.237 Test accuracy 88.050\n",
      "[[45.19128016]] [[44.27410077]] [[1.89617934]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3099 \n",
      "Accuracy: 8776/10000 (87.76%)\n",
      "\n",
      "Round 320, Train average loss 0.237 Test accuracy 87.760\n",
      "[[45.21751519]] [[44.30618756]] [[1.89236455]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2945 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "Round 321, Train average loss 0.237 Test accuracy 88.270\n",
      "[[45.19030948]] [[44.35500401]] [[1.89209822]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3062 \n",
      "Accuracy: 8820/10000 (88.20%)\n",
      "\n",
      "Round 322, Train average loss 0.237 Test accuracy 88.200\n",
      "[[45.17894443]] [[44.37469806]] [[1.85917716]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2965 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "Round 323, Train average loss 0.237 Test accuracy 88.270\n",
      "[[45.18076908]] [[44.38315231]] [[1.84548684]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2948 \n",
      "Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "Round 324, Train average loss 0.237 Test accuracy 88.410\n",
      "[[45.20217497]] [[44.44314021]] [[1.84920851]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3036 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round 325, Train average loss 0.236 Test accuracy 88.020\n",
      "[[45.20076611]] [[44.45576915]] [[1.83045971]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3207 \n",
      "Accuracy: 8739/10000 (87.39%)\n",
      "\n",
      "Round 326, Train average loss 0.237 Test accuracy 87.390\n",
      "[[45.18688976]] [[44.56179352]] [[1.85120694]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3006 \n",
      "Accuracy: 8820/10000 (88.20%)\n",
      "\n",
      "Round 327, Train average loss 0.238 Test accuracy 88.200\n",
      "[[45.16537262]] [[44.56070374]] [[1.79371914]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round 328, Train average loss 0.237 Test accuracy 87.840\n",
      "[[45.15708412]] [[44.58112968]] [[1.77281847]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2938 \n",
      "Accuracy: 8832/10000 (88.32%)\n",
      "\n",
      "Round 329, Train average loss 0.237 Test accuracy 88.320\n",
      "[[45.15042758]] [[44.66334771]] [[1.74786369]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3009 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "Round 330, Train average loss 0.236 Test accuracy 88.240\n",
      "[[45.15103556]] [[44.69380019]] [[1.74782865]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3052 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round 331, Train average loss 0.236 Test accuracy 88.060\n",
      "[[45.15873307]] [[44.72074382]] [[1.72990971]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3027 \n",
      "Accuracy: 8798/10000 (87.98%)\n",
      "\n",
      "Round 332, Train average loss 0.237 Test accuracy 87.980\n",
      "[[45.16510251]] [[44.66728563]] [[1.70842843]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2949 \n",
      "Accuracy: 8845/10000 (88.45%)\n",
      "\n",
      "Round 333, Train average loss 0.236 Test accuracy 88.450\n",
      "[[45.18335242]] [[44.69760666]] [[1.6933533]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3121 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round 334, Train average loss 0.236 Test accuracy 87.970\n",
      "[[45.1883955]] [[44.66975507]] [[1.68342057]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2882 \n",
      "Accuracy: 8859/10000 (88.59%)\n",
      "\n",
      "Round 335, Train average loss 0.236 Test accuracy 88.590\n",
      "[[45.18658599]] [[44.68161444]] [[1.66719565]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2873 \n",
      "Accuracy: 8856/10000 (88.56%)\n",
      "\n",
      "Round 336, Train average loss 0.235 Test accuracy 88.560\n",
      "[[45.17678521]] [[44.68419624]] [[1.65288032]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2953 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round 337, Train average loss 0.235 Test accuracy 88.510\n",
      "[[45.19146791]] [[44.72404721]] [[1.6519604]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3009 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round 338, Train average loss 0.236 Test accuracy 88.080\n",
      "[[45.18848061]] [[44.74617784]] [[1.62564853]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8787/10000 (87.87%)\n",
      "\n",
      "Round 339, Train average loss 0.236 Test accuracy 87.870\n",
      "[[45.16222015]] [[44.73865114]] [[1.62453078]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round 340, Train average loss 0.237 Test accuracy 88.030\n",
      "[[45.16471099]] [[44.68287697]] [[1.62030195]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2922 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round 341, Train average loss 0.236 Test accuracy 88.380\n",
      "[[45.15030605]] [[44.70557246]] [[1.59944052]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2958 \n",
      "Accuracy: 8846/10000 (88.46%)\n",
      "\n",
      "Round 342, Train average loss 0.235 Test accuracy 88.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.12842103]] [[44.75020806]] [[1.58057362]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3064 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "Round 343, Train average loss 0.235 Test accuracy 88.270\n",
      "[[45.13527632]] [[44.75230418]] [[1.56656656]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3034 \n",
      "Accuracy: 8825/10000 (88.25%)\n",
      "\n",
      "Round 344, Train average loss 0.236 Test accuracy 88.250\n",
      "[[45.10524589]] [[44.75875054]] [[1.55128281]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3023 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round 345, Train average loss 0.236 Test accuracy 88.040\n",
      "[[45.11139459]] [[44.77101895]] [[1.53887117]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3024 \n",
      "Accuracy: 8811/10000 (88.11%)\n",
      "\n",
      "Round 346, Train average loss 0.236 Test accuracy 88.110\n",
      "[[45.12963423]] [[44.79672529]] [[1.52984736]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2985 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round 347, Train average loss 0.235 Test accuracy 88.300\n",
      "[[45.12401864]] [[44.77185421]] [[1.52539592]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3070 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round 348, Train average loss 0.235 Test accuracy 87.880\n",
      "[[45.11472045]] [[44.80251915]] [[1.5186823]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3387 \n",
      "Accuracy: 8697/10000 (86.97%)\n",
      "\n",
      "Round 349, Train average loss 0.236 Test accuracy 86.970\n",
      "[[45.07876728]] [[44.81334273]] [[1.50252954]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4152 \n",
      "Accuracy: 8390/10000 (83.90%)\n",
      "\n",
      "Round 350, Train average loss 0.238 Test accuracy 83.900\n",
      "[[45.0722424]] [[44.80929669]] [[1.49418785]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3521 \n",
      "Accuracy: 8652/10000 (86.52%)\n",
      "\n",
      "Round 351, Train average loss 0.251 Test accuracy 86.520\n",
      "[[45.06491742]] [[44.82370348]] [[1.48382117]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3463 \n",
      "Accuracy: 8689/10000 (86.89%)\n",
      "\n",
      "Round 352, Train average loss 0.238 Test accuracy 86.890\n",
      "[[45.02809552]] [[44.81329911]] [[1.47327455]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3516 \n",
      "Accuracy: 8662/10000 (86.62%)\n",
      "\n",
      "Round 353, Train average loss 0.238 Test accuracy 86.620\n",
      "[[45.03454394]] [[44.86290495]] [[1.48932962]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3626 \n",
      "Accuracy: 8614/10000 (86.14%)\n",
      "\n",
      "Round 354, Train average loss 0.238 Test accuracy 86.140\n",
      "[[45.0038595]] [[44.91361561]] [[1.49573057]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3541 \n",
      "Accuracy: 8652/10000 (86.52%)\n",
      "\n",
      "Round 355, Train average loss 0.238 Test accuracy 86.520\n",
      "[[44.98895134]] [[44.86823359]] [[1.45754151]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.7500 \n",
      "Accuracy: 7385/10000 (73.85%)\n",
      "\n",
      "Round 356, Train average loss 0.238 Test accuracy 73.850\n",
      "[[45.05460681]] [[44.86295664]] [[1.45824849]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8177 \n",
      "Accuracy: 7285/10000 (72.85%)\n",
      "\n",
      "Round 357, Train average loss 0.331 Test accuracy 72.850\n",
      "[[45.18056298]] [[44.88615995]] [[1.5054987]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8767 \n",
      "Accuracy: 7242/10000 (72.42%)\n",
      "\n",
      "Round 358, Train average loss 0.299 Test accuracy 72.420\n",
      "[[45.31150932]] [[44.86722924]] [[1.63783502]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6782 \n",
      "Accuracy: 7488/10000 (74.88%)\n",
      "\n",
      "Round 359, Train average loss 0.276 Test accuracy 74.880\n",
      "[[45.40999028]] [[44.88954152]] [[1.71563202]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4762 \n",
      "Accuracy: 8261/10000 (82.61%)\n",
      "\n",
      "Round 360, Train average loss 0.280 Test accuracy 82.610\n",
      "[[45.50466348]] [[44.89929233]] [[1.67312428]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3599 \n",
      "Accuracy: 8638/10000 (86.38%)\n",
      "\n",
      "Round 361, Train average loss 0.252 Test accuracy 86.380\n",
      "[[45.59886561]] [[44.87876073]] [[1.61910148]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3287 \n",
      "Accuracy: 8761/10000 (87.61%)\n",
      "\n",
      "Round 362, Train average loss 0.246 Test accuracy 87.610\n",
      "[[45.64847196]] [[44.90182534]] [[1.60633618]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3202 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round 363, Train average loss 0.245 Test accuracy 87.840\n",
      "[[45.65459114]] [[44.89302694]] [[1.56572341]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3285 \n",
      "Accuracy: 8742/10000 (87.42%)\n",
      "\n",
      "Round 364, Train average loss 0.243 Test accuracy 87.420\n",
      "[[45.65467494]] [[44.89385009]] [[1.56302393]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3381 \n",
      "Accuracy: 8686/10000 (86.86%)\n",
      "\n",
      "Round 365, Train average loss 0.243 Test accuracy 86.860\n",
      "[[45.63541598]] [[44.89704365]] [[1.54302659]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3335 \n",
      "Accuracy: 8719/10000 (87.19%)\n",
      "\n",
      "Round 366, Train average loss 0.243 Test accuracy 87.190\n",
      "[[45.62552728]] [[44.92631096]] [[1.50624228]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3403 \n",
      "Accuracy: 8680/10000 (86.80%)\n",
      "\n",
      "Round 367, Train average loss 0.243 Test accuracy 86.800\n",
      "[[45.64000832]] [[44.96458207]] [[1.50449055]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3145 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "Round 368, Train average loss 0.243 Test accuracy 87.900\n",
      "[[45.64825618]] [[44.9421194]] [[1.48860589]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3157 \n",
      "Accuracy: 8780/10000 (87.80%)\n",
      "\n",
      "Round 369, Train average loss 0.241 Test accuracy 87.800\n",
      "[[45.65232285]] [[44.95652154]] [[1.48465201]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3026 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 370, Train average loss 0.241 Test accuracy 88.310\n",
      "[[45.63399321]] [[44.97446196]] [[1.47842757]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3074 \n",
      "Accuracy: 8819/10000 (88.19%)\n",
      "\n",
      "Round 371, Train average loss 0.240 Test accuracy 88.190\n",
      "[[45.64471979]] [[44.95304526]] [[1.47061541]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3134 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round 372, Train average loss 0.240 Test accuracy 87.960\n",
      "[[45.64127127]] [[44.93933595]] [[1.46429139]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3077 \n",
      "Accuracy: 8812/10000 (88.12%)\n",
      "\n",
      "Round 373, Train average loss 0.239 Test accuracy 88.120\n",
      "[[45.65615479]] [[44.95766324]] [[1.45300622]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2985 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "Round 374, Train average loss 0.239 Test accuracy 88.270\n",
      "[[45.66998653]] [[44.97107985]] [[1.4492332]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2977 \n",
      "Accuracy: 8848/10000 (88.48%)\n",
      "\n",
      "Round 375, Train average loss 0.238 Test accuracy 88.480\n",
      "[[45.66607013]] [[44.99003883]] [[1.44220284]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3035 \n",
      "Accuracy: 8816/10000 (88.16%)\n",
      "\n",
      "Round 376, Train average loss 0.238 Test accuracy 88.160\n",
      "[[45.67347924]] [[45.00085802]] [[1.4348387]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3146 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round 377, Train average loss 0.238 Test accuracy 87.910\n",
      "[[45.64278289]] [[44.99352179]] [[1.42982139]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3238 \n",
      "Accuracy: 8762/10000 (87.62%)\n",
      "\n",
      "Round 378, Train average loss 0.238 Test accuracy 87.620\n",
      "[[45.63074316]] [[44.96760012]] [[1.42985021]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3030 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round 379, Train average loss 0.238 Test accuracy 88.010\n",
      "[[45.62755988]] [[44.9932702]] [[1.4173508]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3063 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round 380, Train average loss 0.238 Test accuracy 88.260\n",
      "[[45.62317684]] [[45.01071025]] [[1.41388234]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3044 \n",
      "Accuracy: 8815/10000 (88.15%)\n",
      "\n",
      "Round 381, Train average loss 0.237 Test accuracy 88.150\n",
      "[[45.62477339]] [[44.99075491]] [[1.40623891]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2993 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round 382, Train average loss 0.237 Test accuracy 88.030\n",
      "[[45.61294909]] [[44.9688776]] [[1.39758598]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2923 \n",
      "Accuracy: 8842/10000 (88.42%)\n",
      "\n",
      "Round 383, Train average loss 0.237 Test accuracy 88.420\n",
      "[[45.60094049]] [[44.98932396]] [[1.39377305]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3083 \n",
      "Accuracy: 8786/10000 (87.86%)\n",
      "\n",
      "Round 384, Train average loss 0.237 Test accuracy 87.860\n",
      "[[45.60734919]] [[44.96350211]] [[1.40186041]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3165 \n",
      "Accuracy: 8751/10000 (87.51%)\n",
      "\n",
      "Round 385, Train average loss 0.237 Test accuracy 87.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.60579954]] [[44.95892167]] [[1.40335667]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3072 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round 386, Train average loss 0.237 Test accuracy 87.990\n",
      "[[45.594205]] [[44.99488591]] [[1.37796949]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2907 \n",
      "Accuracy: 8849/10000 (88.49%)\n",
      "\n",
      "Round 387, Train average loss 0.237 Test accuracy 88.490\n",
      "[[45.58929748]] [[44.9972735]] [[1.38095354]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2983 \n",
      "Accuracy: 8811/10000 (88.11%)\n",
      "\n",
      "Round 388, Train average loss 0.236 Test accuracy 88.110\n",
      "[[45.5825227]] [[44.98718493]] [[1.37412794]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3062 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round 389, Train average loss 0.236 Test accuracy 87.880\n",
      "[[45.56467587]] [[44.98295631]] [[1.36296676]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2873 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round 390, Train average loss 0.237 Test accuracy 88.610\n",
      "[[45.57767313]] [[44.97462564]] [[1.36786847]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2869 \n",
      "Accuracy: 8863/10000 (88.63%)\n",
      "\n",
      "Round 391, Train average loss 0.236 Test accuracy 88.630\n",
      "[[45.58261586]] [[44.96358818]] [[1.37002506]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2969 \n",
      "Accuracy: 8821/10000 (88.21%)\n",
      "\n",
      "Round 392, Train average loss 0.236 Test accuracy 88.210\n",
      "[[45.55451479]] [[45.04906228]] [[1.40590279]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2881 \n",
      "Accuracy: 8843/10000 (88.43%)\n",
      "\n",
      "Round 393, Train average loss 0.236 Test accuracy 88.430\n",
      "[[45.55271438]] [[45.03400617]] [[1.37995543]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2869 \n",
      "Accuracy: 8858/10000 (88.58%)\n",
      "\n",
      "Round 394, Train average loss 0.235 Test accuracy 88.580\n",
      "[[45.56145654]] [[45.04194568]] [[1.3441183]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2947 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round 395, Train average loss 0.236 Test accuracy 88.640\n",
      "[[45.54746774]] [[45.03899732]] [[1.33455504]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2879 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round 396, Train average loss 0.236 Test accuracy 88.510\n",
      "[[45.52205981]] [[45.06403784]] [[1.32698393]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2960 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "Round 397, Train average loss 0.235 Test accuracy 88.240\n",
      "[[45.49089888]] [[45.07396868]] [[1.31291495]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3122 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round 398, Train average loss 0.235 Test accuracy 87.970\n",
      "[[45.5066674]] [[45.07352378]] [[1.3098533]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2927 \n",
      "Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "Round 399, Train average loss 0.236 Test accuracy 88.410\n",
      "[[45.48138806]] [[45.06264492]] [[1.30405658]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3148 \n",
      "Accuracy: 8765/10000 (87.65%)\n",
      "\n",
      "Round 400, Train average loss 0.236 Test accuracy 87.650\n",
      "[[45.46981792]] [[45.09035491]] [[1.29237376]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3165 \n",
      "Accuracy: 8755/10000 (87.55%)\n",
      "\n",
      "Round 401, Train average loss 0.236 Test accuracy 87.550\n",
      "[[45.46416967]] [[45.10174772]] [[1.28650189]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2929 \n",
      "Accuracy: 8833/10000 (88.33%)\n",
      "\n",
      "Round 402, Train average loss 0.239 Test accuracy 88.330\n",
      "[[45.49267262]] [[45.11138772]] [[1.26666108]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2980 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "Round 403, Train average loss 0.236 Test accuracy 88.270\n",
      "[[45.47352604]] [[45.11296424]] [[1.27211569]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3002 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 404, Train average loss 0.236 Test accuracy 88.310\n",
      "[[45.45303298]] [[45.11021286]] [[1.25449626]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3078 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "Round 405, Train average loss 0.236 Test accuracy 87.900\n",
      "[[45.45463575]] [[45.11995568]] [[1.25317874]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3018 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round 406, Train average loss 0.236 Test accuracy 88.060\n",
      "[[45.44856213]] [[45.11893538]] [[1.24143215]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3029 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round 407, Train average loss 0.236 Test accuracy 88.040\n",
      "[[45.42974879]] [[45.15105336]] [[1.24275131]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2903 \n",
      "Accuracy: 8860/10000 (88.60%)\n",
      "\n",
      "Round 408, Train average loss 0.236 Test accuracy 88.600\n",
      "[[45.4266261]] [[45.14707142]] [[1.23744291]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3063 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round 409, Train average loss 0.235 Test accuracy 87.930\n",
      "[[45.43896644]] [[45.14451001]] [[1.22706942]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2881 \n",
      "Accuracy: 8848/10000 (88.48%)\n",
      "\n",
      "Round 410, Train average loss 0.236 Test accuracy 88.480\n",
      "[[45.45545389]] [[45.13988648]] [[1.22728392]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2928 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round 411, Train average loss 0.235 Test accuracy 88.300\n",
      "[[45.43187917]] [[45.1197931]] [[1.22640367]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2830 \n",
      "Accuracy: 8877/10000 (88.77%)\n",
      "\n",
      "Round 412, Train average loss 0.235 Test accuracy 88.770\n",
      "[[45.4276877]] [[45.13329505]] [[1.20914143]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3155 \n",
      "Accuracy: 8756/10000 (87.56%)\n",
      "\n",
      "Round 413, Train average loss 0.235 Test accuracy 87.560\n",
      "[[45.43179081]] [[45.16570351]] [[1.2016183]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3200 \n",
      "Accuracy: 8752/10000 (87.52%)\n",
      "\n",
      "Round 414, Train average loss 0.236 Test accuracy 87.520\n",
      "[[45.4286287]] [[45.17070192]] [[1.19854141]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2903 \n",
      "Accuracy: 8849/10000 (88.49%)\n",
      "\n",
      "Round 415, Train average loss 0.236 Test accuracy 88.490\n",
      "[[45.42315389]] [[45.15254765]] [[1.18500722]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2981 \n",
      "Accuracy: 8832/10000 (88.32%)\n",
      "\n",
      "Round 416, Train average loss 0.235 Test accuracy 88.320\n",
      "[[45.40943004]] [[45.15232725]] [[1.17823512]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3052 \n",
      "Accuracy: 8812/10000 (88.12%)\n",
      "\n",
      "Round 417, Train average loss 0.235 Test accuracy 88.120\n",
      "[[45.39343795]] [[45.15171735]] [[1.17495258]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3011 \n",
      "Accuracy: 8834/10000 (88.34%)\n",
      "\n",
      "Round 418, Train average loss 0.236 Test accuracy 88.340\n",
      "[[45.38786212]] [[45.1567826]] [[1.16749017]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3212 \n",
      "Accuracy: 8751/10000 (87.51%)\n",
      "\n",
      "Round 419, Train average loss 0.236 Test accuracy 87.510\n",
      "[[45.3846557]] [[45.15567768]] [[1.17422281]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2902 \n",
      "Accuracy: 8859/10000 (88.59%)\n",
      "\n",
      "Round 420, Train average loss 0.236 Test accuracy 88.590\n",
      "[[45.38003474]] [[45.16802443]] [[1.16112252]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2940 \n",
      "Accuracy: 8850/10000 (88.50%)\n",
      "\n",
      "Round 421, Train average loss 0.235 Test accuracy 88.500\n",
      "[[45.37154014]] [[45.14967444]] [[1.15713259]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3176 \n",
      "Accuracy: 8771/10000 (87.71%)\n",
      "\n",
      "Round 422, Train average loss 0.235 Test accuracy 87.710\n",
      "[[45.35896668]] [[45.18390573]] [[1.15399882]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2975 \n",
      "Accuracy: 8837/10000 (88.37%)\n",
      "\n",
      "Round 423, Train average loss 0.236 Test accuracy 88.370\n",
      "[[45.35664828]] [[45.18891895]] [[1.15204529]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3002 \n",
      "Accuracy: 8825/10000 (88.25%)\n",
      "\n",
      "Round 424, Train average loss 0.235 Test accuracy 88.250\n",
      "[[45.34812323]] [[45.20132602]] [[1.15404311]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2821 \n",
      "Accuracy: 8882/10000 (88.82%)\n",
      "\n",
      "Round 425, Train average loss 0.236 Test accuracy 88.820\n",
      "[[45.35400805]] [[45.2151373]] [[1.14615088]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2919 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 426, Train average loss 0.235 Test accuracy 88.310\n",
      "[[45.3316782]] [[45.22998283]] [[1.13990505]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2930 \n",
      "Accuracy: 8852/10000 (88.52%)\n",
      "\n",
      "Round 427, Train average loss 0.235 Test accuracy 88.520\n",
      "[[45.30882934]] [[45.24483988]] [[1.13136805]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2925 \n",
      "Accuracy: 8843/10000 (88.43%)\n",
      "\n",
      "Round 428, Train average loss 0.235 Test accuracy 88.430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.30205099]] [[45.26107422]] [[1.13998855]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2988 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round 429, Train average loss 0.234 Test accuracy 88.260\n",
      "[[45.26351558]] [[45.23313507]] [[1.12099201]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3097 \n",
      "Accuracy: 8777/10000 (87.77%)\n",
      "\n",
      "Round 430, Train average loss 0.235 Test accuracy 87.770\n",
      "[[45.27672025]] [[45.21509054]] [[1.11960784]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2878 \n",
      "Accuracy: 8860/10000 (88.60%)\n",
      "\n",
      "Round 431, Train average loss 0.236 Test accuracy 88.600\n",
      "[[45.28081717]] [[45.22719988]] [[1.11131291]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2830 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "Round 432, Train average loss 0.235 Test accuracy 88.620\n",
      "[[45.29518946]] [[45.21952426]] [[1.0975378]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2809 \n",
      "Accuracy: 8877/10000 (88.77%)\n",
      "\n",
      "Round 433, Train average loss 0.234 Test accuracy 88.770\n",
      "[[45.29344047]] [[45.19929062]] [[1.09572691]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2897 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "Round 434, Train average loss 0.235 Test accuracy 88.620\n",
      "[[45.30187859]] [[45.19973012]] [[1.09455602]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2838 \n",
      "Accuracy: 8868/10000 (88.68%)\n",
      "\n",
      "Round 435, Train average loss 0.235 Test accuracy 88.680\n",
      "[[45.29572884]] [[45.23294019]] [[1.09702926]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2814 \n",
      "Accuracy: 8876/10000 (88.76%)\n",
      "\n",
      "Round 436, Train average loss 0.234 Test accuracy 88.760\n",
      "[[45.30478398]] [[45.21007502]] [[1.09159579]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2818 \n",
      "Accuracy: 8869/10000 (88.69%)\n",
      "\n",
      "Round 437, Train average loss 0.234 Test accuracy 88.690\n",
      "[[45.29795238]] [[45.18242293]] [[1.08494596]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2957 \n",
      "Accuracy: 8820/10000 (88.20%)\n",
      "\n",
      "Round 438, Train average loss 0.234 Test accuracy 88.200\n",
      "[[45.26899801]] [[45.18081083]] [[1.08012936]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2997 \n",
      "Accuracy: 8820/10000 (88.20%)\n",
      "\n",
      "Round 439, Train average loss 0.235 Test accuracy 88.200\n",
      "[[45.25289466]] [[45.20399104]] [[1.08088655]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3223 \n",
      "Accuracy: 8778/10000 (87.78%)\n",
      "\n",
      "Round 440, Train average loss 0.235 Test accuracy 87.780\n",
      "[[45.2516952]] [[45.20507092]] [[1.07291756]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3037 \n",
      "Accuracy: 8821/10000 (88.21%)\n",
      "\n",
      "Round 441, Train average loss 0.235 Test accuracy 88.210\n",
      "[[45.23426669]] [[45.20715423]] [[1.07124605]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3012 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round 442, Train average loss 0.235 Test accuracy 88.260\n",
      "[[45.21199546]] [[45.20374513]] [[1.0636394]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3172 \n",
      "Accuracy: 8767/10000 (87.67%)\n",
      "\n",
      "Round 443, Train average loss 0.235 Test accuracy 87.670\n",
      "[[45.22125606]] [[45.18762997]] [[1.06590566]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2944 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round 444, Train average loss 0.236 Test accuracy 88.380\n",
      "[[45.22427498]] [[45.16809366]] [[1.06589576]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2950 \n",
      "Accuracy: 8823/10000 (88.23%)\n",
      "\n",
      "Round 445, Train average loss 0.235 Test accuracy 88.230\n",
      "[[45.22559411]] [[45.16896257]] [[1.05928056]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2886 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round 446, Train average loss 0.235 Test accuracy 88.380\n",
      "[[45.22099935]] [[45.14757166]] [[1.05385348]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2780 \n",
      "Accuracy: 8884/10000 (88.84%)\n",
      "\n",
      "Round 447, Train average loss 0.234 Test accuracy 88.840\n",
      "[[45.22551012]] [[45.14502994]] [[1.05035218]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2901 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round 448, Train average loss 0.234 Test accuracy 88.650\n",
      "[[45.18365255]] [[45.14270269]] [[1.05400015]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3066 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 449, Train average loss 0.234 Test accuracy 88.310\n",
      "[[45.15268003]] [[45.14800838]] [[1.04342624]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2907 \n",
      "Accuracy: 8852/10000 (88.52%)\n",
      "\n",
      "Round 450, Train average loss 0.235 Test accuracy 88.520\n",
      "[[45.16148595]] [[45.16066492]] [[1.03673222]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2862 \n",
      "Accuracy: 8856/10000 (88.56%)\n",
      "\n",
      "Round 451, Train average loss 0.234 Test accuracy 88.560\n",
      "[[45.14730237]] [[45.15963431]] [[1.03182463]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2913 \n",
      "Accuracy: 8857/10000 (88.57%)\n",
      "\n",
      "Round 452, Train average loss 0.234 Test accuracy 88.570\n",
      "[[45.1148314]] [[45.17403744]] [[1.02574719]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2960 \n",
      "Accuracy: 8810/10000 (88.10%)\n",
      "\n",
      "Round 453, Train average loss 0.234 Test accuracy 88.100\n",
      "[[45.11337512]] [[45.15158072]] [[1.01437135]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2941 \n",
      "Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "Round 454, Train average loss 0.235 Test accuracy 88.410\n",
      "[[45.11092457]] [[45.14000539]] [[1.01201459]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2949 \n",
      "Accuracy: 8839/10000 (88.39%)\n",
      "\n",
      "Round 455, Train average loss 0.234 Test accuracy 88.390\n",
      "[[45.11070319]] [[45.11210106]] [[1.00491127]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3167 \n",
      "Accuracy: 8779/10000 (87.79%)\n",
      "\n",
      "Round 456, Train average loss 0.234 Test accuracy 87.790\n",
      "[[45.10238324]] [[45.09627299]] [[1.01452051]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2884 \n",
      "Accuracy: 8853/10000 (88.53%)\n",
      "\n",
      "Round 457, Train average loss 0.235 Test accuracy 88.530\n",
      "[[45.09808645]] [[45.09354044]] [[1.00748953]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2884 \n",
      "Accuracy: 8858/10000 (88.58%)\n",
      "\n",
      "Round 458, Train average loss 0.234 Test accuracy 88.580\n",
      "[[45.07891237]] [[45.05362415]] [[1.00012503]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2883 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round 459, Train average loss 0.234 Test accuracy 88.660\n",
      "[[45.06942261]] [[45.04399285]] [[0.99973642]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3033 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round 460, Train average loss 0.234 Test accuracy 87.840\n",
      "[[45.05961246]] [[45.05570744]] [[0.98837535]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2982 \n",
      "Accuracy: 8836/10000 (88.36%)\n",
      "\n",
      "Round 461, Train average loss 0.234 Test accuracy 88.360\n",
      "[[45.03581262]] [[45.06253548]] [[0.98434105]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3176 \n",
      "Accuracy: 8747/10000 (87.47%)\n",
      "\n",
      "Round 462, Train average loss 0.234 Test accuracy 87.470\n",
      "[[45.03348365]] [[45.0488187]] [[0.98799993]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2942 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 463, Train average loss 0.236 Test accuracy 88.310\n",
      "[[45.05826783]] [[45.04649829]] [[0.98428635]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2992 \n",
      "Accuracy: 8818/10000 (88.18%)\n",
      "\n",
      "Round 464, Train average loss 0.234 Test accuracy 88.180\n",
      "[[45.03790753]] [[45.03995836]] [[0.98631078]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2819 \n",
      "Accuracy: 8874/10000 (88.74%)\n",
      "\n",
      "Round 465, Train average loss 0.234 Test accuracy 88.740\n",
      "[[45.02073933]] [[45.04512941]] [[0.96846646]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2914 \n",
      "Accuracy: 8835/10000 (88.35%)\n",
      "\n",
      "Round 466, Train average loss 0.233 Test accuracy 88.350\n",
      "[[44.99844689]] [[45.03827218]] [[0.96541639]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3108 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round 467, Train average loss 0.234 Test accuracy 87.880\n",
      "[[44.99235331]] [[45.02783445]] [[0.96616257]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3190 \n",
      "Accuracy: 8759/10000 (87.59%)\n",
      "\n",
      "Round 468, Train average loss 0.234 Test accuracy 87.590\n",
      "[[44.99164972]] [[45.02887699]] [[0.97205934]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3066 \n",
      "Accuracy: 8785/10000 (87.85%)\n",
      "\n",
      "Round 469, Train average loss 0.235 Test accuracy 87.850\n",
      "[[44.9643641]] [[45.02733049]] [[0.95945739]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3161 \n",
      "Accuracy: 8759/10000 (87.59%)\n",
      "\n",
      "Round 470, Train average loss 0.235 Test accuracy 87.590\n",
      "[[44.94223218]] [[45.04127949]] [[0.95879306]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3088 \n",
      "Accuracy: 8770/10000 (87.70%)\n",
      "\n",
      "Round 471, Train average loss 0.234 Test accuracy 87.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.91619689]] [[45.03936615]] [[0.9570758]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3424 \n",
      "Accuracy: 8665/10000 (86.65%)\n",
      "\n",
      "Round 472, Train average loss 0.234 Test accuracy 86.650\n",
      "[[44.88476801]] [[45.05026106]] [[0.96076998]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3041 \n",
      "Accuracy: 8786/10000 (87.86%)\n",
      "\n",
      "Round 473, Train average loss 0.236 Test accuracy 87.860\n",
      "[[44.8747458]] [[45.04394201]] [[0.94334118]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2909 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "Round 474, Train average loss 0.234 Test accuracy 88.270\n",
      "[[44.87735253]] [[45.05630484]] [[0.94012943]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2822 \n",
      "Accuracy: 8868/10000 (88.68%)\n",
      "\n",
      "Round 475, Train average loss 0.233 Test accuracy 88.680\n",
      "[[44.87026608]] [[45.05008241]] [[0.93940494]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2842 \n",
      "Accuracy: 8858/10000 (88.58%)\n",
      "\n",
      "Round 476, Train average loss 0.233 Test accuracy 88.580\n",
      "[[44.85437883]] [[45.03690192]] [[0.93387942]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2935 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "Round 477, Train average loss 0.233 Test accuracy 88.310\n",
      "[[44.82936668]] [[45.04642454]] [[0.92724167]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2979 \n",
      "Accuracy: 8843/10000 (88.43%)\n",
      "\n",
      "Round 478, Train average loss 0.234 Test accuracy 88.430\n",
      "[[44.82678668]] [[45.026097]] [[0.92626817]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2807 \n",
      "Accuracy: 8871/10000 (88.71%)\n",
      "\n",
      "Round 479, Train average loss 0.234 Test accuracy 88.710\n",
      "[[44.81703889]] [[45.02133879]] [[0.92487319]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2862 \n",
      "Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "Round 480, Train average loss 0.233 Test accuracy 88.410\n",
      "[[44.79818333]] [[45.03219725]] [[0.92145915]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2838 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round 481, Train average loss 0.233 Test accuracy 88.660\n",
      "[[44.77416348]] [[45.02707664]] [[0.9198598]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3187 \n",
      "Accuracy: 8749/10000 (87.49%)\n",
      "\n",
      "Round 482, Train average loss 0.233 Test accuracy 87.490\n",
      "[[44.76600355]] [[45.02035445]] [[0.91934257]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3215 \n",
      "Accuracy: 8765/10000 (87.65%)\n",
      "\n",
      "Round 483, Train average loss 0.235 Test accuracy 87.650\n",
      "[[44.73823142]] [[45.00631441]] [[0.92074977]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2968 \n",
      "Accuracy: 8816/10000 (88.16%)\n",
      "\n",
      "Round 484, Train average loss 0.235 Test accuracy 88.160\n",
      "[[44.73029585]] [[45.00700909]] [[0.91442665]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2976 \n",
      "Accuracy: 8828/10000 (88.28%)\n",
      "\n",
      "Round 485, Train average loss 0.233 Test accuracy 88.280\n",
      "[[44.73391582]] [[45.01548238]] [[0.9176009]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2836 \n",
      "Accuracy: 8871/10000 (88.71%)\n",
      "\n",
      "Round 486, Train average loss 0.234 Test accuracy 88.710\n",
      "[[44.73006955]] [[45.02369288]] [[0.91334725]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2962 \n",
      "Accuracy: 8819/10000 (88.19%)\n",
      "\n",
      "Round 487, Train average loss 0.233 Test accuracy 88.190\n",
      "[[44.71259187]] [[45.01338863]] [[0.91564151]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2970 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round 488, Train average loss 0.234 Test accuracy 88.300\n",
      "[[44.71069419]] [[45.0702154]] [[0.94141587]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2927 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "Round 489, Train average loss 0.234 Test accuracy 88.240\n",
      "[[44.71254197]] [[45.05704443]] [[0.929441]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2995 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round 490, Train average loss 0.233 Test accuracy 88.020\n",
      "[[44.68718429]] [[44.92318131]] [[1.0642702]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3172 \n",
      "Accuracy: 8749/10000 (87.49%)\n",
      "\n",
      "Round 491, Train average loss 0.234 Test accuracy 87.490\n",
      "[[44.6620965]] [[44.92945814]] [[1.05580627]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4057 \n",
      "Accuracy: 8469/10000 (84.69%)\n",
      "\n",
      "Round 492, Train average loss 0.236 Test accuracy 84.690\n",
      "[[44.68604]] [[44.95277161]] [[1.02562003]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3257 \n",
      "Accuracy: 8715/10000 (87.15%)\n",
      "\n",
      "Round 493, Train average loss 0.269 Test accuracy 87.150\n",
      "[[44.68648042]] [[44.9339762]] [[1.01438781]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4711 \n",
      "Accuracy: 8297/10000 (82.97%)\n",
      "\n",
      "Round 494, Train average loss 0.242 Test accuracy 82.970\n",
      "[[44.70765489]] [[45.01836513]] [[1.04757491]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.4209 \n",
      "Accuracy: 8456/10000 (84.56%)\n",
      "\n",
      "Round 495, Train average loss 0.259 Test accuracy 84.560\n",
      "[[44.70431054]] [[44.99467951]] [[1.06293684]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3134 \n",
      "Accuracy: 8774/10000 (87.74%)\n",
      "\n",
      "Round 496, Train average loss 0.243 Test accuracy 87.740\n",
      "[[44.68134454]] [[45.03042642]] [[1.01208156]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3154 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round 497, Train average loss 0.236 Test accuracy 87.880\n",
      "[[44.66145165]] [[45.0757259]] [[1.01239503]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2809 \n",
      "Accuracy: 8875/10000 (88.75%)\n",
      "\n",
      "Round 498, Train average loss 0.236 Test accuracy 88.750\n",
      "[[44.66584017]] [[45.11874868]] [[0.96797156]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3154 \n",
      "Accuracy: 8741/10000 (87.41%)\n",
      "\n",
      "Round 499, Train average loss 0.234 Test accuracy 87.410\n"
     ]
    }
   ],
   "source": [
    "from models.Nets import *\n",
    "import pickle\n",
    "\n",
    "p = 0\n",
    "N = 40\n",
    "K = 8\n",
    "\n",
    "N_trials = 1\n",
    "Max_iter = 500\n",
    "\n",
    "lr_array = [0.03]\n",
    "\n",
    "acc_test_arr  = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "loss_test_arr = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "\n",
    "\n",
    "\n",
    "P_random = []\n",
    "\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "\n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print()\n",
    "        print('Learning Rate =',args.lr)\n",
    "        print()\n",
    "        net_glob = CNNMnist2(args)\n",
    "        net_glob = net_glob.cuda()\n",
    "        print(net_glob)\n",
    "\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "        \n",
    "#         w_glob_array = []\n",
    "#         w_locals_array = []\n",
    "        \n",
    "        w_locals_array_np = np.zeros((Max_iter,N,d))\n",
    "        w_glob_array_np = np.zeros((Max_iter,d))\n",
    "        \n",
    "        for iter in range(Max_iter): #args.epochs\n",
    "            \n",
    "#             if iter >= 200:\n",
    "#                 args.lr = lr_array[lr_idx] * 0.1\n",
    "#             elif iter >= 300:\n",
    "#                 args.lr = lr_array[lr_idx] * 0.01\n",
    "            \n",
    "            w_locals, loss_locals = [], []\n",
    "            w_locals_all = []\n",
    "            \n",
    "# #             u = np.random.binomial(1, 1-p, size=(N))\n",
    "#             u = np.ones((N,))\n",
    "#             for u_idx in range(N):\n",
    "#                 p_sel = p_per_user[u_idx]\n",
    "#                 u[u_idx] = np.random.binomial(1, 1-p_sel, size=1)[0]\n",
    "            \n",
    "#             result = np.where(u == 1)\n",
    "\n",
    "            ###############################\n",
    "            # 1. Random Selection\n",
    "            ###############################\n",
    "            idxs_users = np.random.choice(N, K, replace=False)\n",
    "\n",
    "            p_tmp = np.zeros(N)\n",
    "            p_tmp[idxs_users] = 1\n",
    "\n",
    "            P_random.append(p_tmp)\n",
    "\n",
    "#             print('Learning Rate =',args.lr)\n",
    "        #     idxs_users = np.random.choice(range(N), K, replace=False)\n",
    "            for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "                \n",
    "                w_locals_all.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "                \n",
    "                if idx in idxs_users:\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    \n",
    "                stt_pos = 0\n",
    "                for k in w.keys():\n",
    "                    tmp1 = w[k].cpu().detach().numpy()\n",
    "                    cur_shape = tmp1.shape\n",
    "                    _d = np.prod(cur_shape)\n",
    "\n",
    "                    end_pos = stt_pos + _d\n",
    "\n",
    "#                     w_glob_array_np[iter,stt_pos:end_pos] = np.reshape(tmp1,(_d,))        \n",
    "\n",
    "                        \n",
    "                    w_locals_array_np[iter,idx,stt_pos:end_pos] = np.reshape(tmp1,(_d,))\n",
    "\n",
    "                    stt_pos = end_pos\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            # update global weights\n",
    "            w_glob = FedAvg(w_locals)\n",
    "            \n",
    "            \n",
    "            stt_pos = 0\n",
    "            for k in w_glob.keys():\n",
    "                tmp2 = w_glob[k].cpu().detach().numpy()\n",
    "                cur_shape = tmp2.shape\n",
    "                _d = np.prod(cur_shape)\n",
    "\n",
    "                end_pos = stt_pos + _d\n",
    "                \n",
    "#                 print(_d, stt_pose, end_pos)\n",
    "\n",
    "                w_glob_array_np[iter,stt_pos:end_pos] = np.reshape(tmp2,(_d,))\n",
    "\n",
    "                stt_pos = end_pos\n",
    "            \n",
    "            \n",
    "#             w_locals_array.append(w_locals_all)\n",
    "            w_glob_array.append(w_glob)\n",
    "            \n",
    "            ModelDiff_tensor(net_glob.state_dict(), w_glob_array[iter])     \n",
    "            \n",
    "            # copy weight to net_glob\n",
    "            if iter < 500:\n",
    "                print('net_glob is updated !!')\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "#             else:\n",
    "#                 net_glob.load_state_dict(w_glob_prev)\n",
    "\n",
    "            # print loss\n",
    "            loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "\n",
    "    #         loss_train.append(loss_avg)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test_arr[lr_idx][trial_idx][iter]  = acc_test\n",
    "            loss_test_arr[lr_idx][trial_idx][iter] = loss_test\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Train average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "            #print(loss_train)\n",
    "            \n",
    "            if iter % 100 == 99:\n",
    "                PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(iter)\n",
    "                torch.save(net_glob.state_dict(), PATH)\n",
    "            \n",
    "# filehandler = open(\"./save_models/MNIST_CNN_N40_K8_w_locals_array_Maxiter\"+str(Max_iter),\"wb\")\n",
    "# pickle.dump(w_locals_array, filehandler)\n",
    "\n",
    "# filehandler = open(\"./save_models/MNIST_CNN_N40_K8_w_glob_array_Maxiter\"+str(Max_iter),\"wb\")\n",
    "# pickle.dump(w_glob_array, filehandler)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 40)\n"
     ]
    }
   ],
   "source": [
    "P_random = np.array(P_random)\n",
    "\n",
    "print(np.shape(P_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 40)\n",
      "40\n",
      "[[45.11347587]] [[57.56493237]] [[9.88459414]]\n",
      "[[45.06166125]] [[46.08330693]] [[1.23249458]]\n",
      "[[45.08332323]] [[89.32356478]] [[48.78044242]]\n",
      "[[45.10018486]] [[97.05632773]] [[56.46577322]]\n",
      "[[45.13946952]] [[83.39793165]] [[39.19530792]]\n",
      "[[45.1682111]] [[53.54351855]] [[8.98731686]]\n",
      "[[45.06014484]] [[123.5963247]] [[77.08259569]]\n",
      "[[44.98212697]] [[101.50533234]] [[57.13177715]]\n",
      "[[44.95527049]] [[83.03442767]] [[38.44616281]]\n",
      "[[45.09457055]] [[56.80745852]] [[10.12496583]]\n",
      "[[45.07276209]] [[112.91740201]] [[66.1472108]]\n",
      "[[45.08603459]] [[52.29430586]] [[7.15653113]]\n",
      "[[45.08523449]] [[116.8727432]] [[72.0502964]]\n",
      "[[45.05599313]] [[95.9279887]] [[51.05106646]]\n",
      "[[45.07246777]] [[127.86338462]] [[81.07367719]]\n",
      "[[44.97584087]] [[221.34518598]] [[175.54092925]]\n",
      "[[45.07988807]] [[74.79157128]] [[30.33736644]]\n",
      "[[45.08386066]] [[73.23700307]] [[26.79661748]]\n",
      "[[45.07860319]] [[115.61449094]] [[70.5827577]]\n",
      "[[45.24767515]] [[441.59741567]] [[393.38272998]]\n",
      "[[45.15482003]] [[108.8557017]] [[62.87652352]]\n",
      "[[45.08454384]] [[184.22441986]] [[135.39377068]]\n",
      "[[45.08509784]] [[118.62982996]] [[68.86090077]]\n",
      "[[45.09005308]] [[224.02412209]] [[183.10950312]]\n",
      "[[45.12845181]] [[353.83913321]] [[312.59302344]]\n",
      "[[45.08677236]] [[59.0936807]] [[17.08055584]]\n",
      "[[45.05476533]] [[173.25254556]] [[126.39082668]]\n",
      "[[45.12778928]] [[90.56072654]] [[44.04887541]]\n",
      "[[45.09206599]] [[199.01762836]] [[152.62399119]]\n",
      "[[45.07345256]] [[171.75633832]] [[128.83790091]]\n",
      "[[45.20141045]] [[57.99295054]] [[13.51974214]]\n",
      "[[45.11038931]] [[310.20491515]] [[263.62171855]]\n",
      "[[45.11529476]] [[86.08787924]] [[43.67999692]]\n",
      "[[45.12875279]] [[218.13414537]] [[177.21790596]]\n",
      "[[45.11268168]] [[229.02908445]] [[184.56710189]]\n",
      "[[45.08429093]] [[63.27762001]] [[18.55356324]]\n",
      "[[45.09911681]] [[55.88765414]] [[12.02408629]]\n",
      "[[44.97755833]] [[91.21754145]] [[46.65791014]]\n",
      "[[45.12946309]] [[344.49783273]] [[300.46916414]]\n",
      "\n",
      "\n",
      "[[45.11347587]] [[79.68385052]] [[31.45840292]]\n",
      "[[45.06166125]] [[53.38861556]] [[7.00689067]]\n",
      "[[45.08332323]] [[46.85053394]] [[3.43991649]]\n",
      "[[45.10018486]] [[192.34566005]] [[154.5693678]]\n",
      "[[45.13946952]] [[66.47080519]] [[22.72459794]]\n",
      "[[45.1682111]] [[107.21521355]] [[65.71266955]]\n",
      "[[45.06014484]] [[129.51935377]] [[83.34935978]]\n",
      "[[44.98212697]] [[121.13494076]] [[74.48352704]]\n",
      "[[44.95527049]] [[126.84708835]] [[84.65429536]]\n",
      "[[45.09457055]] [[92.36897888]] [[44.84149339]]\n",
      "[[45.07276209]] [[85.13017616]] [[39.07256878]]\n",
      "[[45.08603459]] [[148.8712368]] [[101.41687263]]\n",
      "[[45.08523449]] [[269.45416969]] [[226.29030537]]\n",
      "[[45.05599313]] [[56.66083977]] [[10.15702055]]\n",
      "[[45.07246777]] [[180.58428081]] [[138.58302458]]\n",
      "[[44.97584087]] [[874.56094766]] [[825.53801079]]\n",
      "[[45.07988807]] [[56.96571193]] [[14.20968862]]\n",
      "[[45.08386066]] [[131.22797413]] [[84.68676818]]\n",
      "[[45.07860319]] [[49.36205354]] [[3.01888537]]\n",
      "[[45.24767515]] [[292.82956318]] [[245.6225703]]\n",
      "[[45.15482003]] [[626.27927768]] [[579.11831362]]\n",
      "[[45.08454384]] [[120.05451212]] [[73.28822895]]\n",
      "[[45.08509784]] [[272.23002571]] [[219.28879825]]\n",
      "[[45.09005308]] [[50.18575882]] [[3.85575062]]\n",
      "[[45.12845181]] [[869.95767033]] [[830.09560976]]\n",
      "[[45.08677236]] [[95.36231154]] [[52.98103865]]\n",
      "[[45.05476533]] [[58.60204468]] [[16.85136168]]\n",
      "[[45.12778928]] [[558.39889249]] [[511.53855587]]\n",
      "[[45.09206599]] [[162.27315168]] [[116.909843]]\n",
      "[[45.07345256]] [[245.31900956]] [[200.55369085]]\n",
      "[[45.20141045]] [[152.89930343]] [[108.24036356]]\n",
      "[[45.11038931]] [[261.04493128]] [[214.6964976]]\n",
      "[[45.11529476]] [[365.95984279]] [[320.88622841]]\n",
      "[[45.12875279]] [[126.23788826]] [[85.83392286]]\n",
      "[[45.11268168]] [[421.09585027]] [[379.52133167]]\n",
      "[[45.08429093]] [[120.14250857]] [[71.93926742]]\n",
      "[[45.09911681]] [[299.35934469]] [[257.11009979]]\n",
      "[[44.97755833]] [[132.42918865]] [[86.77639079]]\n",
      "[[45.12946309]] [[567.22796734]] [[523.87823796]]\n",
      "[[45.16373144]] [[188.1243985]] [[143.3405133]]\n",
      "2.002783654694666 3.9121907541018515\n"
     ]
    }
   ],
   "source": [
    "# Pseudo Inversion\n",
    "P_random_tmp = P_random[260:300,:]\n",
    "\n",
    "PT = P_random_tmp.transpose()\n",
    "\n",
    "print(np.shape(PT))\n",
    "\n",
    "PTP = np.matmul(P_random_tmp.transpose(), P_random_tmp)\n",
    "\n",
    "print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "# print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "Pw_glob = np.matmul(PT, w_glob_array_np[260:300,:])\n",
    "w_recon_np = K * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "# ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "l2_diff = np.zeros((N))\n",
    "for i in range(N-1):\n",
    "    l2_diff[i] = ModelDiff_np(w_locals_array_np[260,i,:], (w_recon_np[i]+w_recon_np[i+1])/2)\n",
    "print()\n",
    "print()\n",
    "l2_diff_ = np.zeros((N))\n",
    "for i in range(N):\n",
    "    l2_diff_[i] = ModelDiff_np(w_locals_array_np[260,i,:], w_recon_np[i])\n",
    "\n",
    "print(np.sum(l2_diff)/N, np.sum(l2_diff_)/N)\n",
    "# w_recon_np = K * np.matmul(pinv, w_glob_array_np[10:50,:])\n",
    "# u, s, vh = np.linalg.svd(PTP, full_matrices=True)\n",
    "\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3243 \n",
      "Accuracy: 8773/10000 (87.73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_glob = CNNMnist2(args)\n",
    "net_glob = net_glob.cuda()\n",
    "\n",
    "PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(199)\n",
    "net_glob.load_state_dict(torch.load(PATH))\n",
    "net_glob.eval()\n",
    "\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate = 0.00015\n",
      "\n",
      "[[45.09035491]] [[45.09102049]] [[2.23243627e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3239 \n",
      "Accuracy: 8773/10000 (87.73%)\n",
      "\n",
      "Round   0, Train average loss 0.334 Test accuracy 87.730\n",
      "[[45.09102049]] [[45.09211844]] [[4.51292293e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3229 \n",
      "Accuracy: 8778/10000 (87.78%)\n",
      "\n",
      "Round   1, Train average loss 0.333 Test accuracy 87.780\n",
      "[[45.09211844]] [[45.09283159]] [[5.27015701e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3223 \n",
      "Accuracy: 8783/10000 (87.83%)\n",
      "\n",
      "Round   2, Train average loss 0.333 Test accuracy 87.830\n",
      "[[45.09283159]] [[45.0939073]] [[2.80789472e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3218 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round   3, Train average loss 0.332 Test accuracy 87.840\n",
      "[[45.0939073]] [[45.09424912]] [[2.54625493e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3214 \n",
      "Accuracy: 8786/10000 (87.86%)\n",
      "\n",
      "Round   4, Train average loss 0.331 Test accuracy 87.860\n",
      "[[45.09424912]] [[45.09492057]] [[2.81950511e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3206 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round   5, Train average loss 0.331 Test accuracy 87.840\n",
      "[[45.09492057]] [[45.09556234]] [[2.98985185e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3200 \n",
      "Accuracy: 8786/10000 (87.86%)\n",
      "\n",
      "Round   6, Train average loss 0.330 Test accuracy 87.860\n",
      "[[45.09556234]] [[45.09631695]] [[1.98556387e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3195 \n",
      "Accuracy: 8787/10000 (87.87%)\n",
      "\n",
      "Round   7, Train average loss 0.330 Test accuracy 87.870\n",
      "[[45.09631695]] [[45.09710043]] [[3.10929467e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3192 \n",
      "Accuracy: 8789/10000 (87.89%)\n",
      "\n",
      "Round   8, Train average loss 0.330 Test accuracy 87.890\n",
      "[[45.09710043]] [[45.09786687]] [[5.51958932e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3183 \n",
      "Accuracy: 8789/10000 (87.89%)\n",
      "\n",
      "Round   9, Train average loss 0.329 Test accuracy 87.890\n",
      "[[45.09786687]] [[45.09859711]] [[2.65060525e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3182 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round  10, Train average loss 0.328 Test accuracy 87.880\n",
      "[[45.09859711]] [[45.09935698]] [[4.17537023e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3178 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round  11, Train average loss 0.328 Test accuracy 87.910\n",
      "[[45.09935698]] [[45.09990673]] [[4.58633812e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3173 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  12, Train average loss 0.328 Test accuracy 87.920\n",
      "[[45.09990673]] [[45.1003128]] [[2.19213537e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3168 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "Round  13, Train average loss 0.327 Test accuracy 87.900\n",
      "[[45.1003128]] [[45.10085451]] [[1.21301743e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3166 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round  14, Train average loss 0.327 Test accuracy 87.880\n",
      "[[45.10085451]] [[45.10160275]] [[1.78537505e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3164 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  15, Train average loss 0.327 Test accuracy 87.920\n",
      "[[45.10160275]] [[45.10246794]] [[4.39789688e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3157 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "Round  16, Train average loss 0.327 Test accuracy 87.900\n",
      "[[45.10246794]] [[45.10305939]] [[1.88793554e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3154 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round  17, Train average loss 0.326 Test accuracy 87.880\n",
      "[[45.10305939]] [[45.10395589]] [[2.9447994e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3154 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round  18, Train average loss 0.326 Test accuracy 87.880\n",
      "[[45.10395589]] [[45.10440862]] [[1.17949788e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3151 \n",
      "Accuracy: 8789/10000 (87.89%)\n",
      "\n",
      "Round  19, Train average loss 0.326 Test accuracy 87.890\n",
      "[[45.10440862]] [[45.10493482]] [[1.59861131e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3150 \n",
      "Accuracy: 8787/10000 (87.87%)\n",
      "\n",
      "Round  20, Train average loss 0.326 Test accuracy 87.870\n",
      "[[45.10493482]] [[45.10567535]] [[1.76712338e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3146 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round  21, Train average loss 0.325 Test accuracy 87.880\n",
      "[[45.10567535]] [[45.10651933]] [[1.47155187e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3146 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round  22, Train average loss 0.325 Test accuracy 87.880\n",
      "[[45.10651933]] [[45.10668687]] [[2.49920794e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3144 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round  23, Train average loss 0.325 Test accuracy 87.910\n",
      "[[45.10668687]] [[45.10751448]] [[2.77102145e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3140 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "Round  24, Train average loss 0.325 Test accuracy 87.900\n",
      "[[45.10751448]] [[45.10824159]] [[2.10715036e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3137 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  25, Train average loss 0.325 Test accuracy 87.920\n",
      "[[45.10824159]] [[45.10908615]] [[1.75592298e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3137 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  26, Train average loss 0.324 Test accuracy 87.920\n",
      "[[45.10908615]] [[45.10979948]] [[1.8482394e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3133 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  27, Train average loss 0.324 Test accuracy 87.920\n",
      "[[45.10979948]] [[45.11058072]] [[7.19621025e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3129 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  28, Train average loss 0.324 Test accuracy 87.950\n",
      "[[45.11058072]] [[45.11078117]] [[7.76978078e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3128 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  29, Train average loss 0.324 Test accuracy 87.970\n",
      "[[45.11078117]] [[45.11113649]] [[1.09013371e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3127 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  30, Train average loss 0.324 Test accuracy 87.960\n",
      "[[45.11113649]] [[45.11186917]] [[3.19359078e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3128 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  31, Train average loss 0.324 Test accuracy 87.950\n",
      "[[45.11186917]] [[45.11211684]] [[3.39795372e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3126 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  32, Train average loss 0.324 Test accuracy 87.960\n",
      "[[45.11211684]] [[45.11252042]] [[1.04171393e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3126 \n",
      "Accuracy: 8798/10000 (87.98%)\n",
      "\n",
      "Round  33, Train average loss 0.323 Test accuracy 87.980\n",
      "[[45.11252042]] [[45.11345885]] [[4.51214147e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3125 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "Round  34, Train average loss 0.324 Test accuracy 88.000\n",
      "[[45.11345885]] [[45.11394512]] [[2.36606905e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3124 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  35, Train average loss 0.323 Test accuracy 87.990\n",
      "[[45.11394512]] [[45.11463187]] [[1.59827478e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3121 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  36, Train average loss 0.323 Test accuracy 87.970\n",
      "[[45.11463187]] [[45.11475206]] [[1.45675497e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3122 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  37, Train average loss 0.323 Test accuracy 87.950\n",
      "[[45.11475206]] [[45.11542643]] [[1.12303333e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3120 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  38, Train average loss 0.323 Test accuracy 87.960\n",
      "[[45.11542643]] [[45.11610864]] [[1.88343082e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3120 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  39, Train average loss 0.323 Test accuracy 87.950\n",
      "[[45.11610864]] [[45.11672794]] [[1.13359564e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3120 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "Round  40, Train average loss 0.323 Test accuracy 87.940\n",
      "[[45.11672794]] [[45.11737507]] [[2.36302142e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3118 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "Round  41, Train average loss 0.323 Test accuracy 87.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.11737507]] [[45.11770979]] [[9.99811104e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3118 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "Round  42, Train average loss 0.323 Test accuracy 87.940\n",
      "[[45.11770979]] [[45.11830076]] [[1.08330752e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3118 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "Round  43, Train average loss 0.323 Test accuracy 87.940\n",
      "[[45.11830076]] [[45.11881485]] [[1.31003366e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3117 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round  44, Train average loss 0.323 Test accuracy 87.910\n",
      "[[45.11881485]] [[45.11976075]] [[2.86754093e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3116 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  45, Train average loss 0.323 Test accuracy 87.970\n",
      "[[45.11976075]] [[45.12032562]] [[1.59974322e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3114 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  46, Train average loss 0.323 Test accuracy 87.990\n",
      "[[45.12032562]] [[45.12089896]] [[9.58078476e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3112 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  47, Train average loss 0.322 Test accuracy 87.960\n",
      "[[45.12089896]] [[45.12154064]] [[2.32689274e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3111 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  48, Train average loss 0.322 Test accuracy 87.960\n",
      "[[45.12154064]] [[45.12233199]] [[9.39284416e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3111 \n",
      "Accuracy: 8798/10000 (87.98%)\n",
      "\n",
      "Round  49, Train average loss 0.322 Test accuracy 87.980\n",
      "[[45.12233199]] [[45.12273882]] [[1.01577523e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3110 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  50, Train average loss 0.322 Test accuracy 87.990\n",
      "[[45.12273882]] [[45.12313051]] [[7.31087871e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3110 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  51, Train average loss 0.322 Test accuracy 87.990\n",
      "[[45.12313051]] [[45.12384998]] [[1.81627085e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3108 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "Round  52, Train average loss 0.322 Test accuracy 88.000\n",
      "[[45.12384998]] [[45.12479004]] [[3.15970498e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3107 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round  53, Train average loss 0.322 Test accuracy 88.010\n",
      "[[45.12479004]] [[45.12571617]] [[2.82884806e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3107 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round  54, Train average loss 0.322 Test accuracy 88.010\n",
      "[[45.12571617]] [[45.12671639]] [[1.58666704e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3106 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  55, Train average loss 0.322 Test accuracy 88.020\n",
      "[[45.12671639]] [[45.1277835]] [[2.64529559e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round  56, Train average loss 0.322 Test accuracy 88.010\n",
      "[[45.1277835]] [[45.1284595]] [[2.37121001e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "Round  57, Train average loss 0.322 Test accuracy 88.000\n",
      "[[45.1284595]] [[45.12924123]] [[3.03063693e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round  58, Train average loss 0.322 Test accuracy 88.010\n",
      "[[45.12924123]] [[45.12970216]] [[1.69459299e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3104 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  59, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.12970216]] [[45.1302049]] [[1.1577627e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3104 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  60, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.1302049]] [[45.13095967]] [[4.19879392e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3101 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  61, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13095967]] [[45.1315619]] [[1.04621257e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3099 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  62, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.1315619]] [[45.13164249]] [[2.10921293e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3098 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  63, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13164249]] [[45.13242259]] [[2.10866463e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3096 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  64, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13242259]] [[45.13313378]] [[3.46444475e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  65, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.13313378]] [[45.13359926]] [[2.7851268e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  66, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13359926]] [[45.13411193]] [[1.35868457e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3096 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  67, Train average loss 0.321 Test accuracy 88.060\n",
      "[[45.13411193]] [[45.13431966]] [[1.73693884e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  68, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13431966]] [[45.13466929]] [[9.91727169e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3096 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  69, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13466929]] [[45.13498141]] [[4.85310053e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3096 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  70, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13498141]] [[45.13566819]] [[9.61336708e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  71, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.13566819]] [[45.13619561]] [[3.22090426e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3093 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  72, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.13619561]] [[45.13688478]] [[2.07134144e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3093 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  73, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.13688478]] [[45.13722405]] [[1.9628044e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3093 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  74, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13722405]] [[45.13804745]] [[2.28749725e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  75, Train average loss 0.321 Test accuracy 88.060\n",
      "[[45.13804745]] [[45.13854429]] [[1.46080116e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  76, Train average loss 0.321 Test accuracy 88.060\n",
      "[[45.13854429]] [[45.13893312]] [[1.26457736e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  77, Train average loss 0.321 Test accuracy 88.080\n",
      "[[45.13893312]] [[45.13968384]] [[1.93773393e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  78, Train average loss 0.321 Test accuracy 88.060\n",
      "[[45.13968384]] [[45.13993431]] [[3.43908318e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  79, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.13993431]] [[45.14041155]] [[1.16945239e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  80, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.14041155]] [[45.14099247]] [[1.3883001e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  81, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.14099247]] [[45.14120683]] [[4.11933315e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3088 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  82, Train average loss 0.320 Test accuracy 88.080\n",
      "[[45.14120683]] [[45.14155778]] [[2.20224408e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3087 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  83, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.14155778]] [[45.14209606]] [[2.27751166e-06]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3088 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  84, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.14209606]] [[45.14289966]] [[3.39621282e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3087 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round  85, Train average loss 0.320 Test accuracy 88.090\n",
      "[[45.14289966]] [[45.14359761]] [[2.50126528e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8810/10000 (88.10%)\n",
      "\n",
      "Round  86, Train average loss 0.320 Test accuracy 88.100\n",
      "[[45.14359761]] [[45.14442507]] [[1.47331059e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  87, Train average loss 0.320 Test accuracy 88.080\n",
      "[[45.14442507]] [[45.14499996]] [[1.32232087e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3085 \n",
      "Accuracy: 8810/10000 (88.10%)\n",
      "\n",
      "Round  88, Train average loss 0.320 Test accuracy 88.100\n",
      "[[45.14499996]] [[45.14562984]] [[2.54396649e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round  89, Train average loss 0.320 Test accuracy 88.090\n",
      "[[45.14562984]] [[45.14644137]] [[8.7749848e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3085 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round  90, Train average loss 0.320 Test accuracy 88.090\n",
      "[[45.14644137]] [[45.14699335]] [[1.34867964e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  91, Train average loss 0.320 Test accuracy 88.080\n",
      "[[45.14699335]] [[45.14754495]] [[2.89211171e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3085 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  92, Train average loss 0.320 Test accuracy 88.080\n",
      "[[45.14754495]] [[45.1478533]] [[1.29180199e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3085 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  93, Train average loss 0.320 Test accuracy 88.080\n",
      "[[45.1478533]] [[45.14843485]] [[8.86458743e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3084 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  94, Train average loss 0.320 Test accuracy 88.080\n",
      "[[45.14843485]] [[45.14896334]] [[2.79112813e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3083 \n",
      "Accuracy: 8810/10000 (88.10%)\n",
      "\n",
      "Round  95, Train average loss 0.320 Test accuracy 88.100\n",
      "[[45.14896334]] [[45.14979485]] [[3.36017954e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3083 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  96, Train average loss 0.320 Test accuracy 88.060\n",
      "[[45.14979485]] [[45.15020437]] [[1.6157892e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3083 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  97, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.15020437]] [[45.15089681]] [[3.02398205e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3082 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round  98, Train average loss 0.320 Test accuracy 88.090\n",
      "[[45.15089681]] [[45.15158494]] [[2.46544971e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3081 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round  99, Train average loss 0.320 Test accuracy 88.090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = 0\n",
    "N = 40\n",
    "K = 8\n",
    "\n",
    "args.local_ep=1\n",
    "\n",
    "N_trials = 1\n",
    "Max_iter = 100\n",
    "\n",
    "lr_array = [0.03]\n",
    "\n",
    "acc_test_arr_v2  = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "loss_test_arr_v2 = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "\n",
    "\n",
    "\n",
    "P_random = []\n",
    "\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "\n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        print()\n",
    "        print('Learning Rate =',args.lr)\n",
    "        print()\n",
    "#         net_glob = CNNMnist2(args)\n",
    "#         net_glob = net_glob.cuda()\n",
    "#         print(net_glob)\n",
    "\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "        \n",
    "#         w_glob_array = []\n",
    "#         w_locals_array = []\n",
    "        \n",
    "        w_locals_array_np_v2 = np.zeros((Max_iter,N,d))\n",
    "        w_glob_array_np_v2 = np.zeros((Max_iter,d))\n",
    "        \n",
    "        w_glob_array = []\n",
    "        \n",
    "        for iter in range(Max_iter): #args.epochs\n",
    "            \n",
    "            args.lr = lr_array[lr_idx]/(200)\n",
    "#             if iter >= 200:\n",
    "#                 args.lr = lr_array[lr_idx] * 0.1\n",
    "#             elif iter >= 300:\n",
    "#                 args.lr = lr_array[lr_idx] * 0.01\n",
    "            \n",
    "            w_locals, loss_locals = [], []\n",
    "            w_locals_all = []\n",
    "            \n",
    "# #             u = np.random.binomial(1, 1-p, size=(N))\n",
    "#             u = np.ones((N,))\n",
    "#             for u_idx in range(N):\n",
    "#                 p_sel = p_per_user[u_idx]\n",
    "#                 u[u_idx] = np.random.binomial(1, 1-p_sel, size=1)[0]\n",
    "            \n",
    "#             result = np.where(u == 1)\n",
    "\n",
    "            ###############################\n",
    "            # 1. Random Selection\n",
    "            ###############################\n",
    "            idxs_users = np.random.choice(N, K, replace=False)\n",
    "\n",
    "            p_tmp = np.zeros(N)\n",
    "            p_tmp[idxs_users] = 1\n",
    "\n",
    "            P_random.append(p_tmp)\n",
    "\n",
    "#             print('Learning Rate =',args.lr)\n",
    "        #     idxs_users = np.random.choice(range(N), K, replace=False)\n",
    "            for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "                \n",
    "                w_locals_all.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "                \n",
    "                if idx in idxs_users:\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    \n",
    "                stt_pos = 0\n",
    "                for k in w.keys():\n",
    "                    tmp1 = w[k].cpu().detach().numpy()\n",
    "                    cur_shape = tmp1.shape\n",
    "                    _d = np.prod(cur_shape)\n",
    "\n",
    "                    end_pos = stt_pos + _d\n",
    "\n",
    "#                     w_glob_array_np[iter,stt_pos:end_pos] = np.reshape(tmp1,(_d,))        \n",
    "\n",
    "                        \n",
    "                    w_locals_array_np_v2[iter,idx,stt_pos:end_pos] = np.reshape(tmp1,(_d,))\n",
    "\n",
    "                    stt_pos = end_pos\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            # update global weights\n",
    "            w_glob = FedAvg(w_locals)\n",
    "            \n",
    "            \n",
    "            stt_pos = 0\n",
    "            for k in w_glob.keys():\n",
    "                tmp2 = w_glob[k].cpu().detach().numpy()\n",
    "                cur_shape = tmp2.shape\n",
    "                _d = np.prod(cur_shape)\n",
    "\n",
    "                end_pos = stt_pos + _d\n",
    "                \n",
    "#                 print(_d, stt_pose, end_pos)\n",
    "\n",
    "                w_glob_array_np_v2[iter,stt_pos:end_pos] = np.reshape(tmp2,(_d,))\n",
    "\n",
    "                stt_pos = end_pos\n",
    "            \n",
    "            \n",
    "#             w_locals_array.append(w_locals_all)\n",
    "            w_glob_array.append(w_glob)\n",
    "            \n",
    "            ModelDiff_tensor(net_glob.state_dict(), w_glob_array[iter])     \n",
    "            \n",
    "            # copy weight to net_glob\n",
    "            if iter < 1000:\n",
    "                print('net_glob is updated !!')\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "#             else:\n",
    "#                 net_glob.load_state_dict(w_glob_prev)\n",
    "\n",
    "            # print loss\n",
    "            loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "\n",
    "    #         loss_train.append(loss_avg)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test_arr_v2[lr_idx][trial_idx][iter]  = acc_test\n",
    "            loss_test_arr_v2[lr_idx][trial_idx][iter] = loss_test\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Train average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "            #print(loss_train)\n",
    "            \n",
    "            if iter % 100 == 99:\n",
    "                PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(1400+iter)\n",
    "                torch.save(net_glob.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(acc_test_arr_v2[:,:,0:100],  open('./results/MNIST_nonIID_CNN2_accuracy_Random_N40_K20_iter1400_1500', 'wb'), -1)\n",
    "pickle.dump(loss_test_arr_v2[:,:,0:100], open('./results/MNIST_nonIID_CNN2_loss_Random_N40_K20_iter1400_1500', 'wb'), -1)\n",
    "\n",
    "# pickle.dump(acc_test_arr,  open('./results/MNIST_nonIID_CNN2_accuracy_Random_N40_K20_iter1000_1200', 'wb'), -1)\n",
    "# pickle.dump(loss_test_arr, open('./results/MNIST_nonIID_CNN2_loss_Random_N40_K20_iter1000_1200', 'wb'), -1)\n",
    "\n",
    "# acc_test_arr_pre = pickle.load(open('./results/MNIST_nonIID_CNN2_accuracy_Random_N40_K20_iter500_1000','rb'))\n",
    "# loss_test_arr_pre = pickle.load(open('./results/MNIST_nonIID_CNN2_loss_Random_N40_K20_iter500_1000','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXyc1X3v8c9vNo02S/IK2IDtYBaDNzABJwGb4BBI2NqbhQQoITRk4yYhr+aG2+zcJikkJbc0zkIaIPQSyEohLYQQsEigYbHBBhvb2AVjbANeJVu7ZuZ3/3geSSMxksa2RiNpvu/XSy/Ns838nsNYP845zznH3B0REZG+IsUOQERERiYlCBERyUkJQkREclKCEBGRnJQgREQkp1ixAxgqEydO9OnTpxc7jEPW3NxMZWVlscMYEVQWvak8elN59DiUsli5cuUud5+U69iYSRDTp09nxYoVxQ7jkNXX17NkyZJihzEiqCx6U3n0pvLocShlYWav9HdMTUwiIpKTEoSIiOSkBCEiIjmNmT4IESktZsbLL79MW1tbsUMpupqaGtatWzfgOclkkmnTphGPx/N+XyUIERmVKisrqa6uZvr06ZhZscMpqv3791NdXd3vcXdn9+7dbN26lRkzZuT9vmpiEpFRKRqNMmHChJJPDvkwMyZMmHDAtS0lCBEZtZQc8ncwZVXyCaKpPcVND73Iqlcbih2KiMiIUvIJoiOV4eaHN7JaCUJEDlA0GmX+/PmcdNJJXHDBBTQ0DM3fkc2bN3PSSScNyXsdipJPEPFoUO3qSGWKHImIjDbl5eWsWrWKNWvWMH78eJYtW1bskIZUySeIRCwogo60EoSIHLxFixaxbds2AJqamjj77LM5+eSTmTNnDvfeey8Q1AxOOOEEPvaxj3HiiSdyzjnn0NraCsDKlSuZN28eixYt6pVo2trauPLKK5kzZw4LFixg+fLlANx+++1cfPHFXHDBBcyZM4fvf//73HTTTSxYsIDTTz+dPXv2HPI9lfxjrvFImCBUgxAZtb7xu7W8sH3fkL7n7CPG8bULTszr3HQ6zcMPP8xVV10FBGMO7rnnHsaNG8euXbs4/fTTufDCCwHYuHEjd911Fz/5yU/4wAc+wG9+8xsuu+wyrrzySv7lX/6FxYsX84UvfKH7vbuSxfPPP8/69es555xzePHFFwFYs2YNzz77LLt27WL+/PnccMMNPPvss1x77bXccccdfO5znzukMij5GkQkYsQiRqdqECJygFpbW5k/fz4TJkxgz549vOtd7wKCcQd///d/z9y5c1m6dCnbtm3jjTfeAGDGjBnMnz8fgFNOOYXNmzfT2NhIQ0MDixcvBuDyyy/v/ozHHnuse/v444/n6KOP7k4QZ511FtXV1UycOJGamhouuOACAObMmcPmzZsP+f5KvgYBQTOTEoTI6JXv/+kPta4+iMbGRs4//3yWLVvGZz7zGe6880527tzJypUricfjTJ8+vXsMQllZWff10WiU1tZW3L3fx1Ddvd/Pz36vSCTSvR2JREilUod8fyVfgwCIRyNqYhKRg1ZTU8PNN9/Md7/7XTo7O2lsbGTy5MnE43GWL1/OK6/0O6M2ALW1tdTU1PDYY48BcOedd3YfO/PMM7u3X3zxRbZs2cJxxx1XuJvJogRBUIPoSPefpUVEBrNgwQLmzZvH3XffzaWXXsqKFStYuHAhd955J8cff/yg19922218+tOfZtGiRZSXl3fv/9SnPkU6nWbOnDl88IMf5Pbbb+9VcygkG6j6MposXLjQD3bBoLf/4yOcPnMC//SBeUMc1YHTIig9VBa9qTx6e/bZZ1mwYEGxwxgRBpuLqcu6des44YQTeu0zs5XuvjDX+QWtQZjZuWa2wcw2mdl1OY5/wsyeN7NVZvaYmc0O908ws+Vm1mRm3y9kjKA+CBGRXAqWIMwsCiwDzgNmAx/qSgBZfu7uc9x9PnAjcFO4vw34CvB3hYovWzxq6oMQEemjkDWItwKb3P0ld+8A7gYuyj7B3bMfXK4EPNzf7O6PESSKgotHVYMQGY3GShP5cDiYsirkY65TgVeztrcCp/U9ycw+DXweSADvPJAPMLOrgasBpkyZQn19/UEF2tbcyuvtTQd9/VBqahoZcYwEKoveVB69JZNJtm7dSk1NTcnP6ppOp9m/f3+/x92dxsZGmpubD+g7VMgEkeu/2JtSmLsvA5aZ2YeBLwNX5PsB7n4LcAsEndQH24H3gw1/wYAlSxYd1PVDSR2RPVQWvak8env00UdJpVLd01uUsra2NpLJ5IDnJJNJ5s2bN2JWlNsKHJm1PQ3YPsD5dwM/LGA8/SqLRWhuP/RBJSIyfNz9gFZHG8vq6+sL8kRXIfsgngZmmdkMM0sAlwD3ZZ9gZrOyNt8LbCxgPP2KRyOarE9EpI+C1SDcPWVm1wAPAlHgVndfa2bXAyvc/T7gGjNbCnQCe8lqXjKzzcA4IGFmFwPnuPsLhYg1EY3QmVJnl4hItoLOxeTu9wP399n31azXnx3g2umFi6y3eEw1CBGRvjTVBhoHISKSixIEQSe1ahAiIr0pQaCBciIiuShB0NVJrQQhIpJNCQJ1UouI5KIEQViDSLvmdRERyaIEQTDdN6BahIhIFiUIghoEQKdWlRMR6aYEQTAOAtBYCBGRLEoQBJ3UoAQhIpJNCYJgHASgsRAiIlmUIMjug1CCEBHpogRBdg1CndQiIl2UIOjppFYNQkSkhxIEWZ3UShAiIt2UIOjpg0ipiUlEpJsSBHqKSUQkFyUIsgbKKUGIiHRTgiCrBqGBciIi3ZQg6JmsT4+5ioj0UIJAfRAiIrkoQaA+CBGRXJQg0FQbIiK5KEHQ08Sk2VxFRHooQQAVZVEAWjrSRY5ERGTkUIIAymJRErEI+9o6ix2KiMiIoQQRqi6Lsb8tVewwRERGDCWIUHVSCUJEJJsSRKg6GWe/mphERLopQYSqymI0qQYhItJNCSJUnojSltJTTCIiXZQgQsl4hLZOjYMQEemiBBFKxqK0daoGISLSRQkiVBaP0K6R1CIi3ZQgQmWqQYiI9KIEEUrGo7SrD0JEpJsSRKgsFqEjnSGT0aJBIiKgBNEtGQ8m7FM/hIhIIDbQQTNLAO8BzgCOAFqBNcD97r6+8OENn2Q8yJXtqTTliWiRoxERKb5+axBm9mXgSeAsYDXwM+A+gqTyPTP7vZmdNNCbm9m5ZrbBzDaZ2XU5jn/CzJ43s1Vm9piZzc469r/D6zaY2bsP8v7y1lWD0JTfIiKBgWoQz7v7P/Rz7EYzOxw4sr+LzSwKLAPeBWwFnjaz+9z9hazTfu7uPwrPvxC4CTg3TBSXACcS1Fz+aGbHunvB/npPqEwAsLupgyNqywv1MSIio0a/NQh3v7fvPjNLmFlFePw1d39qgPd+K7DJ3V9y9w7gbuCiPp+xL2uzEujqIb4IuNvd2939ZWBT+H4FM2VcEoA39rUV8mNEREaNAfsgspnZlcBVQMTMHnb3rwxyyVTg1aztrcBpOd7308DngQTwzqxrn+hz7dQc114NXA0wZcoU6uvr87qXXPa2BZ3Tf1rxHLEd8YN+n0PV1NR0SPcxlqgselN59Kby6FGosug3QZjZee7+QNaud7v7O8Jjq4HBEoTl2PemZ0jdfRmwzMw+DHwZuOIArr0FuAVg4cKFvmTJkkFC6l9nOsO19Q8wcep0liyZddDvc6jq6+s5lPsYS1QWvak8elN59ChUWQz0mOtpZnZPVkf0WjO7w8xuB/J5gmkrvfsopgHbBzj/buDig7z2kMWjEarKYuxt6Sjkx4iIjBr91iDc/etmNhX4P2bWDnwNGA9UuPszebz308AsM5sBbCPodP5w9glmNsvdN4ab7wW6Xt8H/NzMbiLopJ4FDNTfMSRqK+I0tmjRIBERGLwPYg/wSYKniW4FHid40mhQ7p4ys2uAB4EocKu7rzWz64EV7n4fcI2ZLQU6gb0EzUuE5/0SeAFIAZ8u5BNMXeoqEqpBiIiEBuqD+AawFIgD/+bu55vZXwP3m9m/uvtdg725u98P3N9n31ezXn92gGu/CXxz8FsYOnWVCXY1KUGIiMDAfRAXufvbCZ48uhLA3X8LnEvQ7DPmHDOpik07mkhrPiYRkQGbmNaZ2W1AOfBY10537wT+qdCBFcOxU6po7UyzvaGVI8dXFDscEZGiGqiT+kNmtgDodPc1wxhT0dRWBKOp97Wpo1pEZKA+iNPd/YkBjlcBR/WZOmNUq04GxdHUlipyJCIixTdQE9OlZvYd4AFgJbATSALHEEzgdwzwdwWPcBh1JYj9ShAiIgM2Mf1PM5sIvB+4HDicYLrvdcDP3L1+WCIcRlVlYQ2iXQlCRGTAcRDuvgv4Yfgz5lV11yDUByEiohXlsoxLBpP07VcNQkRECSJbWSxCLGLqpBYRQQmiFzOjOhlTJ7WICHkkCDN70sw+bmbjhiOgYqtKxtRJLSJCfjWIK4CZwCoz+39mdnaBYyqq6rK4OqlFRMgjQbj7enf/IsGU278B7jCzl83sK2ZWW/AIh1mVmphERIA8+yDMbDbwj8C3gXuBy4AO4JHChVYc1WVqYhIRgTzWpDazJwkGyN0KfNXdW8NDj5vZ2wsZXDFUJ2Ns3KEEISIyaIIALnf3F3MdcPcLhzieolMntYhIIJ8mpsuz+xrMrC5cTGhMqk4GndTuWhNCREpbPgnifHdv6Npw973ABYULqbiqymJ0pp32VKbYoYiIFFU+CSJqZomuDTNLAokBzh/VxiU1YZ+ICOTXB3E38JCZ3Qo4cBVwZ0GjKqLqcD6mfa2dTKwqK3I0IiLFM2iCcPdvmdnzwNmAATe6+38WPLIiqa0IEsTeFg2WE5HSlk8NAnf/HfC7AscyIoyvDFrP9jZ3FDkSEZHiymcuplPN7AkzazSzNjNrN7N9wxFcMdSF61LvaVGCEJHSlk8N4gcEI6fvBt4KfAQ4soAxFVWdahAiIkB+TzFF3H0DEHP3Tnf/CbC0wHEVTWUiSiIaUR+EiJS8fGoQzeFjrqvN7FvAa0BVYcMqHjOjrjKuGoSIlLx8ahAfCc+7BkgTzOr6vgLGVHR1FQn1QYhIyRuwBmFmUeBr7n4F0AZ8ZViiKrK6ioRqECJS8gasQbh7GjjczOLDFM+IML5SNQgRkXz6IF4C/mxm9wLNXTvd/eaCRVVkdZVxGtRJLSIlLp8EsRN4CKgIf8a88RUJGlo6SGecaMSKHY6ISFHkM9VGSfQ7ZKurTJDxYD6mrnERIiKlJp8V5R4imKSvF3c/pyARjQDZo6mVIESkVOXTxPTlrNdJ4H8A7YUJZ2ToNZp6UpGDEREpknyamJ7ss+tRM3u0QPGMCOO7ahB61FVESlg+TUzjsjYjwCnA4QWLaASoqwye6tWTTCJSyvJpYlpL0AdhQAp4GfhYIYMqtq4pvzUWQkRKWT5NTGN25tb+lMejJGIRjaYWkZKWz3oQnzCz2qztOjO7urBhFZeZMb4ioT4IESlp+UzW9wl3b+jacPe9wCfzeXMzO9fMNpjZJjO7Lsfxz5vZC2b2nJk9bGZHZx27wczWhD8fzOfzhlJdZYK9amISkRKWT4KIZm+YWQQYdG6mcKK/ZcB5wGzgQ2Y2u89pzwIL3X0u8GvgxvDa9wInA/OB04Av9OksL7jxlXHVIESkpOWTIB4ys7vMbLGZnQncCfwxj+veCmxy95fcvYNgRbqLsk9w9+Xu3hJuPgFMC1/PBh5195S7NwOrgXPz+MwhU1eR0FNMIlLS8nmK6QsETUrXEjzJ9Afgx3lcNxV4NWt7K0FtoD9XAQ+Er1cDXzOzmwjmfzoLeKHvBWFfyNUAU6ZMob6+Po+w8tPS0M4bjakhfc98NDU1DftnjlQqi95UHr2pPHoUqizySRBx4Afu/n3obmJKEDzyOpBcs9y9acqO8D0vAxYCiwHc/Q9mdirwXwSTBf4l1+e5+y3ALQALFy70JUuW5HE7+Xmm80WWv7qRd5xxJrFoPhWtoVFfX89Q3sdoprLoTeXRm8qjR6HKIp+/fMuByqztSuCRPK7bCmQ/IjsN2N73JDNbCnwJuNDdu6fwcPdvuvt8d38XQbLZmMdnDpnxFXHcobFVzUwiUprySRDl7r6/ayN8nc+0308Ds8xsRrim9SXAfdknmNkCguaqC919R9b+qJlNCF/PBeYSNG0Nm675mNRRLSKlKp8E0WJm87o2zGw+wfKjA3L3FME61g8C64BfuvtaM7vezC4MT/sOUAX8ysxWmVlXAokTLFL0AkET0mXh+w2bw8YlAXh936C3KiIyJuXTB3EtcI+ZvRJuHwV8OJ83d/f7gfv77Ptq1uul/VzXRvAkU9EcUVsOwPaG1mKGISJSNHnN5mpmJwAnEPQFrAXShQ6s2A6rSWIG2xpUgxCR0pTX4znu3u7uq4Bq4GZgW0GjGgHi0QhTqpOqQYhIycpnLqZTzOyfzGwzwTiFp4GTCh3YSHBErRKEiJSufhOEmX3DzNYDNxE8YnoqsMPdf+ruu4YrwGI6orZcCUJEStZANYhrgB3A94Bb3X0n/Qx0G6um1pazvbGNTKakbltEBBg4QRxG8BjqB4CXzOw2oDwcSV0SjqgtpyOVYbfGQohICer3j727d7r779z9w8CxBOMZngK2mdkdwxVgMelRVxEpZfk+xdTi7ne7+0WEM60WNqyR4YjaYLCcEoSIlKJ8Bsr1Ei4Y9NMCxDLiTA1rENuUIESkBJVMf8LBqCmPU5GIsl2D5USkBOUzDuJNtYxc+8YiM9OjriJSsvKpQTyV574x6YjacrY3KkGISOnptyZgZpOBwwkebZ1DzwJA48hvuu8xYWptkhe2NxY7DBGRYTdQU9F7gY8SLPSzjJ4EsR/4SoHjGjGOqClnV1MHbZ1pkvFoscMRERk2/SYId78NuM3MPuDuvxzGmEaUrrEQrzW2MWNi5SBni4iMHfn0QUw2s3EAZvYjM3vKzM4ucFwjhgbLiUipyidBXO3u+8zsHILmpk8CNxY2rJFDYyFEpFTlkyC6Zqo7D7jN3Vfmed2YMKWmDFANQkRKTz5/6Feb2f3ABcADZlZFCc3qWhaLMqm6TAlCREpOPgPergROATa5e4uZTQSuKmxYI0swWE6jqUWktAxag3D3NDCToO8BoDyf68aSqbVJ9UGISMnJZ6qN7wNnAZeFu5qBHxUyqJHmmElVvLK7mdaOdLFDEREZNvnUBN7m7h8H2gDcfQ+QKGhUI8yJU2vIOKx/fV+xQxERGTb5JIjOcBU5BzCzCUCmoFGNMDPDAXJb9rQUORIRkeHTb4LImrF1GfAbYJKZfQN4DLhhGGIbMabVBVNPvaoEISIlZKCnmJ4CTnb3O8xsJbCUYD6m97v7mmGJboQoTwSPuqoGISKlZKAE0TU5H+6+Flhb+HBGrqPGVyhBiEhJGShBTDKzz/d30N1vKkA8I9ZR4yt4fNMu3B0zG/wCEZFRbqBO6ihQBVT381NSTj6qlh3729m8W7UIESkNA9UgXnP364ctkhHu9JkTAFixeY+m/RaRkjBQDULtKFneMqmK6mSMZ19tKHYoIiLDYqAEUTJrPuQjEjHmH1nLM6/sLXYoIiLDot8EEY6Ylizzj6xl444mWjpSxQ5FRKTgSmrSvUM1b1ot6Yyzdrum3BCRsU8J4gDMPbIGgNXqhxCREqAEcQAmVyeZWlvOKiUIESkBShAHaN6RNTy7pQH3kllUT0RKlBLEATrruMlsa2jl6c16mklExjYliAN0/twjqE7GuOupLcUORUSkoAqaIMzsXDPbYGabzOy6HMc/b2YvmNlzZvawmR2ddexGM1trZuvM7GYbIRMglSei/NWCqfzn86+xt7mj2OGIiBRMwRKEmUUJ1pI4D5gNfMjMZvc57VlgobvPBX4N3Bhe+zbg7cBc4CTgVGBxoWI9UB8+7Sg6Uhl+seLVYociIlIwhaxBvBXY5O4vuXsHcDdwUfYJ7r7c3btmv3sCmNZ1CEgSLG1aBsSBNwoY6wE5/rBxvO0tE7j98c10pktqcT0RKSGFTBBTgez/xd4a7uvPVcADAO7+F2A58Fr486C7rytQnAflb8+Ywev72rj/+deKHYqISEEMNJvrocrVZ5Dz2VAzuwxYSNiMZGbHACfQU6N4yMzOdPc/9bnuauBqgClTplBfXz80kefDnYnlxs8eeZ6aho1D9rZNTU3Dex8jmMqiN5VHbyqPHoUqi0ImiK3AkVnb04DtfU8ys6XAl4DF7t4e7v4r4Al3bwrPeQA4HeiVINz9FuAWgIULF/qSJUuG+BYGdtprz/Dc1gaG8nPr6+uH9P1GM5VFbyqP3lQePQpVFoVsYnoamGVmM8wsAVwC3Jd9gpktAH4MXOjuO7IObQEWm1nMzOIENYsR1cQEsPDoOl7d08r61zU3k4iMPQVLEO6eAq4BHiT44/5Ld19rZteb2YXhad8hWLXuV2a2ysy6Esivgf8GngdWA6vd/XeFivVgXTjvCCIGDzz/erFDEREZcoVsYsLd7wfu77Pvq1mvl/ZzXRr4eCFjGwoTqso4+ag6/uO57Xzm7FlEIyNiqIaIyJDQSOpDdPGCqfz3zmbuW72t2KGIiAwpJYhDdOlpR3HYuCQPrhkxwzRERIaEEsQhMjPOPmEyf9q4k/ZUutjhiIgMGSWIIXD2CZNp6Ujz5EtapVVExg4liCGwaOZEymIRvnX/OjpSmnpDRMYGJYghUJ6I8tF3zGD96/v59gMjbriGiMhBUYIYIl8893guOfVI7nxiCzv3tw9+gYjICKcEMYSuPnMmHemMFhMSkTFBCWIIzZxUxduPmcAvnn6VTEZrVovI6KYEMcQuOfUotjW08sj6HYOfLCIygilBDLFzTpzCjImVfOv+daS0mJCIjGJKEEOsLBbli+cez0u7mvn3VW+a3VxEZNRQgiiAd584hdmHj+Nr967hiZd2FzscEZGDogRRAGbGjy8/hcqyGNf95jkaWzuLHZKIyAFTgiiQI8dXsOzSk9m6t5Urbn2KLbtbih2SiMgBUYIooFOnj+c775/Lxjf2c81dz2gyPxEZVZQgCuyvFkzju++fx3NbG/n4v61ke0NrsUMSEcmLEsQwOG/O4XztgtnUb9jJef/8Zx5Zr7UjRGTkU4IYJle+fQa//9wZHFFbzkdvX8EP6jeR1mhrERnBlCCG0fGHjeOeT72N8046jBt/v4Hz/vlP/HnjzmKHJSKSU6zYAZSaZDzKDy49md+veZ0bfr+ey3/6FAuOquWYSVWcOmM8dapViMgIoQRRBGbGeXMO56zjJ/PjR1/i92tf54/r3uBXK7cyrcr4r+a1nDFrIotmTqQ8ES12uCJSopQgiigZj/LZpbP47NJZuDsPrHmdb9+3ijuf3MJtj28mEYtw+swJLDl2EmcdP5kZEyuLHbKIlBAliBHCzHjPnMOp2L2BRe84g6de3kP9hp0s37CD6//jBa7/jxc4ekIFZx03mcXHTWLRzAkk46pdiEjhKEGMQGWxKGfMmsQZsybxlfNns2V3C/Uv7qB+w07ufnoLt//XZspiEU6dPp4lx03ilKPrOGlqDfGonjkQkaGjBDEKHDWhgr9ZNJ2/WTSdts40T768h+Xrd7B8ww7+4T+DNbDjUWPmxCqOO6ya4w6rZnJ1GTXlcTIO0+rKGV+ZoCoZY1wyXuS7EZHRQglilEnGoyw+dhKLj53E1zmRN/a1sWLzXp7f1siLb+xn5St7uW91/9OMT6wq44TDq5k+oZLDapLUVSSYXF3GpOoyqpMxqspi1FUmVBsRESWI0W7KuCTvnXs47517ePe+5vYUu5rau2eR3d7QSkNLJ/vaOtn4RhPrX9/Pva9uY19bKud7RiPGtLpypk+o5IjaJJOqk0yoTFBbEaeuIkFdRYLqZIxx5XGqkzElE5ExSgliDKosi1FZ1vOfdu602pzntXWm2dvSweuNbexu6mB/eydN7WneaGxj8+5mXt7VzNrt+9jd3I4PMDwjGY9QnQySRXUyzriwKSvY7tlXV5mgIhGjLBahsixKRSJGRSJKeTxKeSLYjkZsqItDRA6SEkQJS8ajHF5TzuE15QOel0pnaGztZG9LJw0tHexp7mB/W4r9bZ3B7/bg9b7WFPvCfdsbWsNzUrR25j+LbV1FnAlVZUyuLqN5Xxt3vboiTCBBMqlMRKkoi1FTHmdSVRnRiDGuPEZNeVDDqSoLEpCZEo3IoVKCkEHFohEmVJUxoarsoK7vTGfYFyaYlo4U7akMze0pWjrStHakaelM09aRprkjxc797exp7mDH/nYa2p22XS20pdLd5zZ3pAaszQBEjO7aSUVYM6lOBrWTrppNV42lK+HEIkYkYkTNKItFmDyujHg0ggEO3Z/peLCjz/5oxIgY4fuGCS0epUpNcDKKKUFIwcUPMsHU19ezZMmZvfa5O62dafa2dLJrfztpd/a3pWho6aChpZOm9lR3ImntCBJLU3uKprYgMb24Yz/N7SnaOjO0dKToTBd+apOIBWUQ/Fj366qyIHG1pzJEIkZVWZTKRIyq8GGByrLgd1VZjCnjytjekKb6lT1kPHjP6mSc8niURCzCxLA2JTKUlCBkVDGzsHYQY2rtwE1j+ehIZWjtSNOZyZDJOGl3WjvS7GrqIJXOZH0wGBbGABbG0iWVyeBOd42otSNFc3uQnDpSGTozGTpTTiqToTOdoSPl3U10VckYGQ8fLtjfQlN7iuaOIKml+s7N9cRfct5HIhrpfmAgEYtk9etEqSqLBQm6MkFdZYJkPEIyFmV8ZYKainjYdBc0zUUiRnk8SjIeVcIRJQgpbYlY8Ae1r5mTihBMH+5OeypDU3uK1xvb+OPjT7Ng/jyiZqTd2dfaSWtnmvZUhq17W2hqS9GZztAeJr3WzqBZ7pWmFp7Z0sDelo4DmmI+EY2QjEcoDx8kSMajxKNBEolFjESYjMpiEcriUZKxCLFokFTSGSedgYw7ETPK4hES0Uh3/1BXU11XNB621UXMKA+TWmV3LSpIYBWJGLGwBpaIRtjdmmHHvjZi0QjRiBGPWvA7EsQoh04JQmSEMjOS4R/miVVl7JocY/GxB5+5MpmgOa497NPZ29JBQ2tn0FXOgC0AAAn/SURBVCTXHjTBZdxp60zT2pGhtTNNW/jTlWxSGSeVcdJhjaihpYO2zgwd6QxtnWnSGceBqAV9MpGIkck4HekM7Z0Z2rNqZUEtrHfNLPj8TO4byOXRh/spO4hFjFgkEvyOGtGs18HvNx9LxiNUl8WJRY3qZIxkvCc5JuOR4HcsaNYzC/qeso+XxaKYBX1TEbPuJsVo2MdVmYhSnYyPmtqZEoRIiYhEjJqKOBCMpp/OyJz8MZ1xmjtSNLcHP03t6e6HGjrTQRNdKu2sWbeOY2YdSyqdlbTSTjrjpNKZ7mQWHA+3u/aH53WmM8Hv8PrWjjQ797fTmXaa2lPdCXKo+6qiYbKA4H8EIhYkFLPg/lMZxz1IdJFexy3c17U/2J6S6GDJkiENEVCCEJERputps8GmhZmwfxNLTjt6WGJKpTO0pYJaUnsqQ3tnGif4Y97S0VPT6qr9RMLaUGe6q9/JcXea29Psa+ukM52hub3n8e+Me/gT1L6iEeuuiXi43wl/u5MJm+88vDbduKMg960EISIyiFg0QlX45NlIVF9fX5D31QPaIiKSU0EThJmda2YbzGyTmV2X4/jnzewFM3vOzB42s6PD/WeZ2aqsnzYzu7iQsYqISG8FSxBmFgWWAecBs4EPmdnsPqc9Cyx097nAr4EbAdx9ubvPd/f5wDuBFuAPhYpVRETerJA1iLcCm9z9JXfvAO4GLso+IUwELeHmE8C0HO/zPuCBrPNERGQYFDJBTAVezdreGu7rz1XAAzn2XwLcNYRxiYhIHgrZJZ9rJEjOh4nN7DJgIbC4z/7DgTnAg/1cdzVwNcCUKVMK1pM/nJqamsbEfQwFlUVvKo/eVB49ClUWhUwQW4Ejs7anAW9a6szMlgJfAha7e3ufwx8A7nH3zlwf4O63ALcALFy40JcUYqTIMAsmqFtS7DBGBJVFbyqP3lQePQpVFoVsYnoamGVmM8wsQdBUdF/2CWa2APgxcKG75xrp8SHUvCQiUhTmg02ufyhvbvYe4P8CUeBWd/+mmV0PrHD3+8zsjwRNSK+Fl2xx9wvDa6cDjwNHuvugk7OY2U7glaG/i2E3EdhV7CBGCJVFbyqP3lQePQ6lLI5295yTfBU0QciBM7MV7r6w2HGMBCqL3lQevak8ehSqLDSSWkREclKCEBGRnJQgRp5bih3ACKKy6E3l0ZvKo0dBykJ9ECIikpNqECIikpMShIiI5KQEMczMrNbMfm1m681snZktMrPxZvaQmW0Mf9eF55qZ3RxOl/6cmZ1c7PiHkplda2ZrzWyNmd1lZslwYOWTYVn8IhxkiZmVhdubwuPTixv90DCzW81sh5mtydp3wN8HM7siPH+jmV1RjHs5VP2UxXfCfyvPmdk9Zlabdex/h2WxwczenbV/wGUGRotc5ZF17O/MzM1sYrhdmO+Gu+tnGH+AnwF/G75OALUE05xfF+67DrghfP0eggkMDTgdeLLY8Q9hOUwFXgbKw+1fAh8Jf18S7vsR8Mnw9aeAH4WvLwF+Uex7GKJyOBM4GViTte+Avg/AeOCl8Hdd+Lqu2Pc2RGVxDhALX9+QVRazgdVAGTAD+G+CAbnR8PXM8N/XamB2se9tqMoj3H8kwfx0rwATC/ndUA1iGJnZOIL/6D8FcPcOd28gmAb9Z+FpPwO6Fke6CLjDA08AteEEhmNFDCg3sxhQQTCi/p0Ea4PAm8uiq4x+DZxtZrkmhBxV3P1PwJ4+uw/0+/Bu4CF33+Pue4GHgHMLH/3QylUW7v4Hd0+Fm9lLAlwE3O3u7e7+MrCJYImBQZcZGC36+W4AfA/4X/Se/LQg3w0liOE1E9gJ3GZmz5rZv5pZJTDF3V8DCH9PDs8/0CnTRw133wZ8F9hCkBgagZVAQ9YfhOz77S6L8HgjMGE4Yx5GB/p9GLPfkz4+Ss+SACVZFmZ2IbDN3Vf3OVSQ8lCCGF4xgirjD919AdBM0ITQn7ynTB9twnb1iwiaB44AKglWH+yr637HbFkcgP7KYMyXjZl9CUgBd3btynHamC4LM6sgmPn6q7kO59h3yOWhBDG8tgJb3f3JcPvXBAnjja6mo/D3jqzzB50yfZRaCrzs7js9mM79t8DbCKrGXdPQZ99vd1mEx2vIXf0eCw70+zCWvyeEHavnA5d62LBOaZbFWwj+h2q1mW0muLdnzOwwClQeShDDyN1fB141s+PCXWcDLxBMg971dMEVwL3h6/uAvwmfUDgdaOxqehgDtgCnm1lF2JfQVRbLCZaZhTeXRVcZvQ94JOuPxVhzoN+HB4FzzKwurJmdQz+LbI02ZnYu8EWCJQGylx2+D7gkfLptBjALeIo8lhkYrdz9eXef7O7T3X06wR//k8O/K4X5bhS7p77UfoD5wArgOeDfCZ4smAA8DGwMf48PzzVgGcFTGc8DC4sd/xCXxTeA9cAa4N8InkiZSfAPfRPwK6AsPDcZbm8Kj88sdvxDVAZ3EfTBdIb/4K86mO8DQfv8pvDnymLf1xCWxSaCNvRV4c+Pss7/UlgWG4Dzsva/B3gxPPalYt/XUJZHn+Ob6XmKqSDfDU21ISIiOamJSUREclKCEBGRnJQgREQkJyUIERHJSQlCRERyUoIQyWJm3zazJWZ2cX8zgZrZ181sm5mtMrMXzOxDwxDXR8zs+4X+HJFsShAivZ0GPAksBv48wHnfc/f5BNOF/NjM4sMRnMhwUoIQoXvdgeeAU4G/AH8L/NDMcs17083dNwItBAMeMbP5ZvZE1voFXfvrzWxh+HpiOFVCV83gt2b2+3C+/huzYrrSzF40s0eBt2ftf78Fa2isNrM/DWU5iGRTghAB3P0LBEnhdoIk8Zy7z3X36we6LlyYZaO7d82XdAfwRXefSzCi9Wt5fPx84IPAHOCDZnZkOAfTNwgSw7sI1j/o8lXg3e4+D7gwz1sUOWBKECI9FhBM53A8wbxQA7nWzDYQNEd9HcDMaoBad380POdnBOt/DOZhd29097bwc48maOqq92Ayww7gF1nnPw7cbmYfI1ggR6QgYoOfIjK2mdl8gprDNGAXweJFZmargEXu3prjsu+5+3fN7K+BO8zsLYN8TIqe/yFL9jnWnvU6Tc+/y5zz4Lj7J8zsNOC9wCozm+/uuwf5fJEDphqElDx3XxV2OL9I0JTzCEETzvx+kkP2tb8lmHzxCndvBPaa2Rnh4cuBrtrEZuCU8PX7GNyTwBIzmxB2gL+/64CZvcXdn3T3rxIktCP7exORQ6EahAhgZpOAve6eMbPj3X2wJqZs1wM/N7OfEEzP/aNwcZeXgCvDc74L/NLMLidIQANy99fM7OsEHeavAc/Q05z0HTObRTCD58ME6y6LDDnN5ioiIjmpiUlERHJSghARkZyUIEREJCclCBERyUkJQkREclKCEBGRnJQgREQkp/8PMcJh54Ofw28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(range(500,1400),np.concatenate([acc_test_arr_pre[0,0,:], acc_test_arr[0,0,:], acc_test_arr_v2[0,0,0:200]]), label='Random')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.xlabel('# Rounds')\n",
    "plt.ylim([87,90])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(500,1400),np.concatenate([loss_test_arr_pre[0,0,:], loss_test_arr[0,0,:], loss_test_arr_v2[0,0,0:200]]), label='Random')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.xlabel('# Rounds')\n",
    "# plt.ylim([-0.1,1.1])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 40)\n",
      "(100, 62346)\n",
      "[18. 21. 24. 24. 16. 17. 15. 14. 29. 23. 17. 18. 17. 27. 18. 23. 18. 19.\n",
      " 15. 17. 14. 23. 19. 18. 20. 14. 13. 28. 24. 20. 22. 19. 20. 22. 24. 26.\n",
      " 22. 19. 17. 26.]\n"
     ]
    }
   ],
   "source": [
    "P_random = np.array(P_random)\n",
    "\n",
    "print(np.shape(P_random))\n",
    "\n",
    "print(np.shape(w_glob_array_np_v2))\n",
    "\n",
    "print(np.sum(P_random, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 40, 62346)\n",
      "(100, 62346)\n",
      "[[158.21327955]] [[8.84532894]] [[125.04237207]]\n",
      "[[152.73572428]] [[10.32950774]] [[122.62436029]]\n",
      "[[147.18497771]] [[5.50347366]] [[124.88875901]]\n",
      "[[143.75347072]] [[4.99065967]] [[151.09850582]]\n",
      "[[144.23926235]] [[5.52623002]] [[112.9298726]]\n",
      "[[139.76156453]] [[5.86010963]] [[110.75194247]]\n",
      "[[135.98044878]] [[3.89170519]] [[113.62725985]]\n",
      "[[132.71884648]] [[6.09421754]] [[144.50502968]]\n",
      "[[133.06830679]] [[10.81839507]] [[75.07469926]]\n",
      "[[125.18717646]] [[5.19518629]] [[138.93251425]]\n",
      "[[126.06769917]] [[8.18372565]] [[105.0356753]]\n",
      "[[123.95471678]] [[8.98922272]] [[73.75847096]]\n",
      "[[117.54043027]] [[4.29658532]] [[86.74879009]]\n",
      "[[113.70068296]] [[2.37751417]] [[124.14950975]]\n",
      "[[114.57550862]] [[3.49933509]] [[105.35682452]]\n",
      "[[113.04294803]] [[8.61987789]] [[65.86582616]]\n",
      "[[107.08095435]] [[3.70035365]] [[91.62306128]]\n",
      "[[105.0262822]] [[5.77180683]] [[127.64511365]]\n",
      "[[106.81938455]] [[2.31181584]] [[89.19120747]]\n",
      "[[104.81520702]] [[3.13327816]] [[118.26548184]]\n",
      "[[106.00648167]] [[3.46356183]] [[90.33349767]]\n",
      "[[104.17251837]] [[2.88424166]] [[123.68728939]]\n",
      "[[106.10310595]] [[4.89844757]] [[111.92659395]]\n",
      "[[106.43076436]] [[5.43120205]] [[87.90915699]]\n",
      "[[104.08939706]] [[4.1300147]] [[88.74902695]]\n",
      "[[101.84837699]] [[3.44160904]] [[118.88165109]]\n",
      "[[103.41408547]] [[3.62254923]] [[82.32357832]]\n",
      "[[100.77299413]] [[14.1045721]] [[45.52217689]]\n",
      "[[94.8036913]] [[1.52287703]] [[104.48442875]]\n",
      "[[95.69644447]] [[2.13666207]] [[80.42989868]]\n",
      "[[93.94522706]] [[6.25943792]] [[126.20219933]]\n",
      "[[95.71817098]] [[6.6599893]] [[56.06939809]]\n",
      "[[91.9575467]] [[2.0417593]] [[109.60202749]]\n",
      "[[93.61854189]] [[8.84379729]] [[116.55563116]]\n",
      "[[94.97281129]] [[4.63749533]] [[106.81746676]]\n",
      "[[95.9832721]] [[3.13261857]] [[84.15664962]]\n",
      "[[94.47310744]] [[2.85523975]] [[110.5222481]]\n",
      "[[95.93913701]] [[2.20114532]] [[83.47834439]]\n",
      "[[94.52434411]] [[3.6915244]] [[113.54223691]]\n",
      "[[95.97881036]] [[2.22184746]] [[111.95606461]]\n",
      "[[96.41999132]] [[4.63152198]] [[86.12452342]]\n",
      "[[95.8335581]] [[1.95962976]] [[88.73401731]]\n",
      "[[94.90123603]] [[2.12328273]] [[109.77225624]]\n",
      "[[96.22313745]] [[2.56766597]] [[107.47167405]]\n",
      "[[96.15501822]] [[5.62038023]] [[115.09887807]]\n",
      "[[97.66469143]] [[3.1354967]] [[83.73996207]]\n",
      "[[95.89107947]] [[1.87783381]] [[80.827996]]\n",
      "[[95.16502876]] [[4.56070978]] [[84.85234431]]\n",
      "[[93.62772781]] [[1.84099746]] [[81.56404474]]\n",
      "[[92.27106889]] [[1.99091945]] [[85.64682525]]\n",
      "[[91.40794365]] [[1.43293223]] [[83.4844148]]\n",
      "[[90.40579349]] [[3.55989087]] [[86.11479602]]\n",
      "[[89.68665888]] [[6.19302176]] [[69.40014817]]\n",
      "[[87.07309275]] [[5.5445422]] [[115.9924681]]\n",
      "[[89.43487685]] [[3.10986741]] [[85.35043307]]\n",
      "[[88.70057244]] [[5.18477936]] [[65.14145498]]\n",
      "[[85.8699]] [[4.64757161]] [[103.57544106]]\n",
      "[[87.08094614]] [[5.94004839]] [[57.97339746]]\n",
      "[[83.62180497]] [[3.32140226]] [[102.30431489]]\n",
      "[[85.03317833]] [[2.26921488]] [[100.68460386]]\n",
      "[[86.44930348]] [[8.22963609]] [[52.60505068]]\n",
      "[[82.47129419]] [[2.05057663]] [[67.25144182]]\n",
      "[[80.79889245]] [[4.13405734]] [[67.82677836]]\n",
      "[[79.37797364]] [[4.13298267]] [[74.04102778]]\n",
      "[[78.46864115]] [[6.7903117]] [[57.50374695]]\n",
      "[[76.65134896]] [[5.45884853]] [[98.07089324]]\n",
      "[[77.21772597]] [[2.66302176]] [[97.87463042]]\n",
      "[[78.91739059]] [[3.40440013]] [[91.58014597]]\n",
      "[[79.82770179]] [[1.94378525]] [[79.76251005]]\n",
      "[[79.63256928]] [[9.51207703]] [[102.05607811]]\n",
      "[[80.84543566]] [[1.88421995]] [[75.31248341]]\n",
      "[[80.1726316]] [[6.31297235]] [[67.66179987]]\n",
      "[[78.64169751]] [[4.05982922]] [[70.54601776]]\n",
      "[[78.26858778]] [[3.84709663]] [[97.69931953]]\n",
      "[[78.99822772]] [[4.48349461]] [[54.66543828]]\n",
      "[[77.1798644]] [[2.86317028]] [[97.46808293]]\n",
      "[[78.97776068]] [[2.47857163]] [[70.36142754]]\n",
      "[[78.09974371]] [[3.7979585]] [[83.68900755]]\n",
      "[[78.18535284]] [[6.74060304]] [[90.00847666]]\n",
      "[[78.92156822]] [[2.29212668]] [[69.49251726]]\n",
      "[[77.85373088]] [[2.7210682]] [[98.32409864]]\n",
      "[[79.48216096]] [[8.07389297]] [[48.6098699]]\n",
      "[[75.9931312]] [[4.3163984]] [[91.75295939]]\n",
      "[[77.3059245]] [[4.46392285]] [[93.42427254]]\n",
      "[[78.29655284]] [[6.65657713]] [[73.84227866]]\n",
      "[[77.22856031]] [[4.90247995]] [[52.60893007]]\n",
      "[[74.51987164]] [[2.88768876]] [[75.75044455]]\n",
      "[[74.29421147]] [[2.5917489]] [[71.1617502]]\n",
      "[[73.76794593]] [[4.98617433]] [[97.84097106]]\n",
      "[[75.56778006]] [[1.71989702]] [[76.15806124]]\n",
      "[[75.39595294]] [[2.6434121]] [[97.2073792]]\n",
      "[[77.19237969]] [[5.66853895]] [[89.83436852]]\n",
      "[[78.03777427]] [[2.53193189]] [[97.62184833]]\n",
      "[[79.82761371]] [[1.73745914]] [[68.68791798]]\n",
      "[[78.57647912]] [[5.47061113]] [[47.22843745]]\n",
      "[[75.20535525]] [[6.5859519]] [[102.07990501]]\n",
      "[[77.07639166]] [[3.16694684]] [[64.39745712]]\n",
      "[[75.70808825]] [[5.92700482]] [[53.28555032]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(w_locals_array_np_v2))\n",
    "print(np.shape(w_glob_array_np_v2))\n",
    "\n",
    "grad_locals_array_np_v2 = np.zeros((100,N,d))\n",
    "grad_glob_array_np_v2 = np.zeros((100,d))\n",
    "\n",
    "for i in range(1, 99):\n",
    "#     print(i)\n",
    "    grad_locals_array_np_v2[i+1,:,:] = (w_locals_array_np_v2[i,:,:] - w_glob_array_np_v2[i-1,:])*1400\n",
    "    \n",
    "    grad_glob_array_np_v2[i-1,:] = (w_glob_array_np_v2[i,:] - w_glob_array_np_v2[i-1,:])*1400\n",
    "    \n",
    "    ModelDiff_np(grad_locals_array_np_v2[i+1,0,:], grad_glob_array_np_v2[i-1,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.99293432]] [[1.84099746]] [[3.84945838]]\n",
      "[[1.99293432]] [[1.99091945]] [[0.0008409]]\n",
      "[[1.99293432]] [[1.43293223]] [[1.0731225]]\n",
      "[[1.99293432]] [[3.55989087]] [[5.52470489]]\n",
      "[[1.99293432]] [[6.19302176]] [[3.52756645]]\n",
      "(40, 62346)\n",
      "(62346,)\n",
      "[[45.1227395]] [[45.12273882]] [[1.24442894e-13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.75787543e-15]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_idx = 50\n",
    "p_temp = np.reshape(P_random[iter_idx,:],(1,N))\n",
    "temp = np.matmul(p_temp, grad_locals_array_np_v2[iter_idx,:,:])/K\n",
    "ModelDiff_np(temp, grad_glob_array_np_v2[iter_idx-2,:])\n",
    "ModelDiff_np(temp, grad_glob_array_np_v2[iter_idx-1,:])\n",
    "ModelDiff_np(temp, grad_glob_array_np_v2[iter_idx-0,:])\n",
    "ModelDiff_np(temp, grad_glob_array_np_v2[iter_idx+1,:])\n",
    "ModelDiff_np(temp, grad_glob_array_np_v2[iter_idx+2,:])\n",
    "\n",
    "print(np.shape(w_locals_array_np_v2[iter_idx,:,:]))\n",
    "\n",
    "p_temp = np.reshape(P_random[iter_idx,:],(1,N))\n",
    "temp = np.matmul(p_temp, w_locals_array_np_v2[iter_idx,:,:])/K\n",
    "temp = np.reshape(temp, (d,))\n",
    "print(np.shape(temp))\n",
    "ModelDiff_np(temp, w_glob_array_np_v2[iter_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 50)\n",
      "40\n",
      "[[94.52434411]] [[21.14882496]] [[27.62118478]]\n",
      "[[1.74818775]] [[9.55088684]] [[9.90631917]]\n",
      "[[42.40201533]] [[8.08654093]] [[15.41952096]]\n",
      "[[2.29343334]] [[7.29101947]] [[6.7620837]]\n",
      "[[24.41121044]] [[8.49374766]] [[10.70292408]]\n",
      "[[14.11808357]] [[7.14715751]] [[8.83611095]]\n",
      "[[23.04815488]] [[13.29924296]] [[13.52342132]]\n",
      "[[28.25149969]] [[20.78361183]] [[18.3339173]]\n",
      "[[29.7340302]] [[27.23268092]] [[0.32039256]]\n",
      "[[28.68402302]] [[24.08181798]] [[0.47729891]]\n",
      "[[26.3348799]] [[17.14552724]] [[18.79403711]]\n",
      "[[43.15674263]] [[25.14066789]] [[17.88101291]]\n",
      "[[27.98552789]] [[14.45998489]] [[16.17353677]]\n",
      "[[43.26345572]] [[18.53691469]] [[35.59410809]]\n",
      "[[55.77188165]] [[17.72643273]] [[26.33835704]]\n",
      "[[40.36724837]] [[13.19265437]] [[24.16085701]]\n",
      "[[26.80991327]] [[29.09808719]] [[47.19020956]]\n",
      "[[116.38262607]] [[34.82614284]] [[43.01421491]]\n",
      "[[40.03526802]] [[25.01628724]] [[25.14141687]]\n",
      "[[39.93616165]] [[23.44960299]] [[22.18533072]]\n",
      "[[51.03818722]] [[16.77050451]] [[18.37439849]]\n",
      "[[20.79678372]] [[9.24708639]] [[13.91225718]]\n",
      "[[23.77376285]] [[8.97840302]] [[14.15442849]]\n",
      "[[24.6137866]] [[6.02964053]] [[8.62035411]]\n",
      "[[2.2580681]] [[10.3947083]] [[8.39781257]]\n",
      "[[29.89418622]] [[13.37768309]] [[15.82864571]]\n",
      "[[33.72006338]] [[14.73642391]] [[23.9345005]]\n",
      "[[36.39929191]] [[12.42740526]] [[8.84214517]]\n",
      "[[2.85289748]] [[13.78705703]] [[12.18483581]]\n",
      "[[59.53007579]] [[57.6078191]] [[0.44473879]]\n",
      "[[56.1535597]] [[33.56851742]] [[42.50077833]]\n",
      "[[106.87032012]] [[32.81947389]] [[43.96787035]]\n",
      "[[50.71184769]] [[15.53381186]] [[20.3529055]]\n",
      "[[24.42125823]] [[7.56521126]] [[13.80600461]]\n",
      "[[12.14902323]] [[11.96703397]] [[3.73744077]]\n",
      "[[14.47085404]] [[25.64160011]] [[24.92955771]]\n",
      "[[59.90930115]] [[41.69055173]] [[43.62150611]]\n",
      "[[137.87174517]] [[34.81491441]] [[57.18919]]\n",
      "[[54.45038589]] [[18.07242291]] [[19.78207186]]\n",
      "[[23.26024045]] [[18.07242291]] [[18.01543359]]\n",
      "\n",
      "\n",
      "[[93.62772781]] [[85.70932634]] [[0.29493953]]\n",
      "[[2.02641998]] [[2.79326129]] [[0.15899555]]\n",
      "[[42.44739784]] [[35.85365035]] [[0.59366028]]\n",
      "[[2.50718001]] [[2.49390364]] [[0.26565925]]\n",
      "[[26.4857875]] [[28.50647405]] [[0.18804742]]\n",
      "[[14.56855273]] [[14.94399028]] [[1.18097065]]\n",
      "[[20.43976688]] [[17.06720847]] [[0.65131966]]\n",
      "[[26.88874295]] [[35.77520945]] [[5.91487944]]\n",
      "[[26.2517198]] [[29.28637301]] [[0.41681931]]\n",
      "[[25.15874809]] [[25.74258134]] [[0.39432832]]\n",
      "[[23.05252343]] [[23.02520951]] [[0.28881301]]\n",
      "[[44.15272401]] [[43.6077883]] [[1.58624492]]\n",
      "[[30.53957414]] [[55.14220877]] [[9.10841168]]\n",
      "[[41.20365089]] [[36.60015808]] [[0.27518922]]\n",
      "[[52.7047637]] [[66.54167226]] [[1.05772497]]\n",
      "[[39.04575835]] [[33.34732836]] [[0.67391023]]\n",
      "[[29.46181463]] [[28.27155624]] [[0.42506533]]\n",
      "[[115.40748678]] [[120.26093583]] [[0.28381876]]\n",
      "[[40.81139489]] [[38.40270542]] [[3.43691768]]\n",
      "[[41.41252476]] [[43.84090717]] [[0.16033121]]\n",
      "[[52.5821241]] [[50.83169267]] [[1.02269861]]\n",
      "[[18.35623364]] [[20.23898578]] [[0.15920126]]\n",
      "[[25.41846417]] [[26.8389995]] [[0.27519243]]\n",
      "[[24.97315208]] [[22.89912077]] [[0.50960503]]\n",
      "[[2.57634236]] [[3.5999131]] [[0.39185416]]\n",
      "[[31.9575706]] [[32.52770281]] [[0.78605836]]\n",
      "[[28.74855004]] [[29.26710579]] [[0.58628553]]\n",
      "[[38.37753588]] [[42.78907839]] [[1.29715596]]\n",
      "[[3.21592979]] [[3.53621906]] [[0.28293047]]\n",
      "[[55.11296673]] [[53.11577379]] [[0.54691295]]\n",
      "[[51.75070118]] [[63.68117828]] [[1.06400442]]\n",
      "[[106.81128139]] [[103.92219435]] [[0.94576889]]\n",
      "[[48.42433883]] [[48.96234402]] [[1.71313855]]\n",
      "[[22.13624049]] [[20.56646503]] [[0.4969145]]\n",
      "[[13.44990906]] [[16.76216802]] [[0.52076425]]\n",
      "[[14.59460417]] [[17.13907751]] [[0.7735823]]\n",
      "[[55.97412976]] [[75.22966473]] [[2.01951375]]\n",
      "[[137.3891858]] [[110.53096331]] [[3.11493076]]\n",
      "[[54.51001968]] [[46.51997907]] [[0.53499121]]\n",
      "[[20.15595079]] [[20.08787208]] [[0.24469083]]\n",
      "0.9102863824937788\n",
      "0.04291526719242754\n",
      "21.211248165184447\n"
     ]
    }
   ],
   "source": [
    "# Pseudo Inversion\n",
    "\n",
    "offset = 1\n",
    "P_random_tmp = P_random[40+offset:90+offset,:]\n",
    "\n",
    "PT = P_random_tmp.transpose()\n",
    "\n",
    "print(np.shape(PT))\n",
    "\n",
    "PTP = np.matmul(P_random_tmp.transpose(), P_random_tmp)\n",
    "\n",
    "print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "# print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "Pw_glob = np.matmul(PT, grad_glob_array_np_v2[40:90,:])\n",
    "grad_recon_np = K * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "# ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "l2_diff = np.zeros((N))\n",
    "l2_diff_ = np.zeros((N))\n",
    "for i in range(N):\n",
    "    if i == N-1:\n",
    "        l2_diff_[i] = ModelDiff_np(grad_locals_array_np_v2[40,i,:], (grad_recon_np[i-1]+grad_recon_np[i])/2)\n",
    "    else:\n",
    "        l2_diff_[i] = ModelDiff_np(grad_locals_array_np_v2[40,i,:], (grad_recon_np[i]+grad_recon_np[i+1])/2)\n",
    "print()\n",
    "print()\n",
    "for i in range(N):\n",
    "    l2_diff[i] = ModelDiff_np(grad_locals_array_np_v2[50,i,:], (grad_recon_np[i]))\n",
    "\n",
    "print(np.sum(l2_diff_)/N)\n",
    "print(np.sum(l2_diff)/N)\n",
    "\n",
    "print(np.sum(l2_diff_)/np.sum(l2_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42.44739784]] [[92.27106889]] [[142.57820874]]\n",
      "[[42.44739784]] [[2.04458112]] [[47.4900455]]\n",
      "[[42.44739784]] [[43.24968852]] [[0.00392476]]\n",
      "[[42.44739784]] [[2.51753329]] [[51.00369599]]\n",
      "[[42.44739784]] [[26.23746095]] [[74.18581225]]\n",
      "[[42.44739784]] [[14.77269585]] [[24.30169554]]\n",
      "[[42.44739784]] [[20.43750994]] [[62.23948372]]\n",
      "[[42.44739784]] [[26.92286429]] [[66.82356332]]\n",
      "[[42.44739784]] [[25.98121038]] [[65.00767565]]\n",
      "[[42.44739784]] [[24.9311607]] [[64.86158369]]\n",
      "[[42.44739784]] [[22.83117552]] [[62.08315596]]\n",
      "[[42.44739784]] [[45.0273514]] [[81.48255113]]\n",
      "[[42.44739784]] [[30.26037415]] [[74.67877605]]\n",
      "[[42.44739784]] [[41.18476755]] [[82.52481329]]\n",
      "[[42.44739784]] [[52.2919079]] [[143.72091798]]\n",
      "[[42.44739784]] [[39.07241247]] [[79.45112782]]\n",
      "[[42.44739784]] [[29.26440721]] [[76.13237017]]\n",
      "[[42.44739784]] [[113.58913521]] [[159.84661252]]\n",
      "[[42.44739784]] [[41.57052241]] [[77.52419858]]\n",
      "[[42.44739784]] [[42.14560803]] [[0.61085312]]\n",
      "[[42.44739784]] [[53.55718832]] [[90.57772935]]\n",
      "[[42.44739784]] [[18.33673479]] [[62.75922481]]\n",
      "[[42.44739784]] [[25.59119202]] [[68.4246213]]\n",
      "[[42.44739784]] [[25.45492432]] [[37.49319848]]\n",
      "[[42.44739784]] [[2.6369335]] [[49.42600311]]\n",
      "[[42.44739784]] [[32.15944875]] [[74.3998903]]\n",
      "[[42.44739784]] [[28.38563138]] [[67.91057442]]\n",
      "[[42.44739784]] [[38.59364386]] [[80.94133594]]\n",
      "[[42.44739784]] [[3.23680207]] [[49.64466645]]\n",
      "[[42.44739784]] [[54.72603416]] [[144.6823409]]\n",
      "[[42.44739784]] [[51.36420853]] [[142.12740765]]\n",
      "[[42.44739784]] [[106.24670821]] [[156.61506053]]\n",
      "[[42.44739784]] [[48.46545077]] [[86.66791692]]\n",
      "[[42.44739784]] [[22.12861604]] [[64.82594422]]\n",
      "[[42.44739784]] [[13.44909935]] [[59.09958387]]\n",
      "[[42.44739784]] [[14.52568089]] [[59.29038288]]\n",
      "[[42.44739784]] [[55.47412011]] [[153.15327628]]\n",
      "[[42.44739784]] [[135.15383796]] [[180.40901214]]\n",
      "[[42.44739784]] [[55.54634081]] [[1.94357082]]\n",
      "[[42.44739784]] [[20.14976445]] [[63.56281927]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    ModelDiff_np(grad_locals_array_np_v2[iter_idx, 2, :], grad_locals_array_np_v2[iter_idx+1, i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.75913169]] [[44.75922529]] [[2.44796543e-08]]\n",
      "0\n",
      "[[44.75599209]] [[44.75599209]] [[0.]]\n",
      "1\n",
      "[[44.75599209]] [[44.75589091]] [[1.9070126e-06]]\n",
      "2\n",
      "[[44.75599209]] [[44.75594659]] [[1.76214213e-06]]\n",
      "3\n",
      "[[44.75599209]] [[44.75594681]] [[1.80246748e-06]]\n",
      "4\n",
      "[[44.75599209]] [[44.75598224]] [[4.01437614e-06]]\n",
      "5\n",
      "[[44.75599209]] [[44.75602842]] [[3.60583474e-07]]\n",
      "6\n",
      "[[44.75599209]] [[44.75592872]] [[2.00917795e-06]]\n",
      "7\n",
      "[[44.75599209]] [[44.7557381]] [[1.62897341e-06]]\n",
      "8\n",
      "[[44.75599209]] [[44.75569893]] [[1.75513431e-06]]\n",
      "9\n",
      "[[44.75599209]] [[44.75593927]] [[1.84431731e-06]]\n",
      "10\n",
      "[[44.75599209]] [[44.75587031]] [[2.36578366e-06]]\n",
      "11\n",
      "[[44.75599209]] [[44.75591446]] [[1.98395105e-06]]\n",
      "12\n",
      "[[44.75599209]] [[44.7560544]] [[1.75044374e-06]]\n",
      "13\n",
      "[[44.75599209]] [[44.75599681]] [[2.15224022e-06]]\n",
      "14\n",
      "[[44.75599209]] [[44.75602501]] [[3.80992038e-06]]\n",
      "15\n",
      "[[44.75599209]] [[44.75570522]] [[1.70164751e-06]]\n",
      "16\n",
      "[[44.75599209]] [[44.75598649]] [[2.08211805e-06]]\n",
      "17\n",
      "[[44.75599209]] [[44.75591161]] [[1.87072862e-06]]\n",
      "18\n",
      "[[44.75599209]] [[44.75589509]] [[2.00939914e-06]]\n",
      "19\n",
      "[[44.75599209]] [[44.75583676]] [[3.00084697e-06]]\n",
      "20\n",
      "[[44.75599209]] [[44.75580158]] [[3.38749397e-06]]\n",
      "21\n",
      "[[44.75599209]] [[44.75604226]] [[1.95863006e-06]]\n",
      "22\n",
      "[[44.75599209]] [[44.7560032]] [[4.18693902e-06]]\n",
      "23\n",
      "[[44.75599209]] [[44.75604442]] [[1.71950832e-06]]\n",
      "24\n",
      "[[44.75599209]] [[44.75590061]] [[2.30914691e-06]]\n",
      "25\n",
      "[[44.75599209]] [[44.75603425]] [[1.7920539e-06]]\n",
      "26\n",
      "[[44.75599209]] [[44.75585842]] [[3.86171942e-06]]\n",
      "27\n",
      "[[44.75599209]] [[44.75586119]] [[2.60292479e-06]]\n",
      "28\n",
      "[[44.75599209]] [[44.75602481]] [[3.66161101e-08]]\n",
      "29\n",
      "[[44.75599209]] [[44.75600578]] [[3.54189515e-06]]\n",
      "30\n",
      "[[44.75599209]] [[44.75587882]] [[1.97680294e-06]]\n",
      "31\n",
      "[[44.75599209]] [[44.75593504]] [[3.58737471e-06]]\n",
      "32\n",
      "[[44.75599209]] [[44.75600929]] [[8.67619126e-08]]\n",
      "33\n",
      "[[44.75599209]] [[44.75594244]] [[3.52933568e-06]]\n",
      "34\n",
      "[[44.75599209]] [[44.75599097]] [[3.62967971e-06]]\n",
      "35\n",
      "[[44.75599209]] [[44.75599809]] [[2.17370199e-06]]\n",
      "36\n",
      "[[44.75599209]] [[44.75591892]] [[1.80398548e-06]]\n",
      "37\n",
      "[[44.75599209]] [[44.75569104]] [[1.85174895e-06]]\n",
      "38\n",
      "[[44.75599209]] [[44.75593757]] [[1.78904443e-06]]\n",
      "39\n",
      "[[44.75599209]] [[44.75593118]] [[1.62942246e-06]]\n"
     ]
    }
   ],
   "source": [
    "ModelDiff_np(w_locals_array_np[198,0,:], w_locals_array_np[199,0,:])\n",
    "for i in range(N):\n",
    "    print(i)\n",
    "    ModelDiff_np(w_locals_array_np[150,0,:], w_locals_array_np[150,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 50)\n",
      "40\n",
      "[[44.75599209]] [[44.75361664]] [[4.90629811e-06]]\n",
      "[[44.75589091]] [[44.77143191]] [[8.51837026e-05]]\n",
      "[[44.75594659]] [[44.76980173]] [[7.76402974e-05]]\n",
      "[[44.75594681]] [[44.75446699]] [[5.1892672e-06]]\n",
      "[[44.75598224]] [[44.74636244]] [[2.81149664e-05]]\n",
      "[[44.75602842]] [[44.74154127]] [[8.42541597e-05]]\n",
      "[[44.75592872]] [[44.74842066]] [[3.15607891e-05]]\n",
      "[[44.7557381]] [[44.75835744]] [[5.33760941e-06]]\n",
      "[[44.75569893]] [[44.77055598]] [[7.45944303e-05]]\n",
      "[[44.75593927]] [[44.76854604]] [[5.70246892e-05]]\n",
      "[[44.75587031]] [[44.75847165]] [[6.42599425e-06]]\n",
      "[[44.75591446]] [[44.7644094]] [[2.88943412e-05]]\n",
      "[[44.7560544]] [[44.77463761]] [[0.00012428]]\n",
      "[[44.75599681]] [[44.7783401]] [[0.00017312]]\n",
      "[[44.75602501]] [[44.76054495]] [[1.53996658e-05]]\n",
      "[[44.75570522]] [[44.75710806]] [[3.35002451e-06]]\n",
      "[[44.75598649]] [[44.77332986]] [[0.00010532]]\n",
      "[[44.75591161]] [[44.76235669]] [[1.41051588e-05]]\n",
      "[[44.75589509]] [[44.75249422]] [[6.82477079e-06]]\n",
      "[[44.75583676]] [[44.75238997]] [[4.8753227e-06]]\n",
      "[[44.75580158]] [[44.74304907]] [[6.45056669e-05]]\n",
      "[[44.75604226]] [[44.74830148]] [[2.72043446e-05]]\n",
      "[[44.7560032]] [[44.76799352]] [[6.07319299e-05]]\n",
      "[[44.75604442]] [[44.76212999]] [[1.55436264e-05]]\n",
      "[[44.75590061]] [[44.75742965]] [[9.47113322e-06]]\n",
      "[[44.75603425]] [[44.76975387]] [[9.07767986e-05]]\n",
      "[[44.75585842]] [[44.76220645]] [[2.36691624e-05]]\n",
      "[[44.75586119]] [[44.75797158]] [[4.91922398e-06]]\n",
      "[[44.75602481]] [[44.75728928]] [[2.71751395e-06]]\n",
      "[[44.75600578]] [[44.74634556]] [[3.04528567e-05]]\n",
      "[[44.75587882]] [[44.73541762]] [[0.00015256]]\n",
      "[[44.75593504]] [[44.74516356]] [[3.85082482e-05]]\n",
      "[[44.75600929]] [[44.75135491]] [[1.34900624e-05]]\n",
      "[[44.75594244]] [[44.75296414]] [[1.04376939e-05]]\n",
      "[[44.75599097]] [[44.75812839]] [[8.34870151e-06]]\n",
      "[[44.75599809]] [[44.7554895]] [[1.65775681e-05]]\n",
      "[[44.75591892]] [[44.7581913]] [[7.75144635e-06]]\n",
      "[[44.75569104]] [[44.75757424]] [[5.79921293e-06]]\n",
      "[[44.75593757]] [[44.76101769]] [[1.13610799e-05]]\n",
      "\n",
      "\n",
      "[[44.75599209]] [[44.74489387]] [[4.41468933e-05]]\n",
      "[[44.75589091]] [[44.76239043]] [[1.61707856e-05]]\n",
      "[[44.75594659]] [[44.78054126]] [[0.00022271]]\n",
      "[[44.75594681]] [[44.75915738]] [[2.41520489e-05]]\n",
      "[[44.75598224]] [[44.74980541]] [[1.14391985e-05]]\n",
      "[[44.75602842]] [[44.74293453]] [[7.11835016e-05]]\n",
      "[[44.75592872]] [[44.74015655]] [[9.99979908e-05]]\n",
      "[[44.7557381]] [[44.7567375]] [[1.31239783e-05]]\n",
      "[[44.75569893]] [[44.759987]] [[7.38119417e-06]]\n",
      "[[44.75593927]] [[44.78120536]] [[0.00021999]]\n",
      "[[44.75587031]] [[44.7559981]] [[2.48778588e-06]]\n",
      "[[44.75591446]] [[44.76095007]] [[1.55036506e-05]]\n",
      "[[44.7560544]] [[44.76787917]] [[4.89237155e-05]]\n",
      "[[44.75599681]] [[44.78143958]] [[0.00023905]]\n",
      "[[44.75602501]] [[44.77525662]] [[0.00013703]]\n",
      "[[44.75570522]] [[44.74598394]] [[4.21300011e-05]]\n",
      "[[44.75598649]] [[44.76832837]] [[5.84308606e-05]]\n",
      "[[44.75591161]] [[44.77835197]] [[0.00017874]]\n",
      "[[44.75589509]] [[44.74655046]] [[3.85237303e-05]]\n",
      "[[44.75583676]] [[44.75847955]] [[1.45395739e-05]]\n",
      "[[44.75580158]] [[44.74634462]] [[4.08052806e-05]]\n",
      "[[44.75604226]] [[44.73977068]] [[0.00010422]]\n",
      "[[44.7560032]] [[44.75689092]] [[5.67012306e-06]]\n",
      "[[44.75604442]] [[44.77918596]] [[0.00018716]]\n",
      "[[44.75590061]] [[44.74528968]] [[5.56063796e-05]]\n",
      "[[44.75603425]] [[44.76969576]] [[8.82507478e-05]]\n",
      "[[44.75585842]] [[44.76981397]] [[9.22091946e-05]]\n",
      "[[44.75586119]] [[44.75465069]] [[5.43960397e-06]]\n",
      "[[44.75602481]] [[44.76130568]] [[1.39535854e-05]]\n",
      "[[44.75600578]] [[44.75329189]] [[8.81168924e-06]]\n",
      "[[44.75587882]] [[44.73943768]] [[0.00010269]]\n",
      "[[44.75593504]] [[44.73141143]] [[0.00021421]]\n",
      "[[44.75600929]] [[44.75906306]] [[7.50056839e-06]]\n",
      "[[44.75594244]] [[44.74369329]] [[6.09243506e-05]]\n",
      "[[44.75599097]] [[44.76230104]] [[2.56724908e-05]]\n",
      "[[44.75599809]] [[44.75397314]] [[1.05350001e-05]]\n",
      "[[44.75591892]] [[44.75701281]] [[2.49571666e-05]]\n",
      "[[44.75569104]] [[44.75939513]] [[1.50300103e-05]]\n",
      "[[44.75593757]] [[44.7557634]] [[4.93815864e-06]]\n",
      "[[44.75593118]] [[44.76629212]] [[3.83072403e-05]]\n",
      "8.553204212382524e-07 1.4593349900327944e-06\n"
     ]
    }
   ],
   "source": [
    "# Pseudo Inversion\n",
    "P_random_tmp = P_random[150:200,:]\n",
    "\n",
    "PT = P_random_tmp.transpose()\n",
    "\n",
    "print(np.shape(PT))\n",
    "\n",
    "PTP = np.matmul(P_random_tmp.transpose(), P_random_tmp)\n",
    "\n",
    "print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "# print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "Pw_glob = np.matmul(PT, w_glob_array_np[150:200,:])\n",
    "w_recon_np = K * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "# ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "l2_diff = np.zeros((N))\n",
    "for i in range(N-1):\n",
    "    l2_diff[i] = ModelDiff_np(w_locals_array_np[150,i,:], (w_recon_np[i]+w_recon_np[i+1])/2)\n",
    "print()\n",
    "print()\n",
    "l2_diff_ = np.zeros((N))\n",
    "for i in range(N):\n",
    "    l2_diff_[i] = ModelDiff_np(w_locals_array_np[150,i,:], w_recon_np[i])\n",
    "\n",
    "print(np.sum(l2_diff)/N, np.sum(l2_diff_)/N)\n",
    "# w_recon_np = K * np.matmul(pinv, w_glob_array_np[10:50,:])\n",
    "# u, s, vh = np.linalg.svd(PTP, full_matrices=True)\n",
    "\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_glob = CNNMnist2(args)\n",
    "net_glob = net_glob.cuda()\n",
    "\n",
    "PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(1399)\n",
    "net_glob.load_state_dict(torch.load(PATH))\n",
    "net_glob.eval()\n",
    "\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate = 0.03\n",
      "\n",
      "[[44.75914183]] [[44.74717326]] [[0.00363376]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   0, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.745508]] [[0.00260459]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   1, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75167145]] [[0.00148008]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   2, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75584201]] [[0.00199148]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   3, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73853152]] [[0.00380181]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   4, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73509129]] [[0.00264]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   5, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75126229]] [[0.00554237]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   6, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73003702]] [[0.00168024]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   7, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74312628]] [[0.00201778]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   8, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75194399]] [[0.00243658]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   9, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73623966]] [[0.00229931]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  10, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73581598]] [[0.00382675]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  11, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73287657]] [[0.00163876]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  12, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73917483]] [[0.0018008]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  13, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74268898]] [[0.00316385]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  14, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74403202]] [[0.00301955]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  15, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73460534]] [[0.00303397]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  16, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75847142]] [[0.00229134]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  17, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75864553]] [[0.00539742]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  18, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74614845]] [[0.00181696]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  19, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75323209]] [[0.00317052]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  20, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76592862]] [[0.00485672]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  21, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75623505]] [[0.00331708]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  22, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75151125]] [[0.00401507]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  23, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74166961]] [[0.00152607]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  24, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.7540484]] [[0.00272964]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  25, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74150827]] [[0.00257306]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  26, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75408349]] [[0.00260377]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  27, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75079171]] [[0.00560243]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  28, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73831614]] [[0.00279121]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  29, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75563007]] [[0.00273879]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  30, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.7512658]] [[0.00156496]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  31, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74565648]] [[0.00302973]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  32, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74192769]] [[0.00180429]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  33, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76184928]] [[0.00559514]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  34, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75353524]] [[0.00637714]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  35, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73639176]] [[0.0035432]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  36, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73954355]] [[0.00186258]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  37, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74662896]] [[0.00329687]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  38, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75298333]] [[0.00160603]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  39, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73903492]] [[0.0028605]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  40, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75234909]] [[0.00279669]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  41, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74727939]] [[0.00190558]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  42, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76999728]] [[0.00358815]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  43, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74839727]] [[0.00134201]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  44, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73938463]] [[0.00301488]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  45, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73755573]] [[0.00290345]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  46, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76231426]] [[0.0022783]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  47, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75357324]] [[0.00315648]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  48, Train average loss 0.234 Test accuracy 89.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.75914183]] [[44.75602801]] [[0.00409638]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  49, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75964361]] [[0.00292233]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  50, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73882214]] [[0.00389103]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  51, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74380418]] [[0.00219089]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  52, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75728524]] [[0.00302731]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  53, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76245473]] [[0.00180006]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  54, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76084807]] [[0.00273444]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  55, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.7424297]] [[0.00298331]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  56, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74665157]] [[0.00167173]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  57, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76784711]] [[0.00258436]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  58, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74200065]] [[0.00570769]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  59, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74508573]] [[0.00112314]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  60, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74502158]] [[0.0019068]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  61, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74775508]] [[0.003371]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  62, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75649844]] [[0.00327494]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  63, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74065404]] [[0.00315371]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  64, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74384407]] [[0.0028829]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  65, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74707974]] [[0.00375009]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  66, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76184657]] [[0.00526091]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  67, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75121379]] [[0.0032924]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  68, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76326012]] [[0.00294198]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  69, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74810346]] [[0.00155083]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  70, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75956461]] [[0.0043846]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  71, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75905287]] [[0.00352081]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  72, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75499576]] [[0.00353418]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  73, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75369787]] [[0.00314181]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  74, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74404512]] [[0.00353487]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  75, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76262488]] [[0.00121524]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  76, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73918578]] [[0.00379715]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  77, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74437318]] [[0.00325613]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  78, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75888978]] [[0.00350226]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  79, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75375765]] [[0.00317545]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  80, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74313794]] [[0.00181718]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  81, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74157877]] [[0.00293576]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  82, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76313502]] [[0.00434174]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  83, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74333565]] [[0.00151897]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  84, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75613394]] [[0.00190808]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  85, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74675666]] [[0.00154504]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  86, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74748586]] [[0.00311587]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  87, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74744923]] [[0.00252141]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  88, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73943715]] [[0.00223115]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  89, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74720156]] [[0.00353503]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  90, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74003935]] [[0.00424722]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  91, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75486239]] [[0.00539287]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  92, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74288901]] [[0.00193242]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  93, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.73917565]] [[0.00183172]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  94, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.76341761]] [[0.00167419]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  95, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.75107991]] [[0.00330931]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  96, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.7492036]] [[0.00150633]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  97, Train average loss 0.234 Test accuracy 89.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.75914183]] [[44.75828516]] [[0.00144202]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  98, Train average loss 0.234 Test accuracy 89.050\n",
      "[[44.75914183]] [[44.74436356]] [[0.00324174]]\n",
      "\n",
      "Test set: Average loss: 0.2714 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  99, Train average loss 0.234 Test accuracy 89.050\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "N = 40\n",
    "K = 8\n",
    "\n",
    "N_trials = 1\n",
    "Max_iter = 100\n",
    "\n",
    "lr_array = [0.03]\n",
    "\n",
    "acc_test_arr  = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "loss_test_arr = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "\n",
    "\n",
    "\n",
    "P_random_v1 = []\n",
    "w_glob_array = []\n",
    "\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "\n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        print()\n",
    "        print('Learning Rate =',args.lr)\n",
    "        print()\n",
    "#         net_glob = CNNMnist2(args)\n",
    "#         net_glob = net_glob.cuda()\n",
    "#         print(net_glob)\n",
    "\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "        \n",
    "#         w_glob_array = []\n",
    "#         w_locals_array = []\n",
    "        \n",
    "        w_locals_array_np_v1 = np.zeros((Max_iter,N,d))\n",
    "        w_glob_array_np_v1 = np.zeros((Max_iter,d))\n",
    "        \n",
    "        for iter in range(Max_iter): #args.epochs\n",
    "            \n",
    "#             args.lr = lr_array[lr_idx]/(500+iter)\n",
    "#             if iter >= 200:\n",
    "#                 args.lr = lr_array[lr_idx] * 0.1\n",
    "#             elif iter >= 300:\n",
    "#                 args.lr = lr_array[lr_idx] * 0.01\n",
    "            \n",
    "            w_locals, loss_locals = [], []\n",
    "            w_locals_all = []\n",
    "            \n",
    "# #             u = np.random.binomial(1, 1-p, size=(N))\n",
    "#             u = np.ones((N,))\n",
    "#             for u_idx in range(N):\n",
    "#                 p_sel = p_per_user[u_idx]\n",
    "#                 u[u_idx] = np.random.binomial(1, 1-p_sel, size=1)[0]\n",
    "            \n",
    "#             result = np.where(u == 1)\n",
    "\n",
    "            ###############################\n",
    "            # 1. Random Selection\n",
    "            ###############################\n",
    "            idxs_users = np.random.choice(N, K, replace=False)\n",
    "\n",
    "            p_tmp = np.zeros(N)\n",
    "            p_tmp[idxs_users] = 1\n",
    "\n",
    "            P_random_v1.append(p_tmp)\n",
    "\n",
    "#             print('Learning Rate =',args.lr)\n",
    "        #     idxs_users = np.random.choice(range(N), K, replace=False)\n",
    "            for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "                \n",
    "                w_locals_all.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "                \n",
    "                if idx in idxs_users:\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    \n",
    "                stt_pos = 0\n",
    "                for k in w.keys():\n",
    "                    tmp1 = w[k].cpu().detach().numpy()\n",
    "                    cur_shape = tmp1.shape\n",
    "                    _d = np.prod(cur_shape)\n",
    "\n",
    "                    end_pos = stt_pos + _d\n",
    "\n",
    "#                     w_glob_array_np[iter,stt_pos:end_pos] = np.reshape(tmp1,(_d,))        \n",
    "\n",
    "                        \n",
    "                    w_locals_array_np_v1[iter,idx,stt_pos:end_pos] = np.reshape(tmp1,(_d,))\n",
    "\n",
    "                    stt_pos = end_pos\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            # update global weights\n",
    "            w_glob = FedAvg(w_locals)\n",
    "            \n",
    "            \n",
    "            stt_pos = 0\n",
    "            for k in w_glob.keys():\n",
    "                tmp2 = w_glob[k].cpu().detach().numpy()\n",
    "                cur_shape = tmp2.shape\n",
    "                _d = np.prod(cur_shape)\n",
    "\n",
    "                end_pos = stt_pos + _d\n",
    "                \n",
    "#                 print(_d, stt_pose, end_pos)\n",
    "\n",
    "                w_glob_array_np_v1[iter,stt_pos:end_pos] = np.reshape(tmp2,(_d,))\n",
    "\n",
    "                stt_pos = end_pos\n",
    "            \n",
    "            \n",
    "#             w_locals_array.append(w_locals_all)\n",
    "            w_glob_array.append(w_glob)\n",
    "            \n",
    "            ModelDiff_tensor(net_glob.state_dict(), w_glob_array[iter])     \n",
    "            \n",
    "            # copy weight to net_glob\n",
    "            if iter > 500:\n",
    "                print('net_glob is updated !!')\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "#             else:\n",
    "#                 net_glob.load_state_dict(w_glob_prev)\n",
    "\n",
    "            # print loss\n",
    "            loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "\n",
    "    #         loss_train.append(loss_avg)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test_arr[lr_idx][trial_idx][iter]  = acc_test\n",
    "            loss_test_arr[lr_idx][trial_idx][iter] = loss_test\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Train average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "            #print(loss_train)\n",
    "            \n",
    "#             if iter % 100 == 99:\n",
    "#                 PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(iter)\n",
    "#                 torch.save(net_glob.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_glob = CNNMnist2(args)\n",
    "net_glob = net_glob.cuda()\n",
    "\n",
    "PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(1399)\n",
    "net_glob.load_state_dict(torch.load(PATH))\n",
    "net_glob.eval()\n",
    "\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "w_glob_array_np_v1_org = np.zeros((d,))\n",
    "            \n",
    "stt_pos = 0\n",
    "for k in w_glob.keys():\n",
    "    tmp2 = w_glob[k].cpu().detach().numpy()\n",
    "    cur_shape = tmp2.shape\n",
    "    _d = np.prod(cur_shape)\n",
    "\n",
    "    end_pos = stt_pos + _d\n",
    "\n",
    "#                 print(_d, stt_pose, end_pos)\n",
    "\n",
    "    w_glob_array_np_v1_org[stt_pos:end_pos] = np.reshape(tmp2,(_d,))\n",
    "\n",
    "    stt_pos = end_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 40)\n",
      "(100, 40, 62346)\n",
      "(100, 62346)\n",
      "[[0.00363376]] [[0.03373541]] [[0.03101014]]\n",
      "[[0.00260459]] [[0.03373541]] [[0.04222626]]\n",
      "[[0.00148008]] [[0.03373541]] [[0.04095679]]\n",
      "[[0.00199148]] [[0.03373541]] [[0.03553154]]\n",
      "[[0.00380181]] [[0.03373541]] [[0.03178929]]\n",
      "[[0.00264]] [[0.03373541]] [[0.03848967]]\n",
      "[[0.00554237]] [[0.03373541]] [[0.03858597]]\n",
      "[[0.00168024]] [[0.03373541]] [[0.03917936]]\n",
      "[[0.00201778]] [[0.03373541]] [[0.03353628]]\n",
      "[[0.00243658]] [[0.03373541]] [[0.02281142]]\n",
      "[[0.00229931]] [[0.03373541]] [[0.03069973]]\n",
      "[[0.00382675]] [[0.03373541]] [[0.0450255]]\n",
      "[[0.00163876]] [[0.03373541]] [[0.03321811]]\n",
      "[[0.0018008]] [[0.03373541]] [[0.03849655]]\n",
      "[[0.00316385]] [[0.03373541]] [[0.03518007]]\n",
      "[[0.00301955]] [[0.03373541]] [[0.03418658]]\n",
      "[[0.00303397]] [[0.03373541]] [[0.03505186]]\n",
      "[[0.00229134]] [[0.03373541]] [[0.03242318]]\n",
      "[[0.00539742]] [[0.03373541]] [[0.03754665]]\n",
      "[[0.00181696]] [[0.03373541]] [[0.0308132]]\n",
      "[[0.00317052]] [[0.03373541]] [[0.04011153]]\n",
      "[[0.00485672]] [[0.03373541]] [[0.04359519]]\n",
      "[[0.00331708]] [[0.03373541]] [[0.02656138]]\n",
      "[[0.00401507]] [[0.03373541]] [[0.04251561]]\n",
      "[[0.00152607]] [[0.03373541]] [[0.03127511]]\n",
      "[[0.00272964]] [[0.03373541]] [[0.02994563]]\n",
      "[[0.00257306]] [[0.03373541]] [[0.03693369]]\n",
      "[[0.00260377]] [[0.03373541]] [[0.03872744]]\n",
      "[[0.00560243]] [[0.03373541]] [[0.04387601]]\n",
      "[[0.00279121]] [[0.03373541]] [[0.02894813]]\n",
      "[[0.00273879]] [[0.03373541]] [[0.04309663]]\n",
      "[[0.00156496]] [[0.03373541]] [[0.02752269]]\n",
      "[[0.00302973]] [[0.03373541]] [[0.03774253]]\n",
      "[[0.00180429]] [[0.03373541]] [[0.03521563]]\n",
      "[[0.00559513]] [[0.03373541]] [[0.03105996]]\n",
      "[[0.00637714]] [[0.03373541]] [[0.05022523]]\n",
      "[[0.0035432]] [[0.03373541]] [[0.04344439]]\n",
      "[[0.00186258]] [[0.03373541]] [[0.03065444]]\n",
      "[[0.00329687]] [[0.03373541]] [[0.02327789]]\n",
      "[[0.00160603]] [[0.03373541]] [[0.03237719]]\n",
      "[[0.0028605]] [[0.03373541]] [[0.02564345]]\n",
      "[[0.00279669]] [[0.03373541]] [[0.046964]]\n",
      "[[0.00190558]] [[0.03373541]] [[0.03265946]]\n",
      "[[0.00358815]] [[0.03373541]] [[0.03023349]]\n",
      "[[0.00134201]] [[0.03373541]] [[0.0411418]]\n",
      "[[0.00301488]] [[0.03373541]] [[0.03969347]]\n",
      "[[0.00290345]] [[0.03373541]] [[0.03880782]]\n",
      "[[0.0022783]] [[0.03373541]] [[0.02375404]]\n",
      "[[0.00315648]] [[0.03373541]] [[0.0351157]]\n",
      "[[0.00409638]] [[0.03373541]] [[0.0334502]]\n",
      "[[0.00292233]] [[0.03373541]] [[0.04246583]]\n",
      "[[0.00389103]] [[0.03373541]] [[0.04336329]]\n",
      "[[0.00219089]] [[0.03373541]] [[0.02697671]]\n",
      "[[0.00302731]] [[0.03373541]] [[0.02976971]]\n",
      "[[0.00180006]] [[0.03373541]] [[0.02992469]]\n",
      "[[0.00273444]] [[0.03373541]] [[0.03893491]]\n",
      "[[0.00298331]] [[0.03373541]] [[0.03540387]]\n",
      "[[0.00167173]] [[0.03373541]] [[0.02954433]]\n",
      "[[0.00258436]] [[0.03373541]] [[0.03630468]]\n",
      "[[0.00570769]] [[0.03373541]] [[0.02004589]]\n",
      "[[0.00112314]] [[0.03373541]] [[0.04121462]]\n",
      "[[0.0019068]] [[0.03373541]] [[0.03279656]]\n",
      "[[0.003371]] [[0.03373541]] [[0.03203924]]\n",
      "[[0.00327494]] [[0.03373541]] [[0.04509549]]\n",
      "[[0.00315371]] [[0.03373541]] [[0.04281718]]\n",
      "[[0.0028829]] [[0.03373541]] [[0.03210065]]\n",
      "[[0.00375009]] [[0.03373541]] [[0.02990914]]\n",
      "[[0.00526091]] [[0.03373541]] [[0.04667462]]\n",
      "[[0.0032924]] [[0.03373541]] [[0.03061128]]\n",
      "[[0.00294198]] [[0.03373541]] [[0.03051192]]\n",
      "[[0.00155083]] [[0.03373541]] [[0.03852767]]\n",
      "[[0.0043846]] [[0.03373541]] [[0.03467633]]\n",
      "[[0.00352081]] [[0.03373541]] [[0.0267203]]\n",
      "[[0.00353418]] [[0.03373541]] [[0.047273]]\n",
      "[[0.00314181]] [[0.03373541]] [[0.03808141]]\n",
      "[[0.00353487]] [[0.03373541]] [[0.03873964]]\n",
      "[[0.00121524]] [[0.03373541]] [[0.03074714]]\n",
      "[[0.00379715]] [[0.03373541]] [[0.02473995]]\n",
      "[[0.00325613]] [[0.03373541]] [[0.04006166]]\n",
      "[[0.00350226]] [[0.03373541]] [[0.03454774]]\n",
      "[[0.00317545]] [[0.03373541]] [[0.0361234]]\n",
      "[[0.00181718]] [[0.03373541]] [[0.03679507]]\n",
      "[[0.00293576]] [[0.03373541]] [[0.04440389]]\n",
      "[[0.00434174]] [[0.03373541]] [[0.03515843]]\n",
      "[[0.00151897]] [[0.03373541]] [[0.03177267]]\n",
      "[[0.00190808]] [[0.03373541]] [[0.02959887]]\n",
      "[[0.00154504]] [[0.03373541]] [[0.02910126]]\n",
      "[[0.00311587]] [[0.03373541]] [[0.04225495]]\n",
      "[[0.00252141]] [[0.03373541]] [[0.04312055]]\n",
      "[[0.00223115]] [[0.03373541]] [[0.04059568]]\n",
      "[[0.00353503]] [[0.03373541]] [[0.04109123]]\n",
      "[[0.00424722]] [[0.03373541]] [[0.02806718]]\n",
      "[[0.00539287]] [[0.03373541]] [[0.04636102]]\n",
      "[[0.00193242]] [[0.03373541]] [[0.03790075]]\n",
      "[[0.00183172]] [[0.03373541]] [[0.03572613]]\n",
      "[[0.00167419]] [[0.03373541]] [[0.03796583]]\n",
      "[[0.00330931]] [[0.03373541]] [[0.04295848]]\n",
      "[[0.00150633]] [[0.03373541]] [[0.03642931]]\n",
      "[[0.00144202]] [[0.03373541]] [[0.04119954]]\n"
     ]
    }
   ],
   "source": [
    "P_random_v1 = np.array(P_random_v1)\n",
    "\n",
    "print(np.shape(P_random_v1))\n",
    "\n",
    "print(np.shape(w_locals_array_np_v1))\n",
    "print(np.shape(w_glob_array_np_v1))\n",
    "\n",
    "grad_locals_array_np_v1 = np.zeros((100,N,d))\n",
    "grad_glob_array_np_v1 = np.zeros((100,d))\n",
    "\n",
    "for i in range(99):\n",
    "#     print(i)\n",
    "    grad_locals_array_np_v1[i,:,:] = (w_locals_array_np_v1[i+1,:,:] - w_glob_array_np_v1_org)\n",
    "#     grad_locals_array_np_v1[i,:,:] = (w_locals_array_np_v1[i+1,:,:] - w_glob_array_np_v1[i,:])\n",
    "    \n",
    "    tmp_P = np.reshape(P_random_v1[i,:],(1,N))/K\n",
    "#     print(np.shape(tmp_P))\n",
    "#     print(np.shape(grad_locals_array_np_v1[i,:,:]))\n",
    "#     print(tmp_P)\n",
    "    grad_glob_array_np_v1[i,:] = np.reshape(np.matmul(tmp_P, grad_locals_array_np_v1[i,:,:]), (d,))\n",
    "    \n",
    "#     for j in range(N):\n",
    "#         if P_random_v1[i,j] == 1:\n",
    "#             grad_glob_array_np_v1[i,:] = grad_glob_array_np_v1[i,:] + grad_locals_array_np_v1[i,j,:]\n",
    "    \n",
    "    ModelDiff_np(grad_glob_array_np_v1[i,:], grad_locals_array_np_v1[i,0,:])\n",
    "#     print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 50)\n",
      "40\n",
      "[[0.03373541]] [[0.01048901]] [[0.02256364]]\n",
      "[[0.03236988]] [[0.01233108]] [[0.02186596]]\n",
      "[[0.0360242]] [[0.01166868]] [[0.01498924]]\n",
      "[[0.01729164]] [[0.01741887]] [[0.00060738]]\n",
      "[[0.01876086]] [[0.01176124]] [[0.01529103]]\n",
      "[[0.03534368]] [[0.01464963]] [[0.016746]]\n",
      "[[0.02744758]] [[0.01182841]] [[0.0188164]]\n",
      "[[0.03384204]] [[0.03744314]] [[0.00301466]]\n",
      "[[0.04707357]] [[0.01165862]] [[0.02071938]]\n",
      "[[0.01768242]] [[0.01363226]] [[0.01914234]]\n",
      "[[0.04786676]] [[0.01799496]] [[0.01710385]]\n",
      "[[0.02233086]] [[0.01240921]] [[0.01563647]]\n",
      "[[0.03376051]] [[0.01098211]] [[0.01428381]]\n",
      "[[0.01677133]] [[0.0096192]] [[0.00984772]]\n",
      "[[0.02216251]] [[0.00964121]] [[0.01047643]]\n",
      "[[0.01807277]] [[0.00715772]] [[0.01121615]]\n",
      "[[0.01867495]] [[0.01522724]] [[0.01466374]]\n",
      "[[0.041107]] [[0.0130178]] [[0.01402018]]\n",
      "[[0.01296897]] [[0.00873155]] [[0.00909686]]\n",
      "[[0.02268786]] [[0.02384193]] [[0.00163652]]\n",
      "[[0.02826905]] [[0.01280592]] [[0.01657825]]\n",
      "[[0.03049929]] [[0.00880747]] [[0.01137168]]\n",
      "[[0.009859]] [[0.01255199]] [[0.01414407]]\n",
      "[[0.04353311]] [[0.01265439]] [[0.0160529]]\n",
      "[[0.01388148]] [[0.0089623]] [[0.01054398]]\n",
      "[[0.02513109]] [[0.01238564]] [[0.00814992]]\n",
      "[[0.01594003]] [[0.01314966]] [[0.01069826]]\n",
      "[[0.03175581]] [[0.01822985]] [[0.02228507]]\n",
      "[[0.04927403]] [[0.01916998]] [[0.02179802]]\n",
      "[[0.03266197]] [[0.01784349]] [[0.01860069]]\n",
      "[[0.04022638]] [[0.01078922]] [[0.01748997]]\n",
      "[[0.01633201]] [[0.01410975]] [[0.01034121]]\n",
      "[[0.0325699]] [[0.007395]] [[0.01883228]]\n",
      "[[0.01988465]] [[0.00728961]] [[0.00968857]]\n",
      "[[0.01407171]] [[0.00633197]] [[0.00918422]]\n",
      "[[0.01696068]] [[0.00920118]] [[0.00899881]]\n",
      "[[0.01943931]] [[0.01198114]] [[0.01138033]]\n",
      "[[0.02728361]] [[0.01288883]] [[0.01674638]]\n",
      "[[0.03198679]] [[0.01363483]] [[0.01486409]]\n",
      "[[0.02501104]] [[0.01363483]] [[0.01486409]]\n",
      "\n",
      "\n",
      "[[0.03373541]] [[0.03373541]] [[1.13681432e-29]]\n",
      "[[0.03236988]] [[0.03236988]] [[2.54552561e-30]]\n",
      "[[0.0360242]] [[0.0360242]] [[1.25410826e-29]]\n",
      "[[0.01729164]] [[0.01729164]] [[1.26986244e-30]]\n",
      "[[0.01876086]] [[0.01876086]] [[4.50520044e-29]]\n",
      "[[0.03534368]] [[0.03534368]] [[8.39241558e-30]]\n",
      "[[0.02744758]] [[0.02744758]] [[6.81112996e-30]]\n",
      "[[0.03384204]] [[0.03384204]] [[7.38578594e-30]]\n",
      "[[0.04707357]] [[0.04707357]] [[1.77177772e-30]]\n",
      "[[0.01768242]] [[0.01768242]] [[9.78128889e-30]]\n",
      "[[0.04786676]] [[0.04786676]] [[6.75021453e-31]]\n",
      "[[0.02233086]] [[0.02233086]] [[1.53184931e-29]]\n",
      "[[0.03376051]] [[0.03376051]] [[8.02606165e-30]]\n",
      "[[0.01677133]] [[0.01677133]] [[3.96048151e-30]]\n",
      "[[0.02216251]] [[0.02216251]] [[2.98270242e-29]]\n",
      "[[0.01807277]] [[0.01807277]] [[1.50736904e-30]]\n",
      "[[0.01867495]] [[0.01867495]] [[1.51126509e-29]]\n",
      "[[0.041107]] [[0.041107]] [[2.17036507e-30]]\n",
      "[[0.01296897]] [[0.01296897]] [[2.83727692e-30]]\n",
      "[[0.02268786]] [[0.02268786]] [[7.94705316e-30]]\n",
      "[[0.02826905]] [[0.02826905]] [[1.3288025e-29]]\n",
      "[[0.03049929]] [[0.03049929]] [[2.14283911e-30]]\n",
      "[[0.009859]] [[0.009859]] [[3.71838064e-30]]\n",
      "[[0.04353311]] [[0.04353311]] [[5.98048131e-29]]\n",
      "[[0.01388148]] [[0.01388148]] [[3.01596395e-30]]\n",
      "[[0.02513109]] [[0.02513109]] [[1.7574484e-30]]\n",
      "[[0.01594003]] [[0.01594003]] [[1.45838364e-30]]\n",
      "[[0.03175581]] [[0.03175581]] [[1.78164511e-29]]\n",
      "[[0.04927403]] [[0.04927403]] [[1.99322147e-29]]\n",
      "[[0.03266197]] [[0.03266197]] [[1.64884314e-30]]\n",
      "[[0.04022638]] [[0.04022638]] [[1.38211532e-30]]\n",
      "[[0.01633201]] [[0.01633201]] [[1.29002298e-30]]\n",
      "[[0.0325699]] [[0.0325699]] [[1.69715741e-30]]\n",
      "[[0.01988465]] [[0.01988465]] [[1.54558196e-30]]\n",
      "[[0.01407171]] [[0.01407171]] [[2.36750324e-30]]\n",
      "[[0.01696068]] [[0.01696068]] [[9.35922845e-31]]\n",
      "[[0.01943931]] [[0.01943931]] [[9.06172801e-31]]\n",
      "[[0.02728361]] [[0.02728361]] [[2.10122377e-29]]\n",
      "[[0.03198679]] [[0.03198679]] [[1.17768096e-30]]\n",
      "[[0.02501104]] [[0.02501104]] [[8.76366708e-31]]\n",
      "0.5562234306388947\n",
      "3.402179326241741e-28\n",
      "1.6349033290180316e+27\n"
     ]
    }
   ],
   "source": [
    "# Pseudo Inversion\n",
    "\n",
    "offset = 0\n",
    "P_random_tmp = P_random_v1[40+offset:90+offset,:]\n",
    "\n",
    "PT = P_random_tmp.transpose()\n",
    "\n",
    "print(np.shape(PT))\n",
    "\n",
    "PTP = np.matmul(P_random_tmp.transpose(), P_random_tmp)\n",
    "\n",
    "print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "# print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "Pw_glob = np.matmul(PT, grad_glob_array_np_v1[40:90,:])\n",
    "grad_recon_np = K * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "# ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "l2_diff = np.zeros((N))\n",
    "l2_diff_ = np.zeros((N))\n",
    "for i in range(N):\n",
    "    if i == N-1:\n",
    "        l2_diff_[i] = ModelDiff_np(grad_locals_array_np_v1[40,i,:], (grad_recon_np[i-1]+grad_recon_np[i])/2)\n",
    "    else:\n",
    "        l2_diff_[i] = ModelDiff_np(grad_locals_array_np_v1[40,i,:], (grad_recon_np[i]+grad_recon_np[i+1])/2)\n",
    "print()\n",
    "print()\n",
    "for i in range(N):\n",
    "    l2_diff[i] = ModelDiff_np(grad_locals_array_np_v1[40,i,:], (grad_recon_np[i]))\n",
    "\n",
    "print(np.sum(l2_diff_)/N)\n",
    "print(np.sum(l2_diff)/N)\n",
    "\n",
    "print(np.sum(l2_diff_)/np.sum(l2_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 50)\n",
      "40\n",
      "[[44.74527544]] [[44.74423246]] [[3.92020703e-05]]\n",
      "[[44.69990434]] [[44.6741998]] [[0.01827155]]\n",
      "[[44.68503851]] [[44.65838646]] [[0.01140505]]\n",
      "[[44.6545448]] [[44.6704567]] [[0.01148828]]\n",
      "[[44.70934534]] [[44.68937098]] [[0.01845699]]\n",
      "[[44.70631076]] [[44.69045022]] [[0.02362571]]\n",
      "[[44.72184091]] [[44.67270129]] [[0.01548066]]\n",
      "[[44.65452279]] [[44.62459217]] [[0.00576976]]\n",
      "[[44.606201]] [[44.59303721]] [[0.0006662]]\n",
      "[[44.58120589]] [[44.62915121]] [[0.01077222]]\n",
      "[[44.69864105]] [[44.65501601]] [[0.02070602]]\n",
      "[[44.65280296]] [[44.65631088]] [[0.00147398]]\n",
      "[[44.66276693]] [[44.66339528]] [[0.00819037]]\n",
      "[[44.68040443]] [[44.65223168]] [[0.00930277]]\n",
      "[[44.64266417]] [[44.64279152]] [[0.01223596]]\n",
      "[[44.66739048]] [[44.61444307]] [[0.01179359]]\n",
      "[[44.58508289]] [[44.60973182]] [[0.00956386]]\n",
      "[[44.65350925]] [[44.66304426]] [[0.01853445]]\n",
      "[[44.70964847]] [[44.69591597]] [[0.00354569]]\n",
      "[[44.68927441]] [[44.66179522]] [[0.03135927]]\n",
      "[[44.69703478]] [[44.6845992]] [[0.00285973]]\n",
      "[[44.67788334]] [[44.66921298]] [[0.01443521]]\n",
      "[[44.68941281]] [[44.66647978]] [[0.01500282]]\n",
      "[[44.67355152]] [[44.66238667]] [[0.01480622]]\n",
      "[[44.68083379]] [[44.68757313]] [[0.02101761]]\n",
      "[[44.7363479]] [[44.69036276]] [[0.01902924]]\n",
      "[[44.68243613]] [[44.70998314]] [[0.07042748]]\n",
      "[[44.87838553]] [[44.71128563]] [[0.06868102]]\n",
      "[[44.68154798]] [[44.65689757]] [[0.0220602]]\n",
      "[[44.67636689]] [[44.64543738]] [[0.01997503]]\n",
      "[[44.65445722]] [[44.63665879]] [[0.01642266]]\n",
      "[[44.65170614]] [[44.71223777]] [[0.10389244]]\n",
      "[[44.98055503]] [[44.74317258]] [[0.0914232]]\n",
      "[[44.6886366]] [[44.6839593]] [[0.03419356]]\n",
      "[[44.7476689]] [[44.80298602]] [[0.00890754]]\n",
      "[[44.87611872]] [[44.70018459]] [[0.06682704]]\n",
      "[[44.65790554]] [[44.67333336]] [[0.00984706]]\n",
      "[[44.70845595]] [[44.64141623]] [[0.01049801]]\n",
      "[[44.59537262]] [[44.65589375]] [[0.01923795]]\n",
      "[[44.75488994]] [[44.73449583]] [[0.02529182]]\n",
      "\n",
      "\n",
      "[[44.69990434]] [[44.69990445]] [[1.22275888e-12]]\n",
      "[[44.68503851]] [[44.68503824]] [[2.19281889e-12]]\n",
      "[[44.6545448]] [[44.65454478]] [[1.26444802e-12]]\n",
      "[[44.70934534]] [[44.70934516]] [[1.05131993e-12]]\n",
      "[[44.70631076]] [[44.70631079]] [[1.90332551e-12]]\n",
      "[[44.72184091]] [[44.72184108]] [[2.99433159e-12]]\n",
      "[[44.65452279]] [[44.65452283]] [[2.10013103e-12]]\n",
      "[[44.606201]] [[44.60620104]] [[9.15306328e-13]]\n",
      "[[44.58120589]] [[44.58120579]] [[3.15990611e-12]]\n",
      "[[44.69864105]] [[44.69864106]] [[8.83079664e-13]]\n",
      "[[44.65280296]] [[44.652803]] [[9.62702868e-13]]\n",
      "[[44.66276693]] [[44.66276671]] [[2.88926724e-12]]\n",
      "[[44.68040443]] [[44.6804046]] [[2.67561614e-12]]\n",
      "[[44.64266417]] [[44.64266431]] [[7.88597747e-13]]\n",
      "[[44.66739048]] [[44.66739063]] [[1.54073169e-12]]\n",
      "[[44.58508289]] [[44.58508268]] [[1.42029239e-12]]\n",
      "[[44.65350925]] [[44.6535087]] [[1.72308197e-12]]\n",
      "[[44.70964847]] [[44.70964873]] [[5.57886656e-12]]\n",
      "[[44.68927441]] [[44.68927461]] [[1.80779527e-12]]\n",
      "[[44.69703478]] [[44.69703436]] [[1.87679149e-12]]\n",
      "[[44.67788334]] [[44.67788349]] [[1.25112996e-12]]\n",
      "[[44.68941281]] [[44.68941288]] [[1.06904377e-12]]\n",
      "[[44.67355152]] [[44.67355233]] [[4.02715349e-12]]\n",
      "[[44.68083379]] [[44.68083344]] [[2.7612558e-12]]\n",
      "[[44.7363479]] [[44.73634803]] [[1.78411472e-12]]\n",
      "[[44.68243613]] [[44.68243597]] [[2.36570783e-12]]\n",
      "[[44.87838553]] [[44.87838525]] [[1.73015711e-12]]\n",
      "[[44.68154798]] [[44.68154806]] [[2.03211902e-12]]\n",
      "[[44.67636689]] [[44.67636748]] [[1.31424398e-12]]\n",
      "[[44.65445722]] [[44.65445734]] [[1.90240664e-12]]\n",
      "[[44.65170614]] [[44.65170559]] [[1.41234511e-12]]\n",
      "[[44.98055503]] [[44.98055485]] [[1.80085537e-12]]\n",
      "[[44.6886366]] [[44.68863671]] [[2.48756798e-12]]\n",
      "[[44.7476689]] [[44.74766899]] [[2.16234442e-12]]\n",
      "[[44.87611872]] [[44.87611813]] [[1.87083629e-12]]\n",
      "[[44.65790554]] [[44.65790515]] [[2.01352427e-12]]\n",
      "[[44.70845595]] [[44.70845569]] [[4.33848016e-12]]\n",
      "[[44.59537262]] [[44.59537279]] [[2.01960663e-12]]\n",
      "[[44.75488994]] [[44.75489061]] [[1.79979434e-12]]\n",
      "[[44.76468461]] [[44.76468467]] [[3.79213766e-12]]\n",
      "0.000501514503074638 4.635909053403189e-14\n"
     ]
    }
   ],
   "source": [
    "# Pseudo Inversion\n",
    "P_random_v1 = np.array(P_random_v1)\n",
    "P_random_tmp_v1 = np.array(P_random_v1[:,:])\n",
    "\n",
    "PT = P_random_tmp_v1.transpose()\n",
    "\n",
    "print(np.shape(PT))\n",
    "\n",
    "PTP = np.matmul(P_random_tmp_v1.transpose(), P_random_tmp_v1)\n",
    "\n",
    "print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "# print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "Pw_glob = np.matmul(PT, w_glob_array_np_v1[:,:])\n",
    "w_recon_np_v1 = 8 * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "l2_diff = np.zeros((N))\n",
    "for i in range(N-1):\n",
    "    l2_diff[i] = ModelDiff_np(w_locals_array_np_v1[0,i,:], (w_recon_np_v1[i]+w_recon_np_v1[i+1])/2)\n",
    "print()\n",
    "print()\n",
    "l2_diff_ = np.zeros((N))\n",
    "for i in range(N):\n",
    "    l2_diff_[i] = ModelDiff_np(w_locals_array_np_v1[0,i,:], w_recon_np_v1[i])\n",
    "\n",
    "print(np.sum(l2_diff)/N, np.sum(l2_diff_)/N)\n",
    "# w_recon_np = K * np.matmul(pinv, w_glob_array_np[10:50,:])\n",
    "# u, s, vh = np.linalg.svd(PTP, full_matrices=True)\n",
    "\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=l2_diff_, bins=20, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('My Very Own Histogram')\n",
    "# plt.text(23, 45, r'$mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=22)\n",
    "# plt.savefig('./plots/error_histo_proposed.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.savefig('./plots/error_histo_proposed.eps', format='eps',dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=l2_diff*0.1, bins=20, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('My Very Own Histogram')\n",
    "# plt.text(23, 45, r'$mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=22)\n",
    "# plt.savefig('./plots/error_histo_random.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.savefig('./plots/error_histo_random.eps', format='eps',dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.856164383561644"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.55/1.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
