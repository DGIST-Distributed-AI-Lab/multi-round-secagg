{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, cifar_noniid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# from sympy import * \n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 40  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep=1 #\"the number of local epochs: E\"\n",
    "    local_bs=100 #\"local batch size: B\"\n",
    "    bs=100 #\"test batch size\"\n",
    "    lr=0.03 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    weight_decay = 5e-4\n",
    "    opt = 'SGD' #'ADAM'\n",
    "    loss = 'Cross'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    if args.iid:\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = cifar_noniid(dataset_train, args.num_users)\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.3 0.4 0.3 0.3 0.5 0.1 0.3 0.4 0.2 0.4 0.2 0.4 0.4 0.5 0.2 0.5 0.2\n",
      " 0.4 0.5 0.3 0.2 0.4 0.4 0.5 0.5 0.1 0.4 0.1 0.5 0.3 0.2 0.2 0.5 0.3 0.4\n",
      " 0.5 0.5 0.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "N = args.num_users\n",
    "\n",
    "p_array = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "# print(p_matrix)\n",
    "p_sel = np.random.randint(low=0, high=len(p_array), size=(N,))\n",
    "\n",
    "p_per_user = np.ones((N,))\n",
    "\n",
    "for i in range(N):\n",
    "    p_per_user[i] = p_array[p_sel[i]]\n",
    "\n",
    "print(p_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelDiff_tensor(w1,w2):\n",
    "    w_avg = copy.deepcopy(w1)\n",
    "    w1_np = np.zeros((1,1))\n",
    "    w2_np = np.zeros((1,1))\n",
    "    for k in w_avg.keys():\n",
    "        tmp1 = w1[k].cpu().detach().numpy()\n",
    "        tmp2 = w2[k].cpu().detach().numpy()\n",
    "        cur_shape = tmp1.shape\n",
    "        _d = np.prod(cur_shape)\n",
    "        \n",
    "        tmp1 = np.reshape(tmp1,(1,_d))\n",
    "        tmp2 = np.reshape(tmp2,(1,_d))\n",
    "        \n",
    "        dist = tmp1 - tmp2\n",
    "        \n",
    "        pow1 = np.matmul(tmp1, tmp1.transpose())\n",
    "        pow2 = np.matmul(tmp2, tmp2.transpose())\n",
    "        \n",
    "        dist_l2 = np.matmul(dist, dist.transpose())\n",
    "        \n",
    "#         print(k,cur_shape)\n",
    "#         print(pow1, pow2, dist_l2)\n",
    "#         print()\n",
    "        \n",
    "        w1_np = np.concatenate([w1_np,tmp1], axis=1)\n",
    "        w2_np = np.concatenate([w2_np,tmp2], axis=1)\n",
    "    \n",
    "    w1_l2 = np.matmul(w1_np, w1_np.transpose())\n",
    "    w2_l2 = np.matmul(w2_np, w2_np.transpose())\n",
    "    \n",
    "    dist = w1_np - w2_np\n",
    "    dist_l2 = np.matmul(dist, dist.transpose())\n",
    "    \n",
    "    print(w1_l2, w2_l2, dist_l2)\n",
    "    \n",
    "    return dist_l2/w1_l2\n",
    "\n",
    "def ModelDiff_np(w1,w2):\n",
    "    cur_shape = w1.shape\n",
    "    _d = np.prod(cur_shape)\n",
    "    \n",
    "    tmp1 = np.reshape(w1,(1,_d))\n",
    "    tmp2 = np.reshape(w2,(1,_d))\n",
    "        \n",
    "    dist = tmp1 - tmp2\n",
    "        \n",
    "    pow1 = np.matmul(tmp1, tmp1.transpose())\n",
    "    pow2 = np.matmul(tmp2, tmp2.transpose())\n",
    "        \n",
    "    dist_l2 = np.matmul(dist, dist.transpose())\n",
    "    \n",
    "    print(pow1, pow2, dist_l2)\n",
    "    \n",
    "    return dist_l2/pow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62346\n"
     ]
    }
   ],
   "source": [
    "from models.Nets import *\n",
    "def tensor_dim(w0):\n",
    "    model_dim = 0\n",
    "    \n",
    "    w_avg = copy.deepcopy(w0)\n",
    "\n",
    "    for k in w_avg.keys():\n",
    "        tmp1 = w0[k].cpu().detach().numpy()\n",
    "        cur_shape = tmp1.shape\n",
    "        _d = np.prod(cur_shape)\n",
    "        \n",
    "        model_dim = model_dim + _d\n",
    "        \n",
    "    return model_dim\n",
    "\n",
    "net_glob = CNNMnist3(args)\n",
    "net_glob = net_glob.cuda()\n",
    "\n",
    "net_glob.train()\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "d = tensor_dim(w_glob)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate = 0.03\n",
      "\n",
      "CNNMnist2(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "[[35.31202944]] [[35.8765838]] [[0.26753488]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.9165 \n",
      "Accuracy: 4026/10000 (40.26%)\n",
      "\n",
      "Round   0, Train average loss 2.152 Test accuracy 40.260\n",
      "[[35.8765838]] [[37.47843597]] [[0.48986032]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.2368 \n",
      "Accuracy: 6118/10000 (61.18%)\n",
      "\n",
      "Round   1, Train average loss 1.600 Test accuracy 61.180\n",
      "[[37.47843597]] [[38.66248373]] [[0.14775934]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 1.1252 \n",
      "Accuracy: 6584/10000 (65.84%)\n",
      "\n",
      "Round   2, Train average loss 1.224 Test accuracy 65.840\n",
      "[[38.66248373]] [[39.54896333]] [[0.07640478]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9960 \n",
      "Accuracy: 6847/10000 (68.47%)\n",
      "\n",
      "Round   3, Train average loss 1.083 Test accuracy 68.470\n",
      "[[39.54896333]] [[40.24146293]] [[0.04342062]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9545 \n",
      "Accuracy: 6990/10000 (69.90%)\n",
      "\n",
      "Round   4, Train average loss 1.011 Test accuracy 69.900\n",
      "[[40.24146293]] [[40.83750199]] [[0.03034022]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9264 \n",
      "Accuracy: 7213/10000 (72.13%)\n",
      "\n",
      "Round   5, Train average loss 0.970 Test accuracy 72.130\n",
      "[[40.83750199]] [[41.33655191]] [[0.02240752]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.9085 \n",
      "Accuracy: 7203/10000 (72.03%)\n",
      "\n",
      "Round   6, Train average loss 0.940 Test accuracy 72.030\n",
      "[[41.33655191]] [[41.76925118]] [[0.01862073]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8850 \n",
      "Accuracy: 7349/10000 (73.49%)\n",
      "\n",
      "Round   7, Train average loss 0.918 Test accuracy 73.490\n",
      "[[41.76925118]] [[42.18807902]] [[0.01518515]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8766 \n",
      "Accuracy: 7344/10000 (73.44%)\n",
      "\n",
      "Round   8, Train average loss 0.900 Test accuracy 73.440\n",
      "[[42.18807902]] [[42.54481473]] [[0.01265059]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.8624 \n",
      "Accuracy: 7417/10000 (74.17%)\n",
      "\n",
      "Round   9, Train average loss 0.885 Test accuracy 74.170\n",
      "[[42.54481473]] [[42.9997951]] [[0.08158079]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.6358 \n",
      "Accuracy: 7906/10000 (79.06%)\n",
      "\n",
      "Round  10, Train average loss 0.719 Test accuracy 79.060\n",
      "[[42.9997951]] [[43.68273335]] [[0.14412925]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.1652 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  11, Train average loss 0.359 Test accuracy 95.820\n",
      "[[43.68273335]] [[44.13894055]] [[0.02324441]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.1409 \n",
      "Accuracy: 9611/10000 (96.11%)\n",
      "\n",
      "Round  12, Train average loss 0.187 Test accuracy 96.110\n",
      "[[44.13894055]] [[44.46103311]] [[0.01456779]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.1267 \n",
      "Accuracy: 9655/10000 (96.55%)\n",
      "\n",
      "Round  13, Train average loss 0.169 Test accuracy 96.550\n",
      "[[44.46103311]] [[44.78261366]] [[0.01227092]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.1179 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round  14, Train average loss 0.155 Test accuracy 96.780\n",
      "[[44.78261366]] [[45.07753541]] [[0.01184223]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.1116 \n",
      "Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Round  15, Train average loss 0.145 Test accuracy 96.820\n",
      "[[45.07753541]] [[45.35815317]] [[0.00943049]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.1072 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  16, Train average loss 0.136 Test accuracy 97.070\n",
      "[[45.35815317]] [[45.58488351]] [[0.00799338]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0992 \n",
      "Accuracy: 9718/10000 (97.18%)\n",
      "\n",
      "Round  17, Train average loss 0.130 Test accuracy 97.180\n",
      "[[45.58488351]] [[45.78121686]] [[0.00730191]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0971 \n",
      "Accuracy: 9717/10000 (97.17%)\n",
      "\n",
      "Round  18, Train average loss 0.124 Test accuracy 97.170\n",
      "[[45.78121686]] [[46.03758191]] [[0.00699966]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0925 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "Round  19, Train average loss 0.118 Test accuracy 97.400\n",
      "[[46.03758191]] [[46.25158061]] [[0.00630595]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0895 \n",
      "Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "Round  20, Train average loss 0.114 Test accuracy 97.340\n",
      "[[46.25158061]] [[46.47551128]] [[0.00550088]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0854 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Round  21, Train average loss 0.110 Test accuracy 97.520\n",
      "[[46.47551128]] [[46.66920614]] [[0.00584194]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0843 \n",
      "Accuracy: 9754/10000 (97.54%)\n",
      "\n",
      "Round  22, Train average loss 0.107 Test accuracy 97.540\n",
      "[[46.66920614]] [[46.83617932]] [[0.00541424]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0803 \n",
      "Accuracy: 9775/10000 (97.75%)\n",
      "\n",
      "Round  23, Train average loss 0.103 Test accuracy 97.750\n",
      "[[46.83617932]] [[47.04861328]] [[0.00560873]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0799 \n",
      "Accuracy: 9758/10000 (97.58%)\n",
      "\n",
      "Round  24, Train average loss 0.100 Test accuracy 97.580\n",
      "[[47.04861328]] [[47.20860954]] [[0.00602698]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0758 \n",
      "Accuracy: 9775/10000 (97.75%)\n",
      "\n",
      "Round  25, Train average loss 0.098 Test accuracy 97.750\n",
      "[[47.20860954]] [[47.40166073]] [[0.00454576]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0744 \n",
      "Accuracy: 9776/10000 (97.76%)\n",
      "\n",
      "Round  26, Train average loss 0.095 Test accuracy 97.760\n",
      "[[47.40166073]] [[47.55154515]] [[0.00489352]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0719 \n",
      "Accuracy: 9778/10000 (97.78%)\n",
      "\n",
      "Round  27, Train average loss 0.093 Test accuracy 97.780\n",
      "[[47.55154515]] [[47.71714026]] [[0.00517257]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0702 \n",
      "Accuracy: 9798/10000 (97.98%)\n",
      "\n",
      "Round  28, Train average loss 0.091 Test accuracy 97.980\n",
      "[[47.71714026]] [[47.86301587]] [[0.00509028]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0708 \n",
      "Accuracy: 9779/10000 (97.79%)\n",
      "\n",
      "Round  29, Train average loss 0.089 Test accuracy 97.790\n",
      "[[47.86301587]] [[48.00863094]] [[0.00443359]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0666 \n",
      "Accuracy: 9802/10000 (98.02%)\n",
      "\n",
      "Round  30, Train average loss 0.086 Test accuracy 98.020\n",
      "[[48.00863094]] [[48.16383455]] [[0.0040711]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0655 \n",
      "Accuracy: 9798/10000 (97.98%)\n",
      "\n",
      "Round  31, Train average loss 0.084 Test accuracy 97.980\n",
      "[[48.16383455]] [[48.31917437]] [[0.00381324]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0640 \n",
      "Accuracy: 9811/10000 (98.11%)\n",
      "\n",
      "Round  32, Train average loss 0.083 Test accuracy 98.110\n",
      "[[48.31917437]] [[48.47412916]] [[0.00368972]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0635 \n",
      "Accuracy: 9808/10000 (98.08%)\n",
      "\n",
      "Round  33, Train average loss 0.081 Test accuracy 98.080\n",
      "[[48.47412916]] [[48.59726655]] [[0.00330819]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0642 \n",
      "Accuracy: 9803/10000 (98.03%)\n",
      "\n",
      "Round  34, Train average loss 0.080 Test accuracy 98.030\n",
      "[[48.59726655]] [[48.71206464]] [[0.0030694]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0606 \n",
      "Accuracy: 9818/10000 (98.18%)\n",
      "\n",
      "Round  35, Train average loss 0.079 Test accuracy 98.180\n",
      "[[48.71206464]] [[48.81858409]] [[0.00419684]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0593 \n",
      "Accuracy: 9817/10000 (98.17%)\n",
      "\n",
      "Round  36, Train average loss 0.077 Test accuracy 98.170\n",
      "[[48.81858409]] [[48.98309915]] [[0.00377569]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0581 \n",
      "Accuracy: 9824/10000 (98.24%)\n",
      "\n",
      "Round  37, Train average loss 0.075 Test accuracy 98.240\n",
      "[[48.98309915]] [[49.08103544]] [[0.00316767]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0572 \n",
      "Accuracy: 9828/10000 (98.28%)\n",
      "\n",
      "Round  38, Train average loss 0.075 Test accuracy 98.280\n",
      "[[49.08103544]] [[49.23745251]] [[0.00377028]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0553 \n",
      "Accuracy: 9836/10000 (98.36%)\n",
      "\n",
      "Round  39, Train average loss 0.073 Test accuracy 98.360\n",
      "[[49.23745251]] [[49.36386867]] [[0.00350799]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0563 \n",
      "Accuracy: 9825/10000 (98.25%)\n",
      "\n",
      "Round  40, Train average loss 0.072 Test accuracy 98.250\n",
      "[[49.36386867]] [[49.47925616]] [[0.0042959]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0534 \n",
      "Accuracy: 9834/10000 (98.34%)\n",
      "\n",
      "Round  41, Train average loss 0.071 Test accuracy 98.340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49.47925616]] [[49.58778393]] [[0.00315312]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0535 \n",
      "Accuracy: 9837/10000 (98.37%)\n",
      "\n",
      "Round  42, Train average loss 0.070 Test accuracy 98.370\n",
      "[[49.58778393]] [[49.68000209]] [[0.00307202]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0542 \n",
      "Accuracy: 9839/10000 (98.39%)\n",
      "\n",
      "Round  43, Train average loss 0.069 Test accuracy 98.390\n",
      "[[49.68000209]] [[49.80159044]] [[0.00318779]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0537 \n",
      "Accuracy: 9838/10000 (98.38%)\n",
      "\n",
      "Round  44, Train average loss 0.068 Test accuracy 98.380\n",
      "[[49.80159044]] [[49.90893545]] [[0.00282306]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0513 \n",
      "Accuracy: 9840/10000 (98.40%)\n",
      "\n",
      "Round  45, Train average loss 0.067 Test accuracy 98.400\n",
      "[[49.90893545]] [[50.01404213]] [[0.00226231]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0507 \n",
      "Accuracy: 9846/10000 (98.46%)\n",
      "\n",
      "Round  46, Train average loss 0.066 Test accuracy 98.460\n",
      "[[50.01404213]] [[50.12880001]] [[0.00317188]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0505 \n",
      "Accuracy: 9847/10000 (98.47%)\n",
      "\n",
      "Round  47, Train average loss 0.065 Test accuracy 98.470\n",
      "[[50.12880001]] [[50.22898025]] [[0.00292197]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0509 \n",
      "Accuracy: 9848/10000 (98.48%)\n",
      "\n",
      "Round  48, Train average loss 0.065 Test accuracy 98.480\n",
      "[[50.22898025]] [[50.34369762]] [[0.00304081]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0507 \n",
      "Accuracy: 9847/10000 (98.47%)\n",
      "\n",
      "Round  49, Train average loss 0.064 Test accuracy 98.470\n",
      "[[50.34369762]] [[50.44773747]] [[0.00369261]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0489 \n",
      "Accuracy: 9846/10000 (98.46%)\n",
      "\n",
      "Round  50, Train average loss 0.063 Test accuracy 98.460\n",
      "[[50.44773747]] [[50.5400683]] [[0.00386264]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0502 \n",
      "Accuracy: 9840/10000 (98.40%)\n",
      "\n",
      "Round  51, Train average loss 0.062 Test accuracy 98.400\n",
      "[[50.5400683]] [[50.65107065]] [[0.00308758]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0474 \n",
      "Accuracy: 9848/10000 (98.48%)\n",
      "\n",
      "Round  52, Train average loss 0.061 Test accuracy 98.480\n",
      "[[50.65107065]] [[50.73906565]] [[0.00270422]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0471 \n",
      "Accuracy: 9850/10000 (98.50%)\n",
      "\n",
      "Round  53, Train average loss 0.061 Test accuracy 98.500\n",
      "[[50.73906565]] [[50.82717315]] [[0.00419644]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0461 \n",
      "Accuracy: 9853/10000 (98.53%)\n",
      "\n",
      "Round  54, Train average loss 0.060 Test accuracy 98.530\n",
      "[[50.82717315]] [[50.933598]] [[0.00321944]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0467 \n",
      "Accuracy: 9845/10000 (98.45%)\n",
      "\n",
      "Round  55, Train average loss 0.059 Test accuracy 98.450\n",
      "[[50.933598]] [[51.00581416]] [[0.00252403]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0467 \n",
      "Accuracy: 9851/10000 (98.51%)\n",
      "\n",
      "Round  56, Train average loss 0.059 Test accuracy 98.510\n",
      "[[51.00581416]] [[51.09103072]] [[0.0033381]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0459 \n",
      "Accuracy: 9861/10000 (98.61%)\n",
      "\n",
      "Round  57, Train average loss 0.058 Test accuracy 98.610\n",
      "[[51.09103072]] [[51.16728968]] [[0.00349329]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0455 \n",
      "Accuracy: 9862/10000 (98.62%)\n",
      "\n",
      "Round  58, Train average loss 0.058 Test accuracy 98.620\n",
      "[[51.16728968]] [[51.25442835]] [[0.00296064]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0439 \n",
      "Accuracy: 9857/10000 (98.57%)\n",
      "\n",
      "Round  59, Train average loss 0.057 Test accuracy 98.570\n",
      "[[51.25442835]] [[51.33691942]] [[0.00294074]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0444 \n",
      "Accuracy: 9864/10000 (98.64%)\n",
      "\n",
      "Round  60, Train average loss 0.056 Test accuracy 98.640\n",
      "[[51.33691942]] [[51.4295133]] [[0.00286804]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0426 \n",
      "Accuracy: 9864/10000 (98.64%)\n",
      "\n",
      "Round  61, Train average loss 0.055 Test accuracy 98.640\n",
      "[[51.4295133]] [[51.51808119]] [[0.002471]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0432 \n",
      "Accuracy: 9861/10000 (98.61%)\n",
      "\n",
      "Round  62, Train average loss 0.055 Test accuracy 98.610\n",
      "[[51.51808119]] [[51.60576848]] [[0.00327557]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0436 \n",
      "Accuracy: 9856/10000 (98.56%)\n",
      "\n",
      "Round  63, Train average loss 0.054 Test accuracy 98.560\n",
      "[[51.60576848]] [[51.68676353]] [[0.00271448]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0420 \n",
      "Accuracy: 9869/10000 (98.69%)\n",
      "\n",
      "Round  64, Train average loss 0.054 Test accuracy 98.690\n",
      "[[51.68676353]] [[51.7752395]] [[0.00273243]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0413 \n",
      "Accuracy: 9872/10000 (98.72%)\n",
      "\n",
      "Round  65, Train average loss 0.053 Test accuracy 98.720\n",
      "[[51.7752395]] [[51.84914442]] [[0.00296172]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0419 \n",
      "Accuracy: 9867/10000 (98.67%)\n",
      "\n",
      "Round  66, Train average loss 0.053 Test accuracy 98.670\n",
      "[[51.84914442]] [[51.92333564]] [[0.00249311]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0408 \n",
      "Accuracy: 9870/10000 (98.70%)\n",
      "\n",
      "Round  67, Train average loss 0.052 Test accuracy 98.700\n",
      "[[51.92333564]] [[52.00294821]] [[0.00276681]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0415 \n",
      "Accuracy: 9870/10000 (98.70%)\n",
      "\n",
      "Round  68, Train average loss 0.052 Test accuracy 98.700\n",
      "[[52.00294821]] [[52.08720675]] [[0.00256432]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0405 \n",
      "Accuracy: 9873/10000 (98.73%)\n",
      "\n",
      "Round  69, Train average loss 0.051 Test accuracy 98.730\n",
      "[[52.08720675]] [[52.16482222]] [[0.0025831]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0414 \n",
      "Accuracy: 9868/10000 (98.68%)\n",
      "\n",
      "Round  70, Train average loss 0.051 Test accuracy 98.680\n",
      "[[52.16482222]] [[52.25905784]] [[0.00234462]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0405 \n",
      "Accuracy: 9875/10000 (98.75%)\n",
      "\n",
      "Round  71, Train average loss 0.050 Test accuracy 98.750\n",
      "[[52.25905784]] [[52.32642653]] [[0.00198312]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0406 \n",
      "Accuracy: 9872/10000 (98.72%)\n",
      "\n",
      "Round  72, Train average loss 0.050 Test accuracy 98.720\n",
      "[[52.32642653]] [[52.41950816]] [[0.00236323]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0396 \n",
      "Accuracy: 9871/10000 (98.71%)\n",
      "\n",
      "Round  73, Train average loss 0.050 Test accuracy 98.710\n",
      "[[52.41950816]] [[52.49013918]] [[0.00313405]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0393 \n",
      "Accuracy: 9876/10000 (98.76%)\n",
      "\n",
      "Round  74, Train average loss 0.049 Test accuracy 98.760\n",
      "[[52.49013918]] [[52.54808064]] [[0.00322291]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0396 \n",
      "Accuracy: 9871/10000 (98.71%)\n",
      "\n",
      "Round  75, Train average loss 0.049 Test accuracy 98.710\n",
      "[[52.54808064]] [[52.61112321]] [[0.00270463]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0386 \n",
      "Accuracy: 9867/10000 (98.67%)\n",
      "\n",
      "Round  76, Train average loss 0.048 Test accuracy 98.670\n",
      "[[52.61112321]] [[52.68634194]] [[0.00259752]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0402 \n",
      "Accuracy: 9869/10000 (98.69%)\n",
      "\n",
      "Round  77, Train average loss 0.048 Test accuracy 98.690\n",
      "[[52.68634194]] [[52.76015192]] [[0.00295535]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0396 \n",
      "Accuracy: 9871/10000 (98.71%)\n",
      "\n",
      "Round  78, Train average loss 0.048 Test accuracy 98.710\n",
      "[[52.76015192]] [[52.81975942]] [[0.00303024]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0396 \n",
      "Accuracy: 9870/10000 (98.70%)\n",
      "\n",
      "Round  79, Train average loss 0.047 Test accuracy 98.700\n",
      "[[52.81975942]] [[52.88555191]] [[0.00309912]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0402 \n",
      "Accuracy: 9870/10000 (98.70%)\n",
      "\n",
      "Round  80, Train average loss 0.047 Test accuracy 98.700\n",
      "[[52.88555191]] [[52.94899943]] [[0.00316786]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0382 \n",
      "Accuracy: 9873/10000 (98.73%)\n",
      "\n",
      "Round  81, Train average loss 0.047 Test accuracy 98.730\n",
      "[[52.94899943]] [[53.00741049]] [[0.00259605]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0388 \n",
      "Accuracy: 9873/10000 (98.73%)\n",
      "\n",
      "Round  82, Train average loss 0.046 Test accuracy 98.730\n",
      "[[53.00741049]] [[53.07977094]] [[0.00212043]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0376 \n",
      "Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Round  83, Train average loss 0.046 Test accuracy 98.790\n",
      "[[53.07977094]] [[53.15293003]] [[0.00229587]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0374 \n",
      "Accuracy: 9876/10000 (98.76%)\n",
      "\n",
      "Round  84, Train average loss 0.046 Test accuracy 98.760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53.15293003]] [[53.21817781]] [[0.00291759]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0394 \n",
      "Accuracy: 9873/10000 (98.73%)\n",
      "\n",
      "Round  85, Train average loss 0.045 Test accuracy 98.730\n",
      "[[53.21817781]] [[53.28876663]] [[0.00236856]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0372 \n",
      "Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Round  86, Train average loss 0.045 Test accuracy 98.790\n",
      "[[53.28876663]] [[53.36846683]] [[0.00226055]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0382 \n",
      "Accuracy: 9880/10000 (98.80%)\n",
      "\n",
      "Round  87, Train average loss 0.045 Test accuracy 98.800\n",
      "[[53.36846683]] [[53.44562147]] [[0.00174035]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0375 \n",
      "Accuracy: 9878/10000 (98.78%)\n",
      "\n",
      "Round  88, Train average loss 0.045 Test accuracy 98.780\n",
      "[[53.44562147]] [[53.51390911]] [[0.00170359]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0377 \n",
      "Accuracy: 9877/10000 (98.77%)\n",
      "\n",
      "Round  89, Train average loss 0.044 Test accuracy 98.770\n",
      "[[53.51390911]] [[53.57028157]] [[0.0022357]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0367 \n",
      "Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Round  90, Train average loss 0.044 Test accuracy 98.790\n",
      "[[53.57028157]] [[53.63102161]] [[0.00261673]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0360 \n",
      "Accuracy: 9882/10000 (98.82%)\n",
      "\n",
      "Round  91, Train average loss 0.044 Test accuracy 98.820\n",
      "[[53.63102161]] [[53.66020115]] [[0.0038984]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0356 \n",
      "Accuracy: 9883/10000 (98.83%)\n",
      "\n",
      "Round  92, Train average loss 0.043 Test accuracy 98.830\n",
      "[[53.66020115]] [[53.73942407]] [[0.00230947]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0353 \n",
      "Accuracy: 9886/10000 (98.86%)\n",
      "\n",
      "Round  93, Train average loss 0.043 Test accuracy 98.860\n",
      "[[53.73942407]] [[53.82149529]] [[0.00231682]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0365 \n",
      "Accuracy: 9885/10000 (98.85%)\n",
      "\n",
      "Round  94, Train average loss 0.043 Test accuracy 98.850\n",
      "[[53.82149529]] [[53.8557533]] [[0.00275121]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0364 \n",
      "Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Round  95, Train average loss 0.043 Test accuracy 98.790\n",
      "[[53.8557533]] [[53.91472353]] [[0.0023746]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0364 \n",
      "Accuracy: 9881/10000 (98.81%)\n",
      "\n",
      "Round  96, Train average loss 0.042 Test accuracy 98.810\n",
      "[[53.91472353]] [[53.97170234]] [[0.00296921]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0372 \n",
      "Accuracy: 9881/10000 (98.81%)\n",
      "\n",
      "Round  97, Train average loss 0.042 Test accuracy 98.810\n",
      "[[53.97170234]] [[54.03071897]] [[0.00283418]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0367 \n",
      "Accuracy: 9875/10000 (98.75%)\n",
      "\n",
      "Round  98, Train average loss 0.042 Test accuracy 98.750\n",
      "[[54.03071897]] [[54.07799499]] [[0.00293968]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0349 \n",
      "Accuracy: 9886/10000 (98.86%)\n",
      "\n",
      "Round  99, Train average loss 0.041 Test accuracy 98.860\n",
      "[[54.07799499]] [[54.14709715]] [[0.00266929]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0357 \n",
      "Accuracy: 9886/10000 (98.86%)\n",
      "\n",
      "Round 100, Train average loss 0.041 Test accuracy 98.860\n",
      "[[54.14709715]] [[54.20768752]] [[0.00236162]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0363 \n",
      "Accuracy: 9876/10000 (98.76%)\n",
      "\n",
      "Round 101, Train average loss 0.041 Test accuracy 98.760\n",
      "[[54.20768752]] [[54.24901651]] [[0.0022464]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0353 \n",
      "Accuracy: 9886/10000 (98.86%)\n",
      "\n",
      "Round 102, Train average loss 0.040 Test accuracy 98.860\n",
      "[[54.24901651]] [[54.3032854]] [[0.00286081]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0360 \n",
      "Accuracy: 9877/10000 (98.77%)\n",
      "\n",
      "Round 103, Train average loss 0.040 Test accuracy 98.770\n",
      "[[54.3032854]] [[54.34032111]] [[0.00267367]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0342 \n",
      "Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Round 104, Train average loss 0.040 Test accuracy 98.790\n",
      "[[54.34032111]] [[54.39171547]] [[0.00213408]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0339 \n",
      "Accuracy: 9887/10000 (98.87%)\n",
      "\n",
      "Round 105, Train average loss 0.040 Test accuracy 98.870\n",
      "[[54.39171547]] [[54.4684961]] [[0.00210133]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0349 \n",
      "Accuracy: 9888/10000 (98.88%)\n",
      "\n",
      "Round 106, Train average loss 0.039 Test accuracy 98.880\n",
      "[[54.4684961]] [[54.52597311]] [[0.00251839]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0344 \n",
      "Accuracy: 9882/10000 (98.82%)\n",
      "\n",
      "Round 107, Train average loss 0.039 Test accuracy 98.820\n",
      "[[54.52597311]] [[54.58761386]] [[0.00189638]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0353 \n",
      "Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Round 108, Train average loss 0.039 Test accuracy 98.790\n",
      "[[54.58761386]] [[54.61531777]] [[0.00310752]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0343 \n",
      "Accuracy: 9889/10000 (98.89%)\n",
      "\n",
      "Round 109, Train average loss 0.039 Test accuracy 98.890\n",
      "[[54.61531777]] [[54.6724563]] [[0.00257096]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0337 \n",
      "Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Round 110, Train average loss 0.038 Test accuracy 98.930\n",
      "[[54.6724563]] [[54.73556806]] [[0.00172075]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0333 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 111, Train average loss 0.038 Test accuracy 98.990\n",
      "[[54.73556806]] [[54.78588066]] [[0.00218045]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0332 \n",
      "Accuracy: 9885/10000 (98.85%)\n",
      "\n",
      "Round 112, Train average loss 0.038 Test accuracy 98.850\n",
      "[[54.78588066]] [[54.8288201]] [[0.00199174]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0327 \n",
      "Accuracy: 9888/10000 (98.88%)\n",
      "\n",
      "Round 113, Train average loss 0.038 Test accuracy 98.880\n",
      "[[54.8288201]] [[54.87689393]] [[0.00226579]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0329 \n",
      "Accuracy: 9896/10000 (98.96%)\n",
      "\n",
      "Round 114, Train average loss 0.038 Test accuracy 98.960\n",
      "[[54.87689393]] [[54.9288816]] [[0.00244055]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0322 \n",
      "Accuracy: 9896/10000 (98.96%)\n",
      "\n",
      "Round 115, Train average loss 0.037 Test accuracy 98.960\n",
      "[[54.9288816]] [[54.98946403]] [[0.002277]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0336 \n",
      "Accuracy: 9891/10000 (98.91%)\n",
      "\n",
      "Round 116, Train average loss 0.037 Test accuracy 98.910\n",
      "[[54.98946403]] [[55.02799319]] [[0.00269628]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0329 \n",
      "Accuracy: 9891/10000 (98.91%)\n",
      "\n",
      "Round 117, Train average loss 0.037 Test accuracy 98.910\n",
      "[[55.02799319]] [[55.08206741]] [[0.00237732]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0318 \n",
      "Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Round 118, Train average loss 0.037 Test accuracy 98.930\n",
      "[[55.08206741]] [[55.1289632]] [[0.00208574]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0337 \n",
      "Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Round 119, Train average loss 0.036 Test accuracy 98.930\n",
      "[[55.1289632]] [[55.17598129]] [[0.00340354]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0329 \n",
      "Accuracy: 9890/10000 (98.90%)\n",
      "\n",
      "Round 120, Train average loss 0.037 Test accuracy 98.900\n",
      "[[55.17598129]] [[55.22687878]] [[0.00190673]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0336 \n",
      "Accuracy: 9887/10000 (98.87%)\n",
      "\n",
      "Round 121, Train average loss 0.036 Test accuracy 98.870\n",
      "[[55.22687878]] [[55.27644447]] [[0.00278882]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0332 \n",
      "Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Round 122, Train average loss 0.036 Test accuracy 98.940\n",
      "[[55.27644447]] [[55.33036135]] [[0.00301924]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0326 \n",
      "Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Round 123, Train average loss 0.036 Test accuracy 98.930\n",
      "[[55.33036135]] [[55.38913898]] [[0.00207471]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0332 \n",
      "Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Round 124, Train average loss 0.035 Test accuracy 98.940\n",
      "[[55.38913898]] [[55.44001354]] [[0.00187236]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0333 \n",
      "Accuracy: 9892/10000 (98.92%)\n",
      "\n",
      "Round 125, Train average loss 0.035 Test accuracy 98.920\n",
      "[[55.44001354]] [[55.48176634]] [[0.00200466]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0335 \n",
      "Accuracy: 9891/10000 (98.91%)\n",
      "\n",
      "Round 126, Train average loss 0.036 Test accuracy 98.910\n",
      "[[55.48176634]] [[55.54030559]] [[0.0028632]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0319 \n",
      "Accuracy: 9900/10000 (99.00%)\n",
      "\n",
      "Round 127, Train average loss 0.036 Test accuracy 99.000\n",
      "[[55.54030559]] [[55.58119324]] [[0.00217211]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0318 \n",
      "Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Round 128, Train average loss 0.035 Test accuracy 98.980\n",
      "[[55.58119324]] [[55.61847687]] [[0.00192476]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0314 \n",
      "Accuracy: 9889/10000 (98.89%)\n",
      "\n",
      "Round 129, Train average loss 0.035 Test accuracy 98.890\n",
      "[[55.61847687]] [[55.66730445]] [[0.0024342]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0325 \n",
      "Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Round 130, Train average loss 0.035 Test accuracy 98.930\n",
      "[[55.66730445]] [[55.69842745]] [[0.00246083]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0325 \n",
      "Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Round 131, Train average loss 0.035 Test accuracy 98.950\n",
      "[[55.69842745]] [[55.74960194]] [[0.00237097]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0324 \n",
      "Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Round 132, Train average loss 0.034 Test accuracy 98.950\n",
      "[[55.74960194]] [[55.80196774]] [[0.00170806]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0318 \n",
      "Accuracy: 9892/10000 (98.92%)\n",
      "\n",
      "Round 133, Train average loss 0.034 Test accuracy 98.920\n",
      "[[55.80196774]] [[55.83963387]] [[0.00221144]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0312 \n",
      "Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Round 134, Train average loss 0.034 Test accuracy 98.930\n",
      "[[55.83963387]] [[55.88007671]] [[0.00241089]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0316 \n",
      "Accuracy: 9896/10000 (98.96%)\n",
      "\n",
      "Round 135, Train average loss 0.034 Test accuracy 98.960\n",
      "[[55.88007671]] [[55.92427498]] [[0.00206892]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0311 \n",
      "Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Round 136, Train average loss 0.034 Test accuracy 98.950\n",
      "[[55.92427498]] [[55.97214062]] [[0.00247726]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0313 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 137, Train average loss 0.033 Test accuracy 99.020\n",
      "[[55.97214062]] [[56.01773598]] [[0.00200528]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0303 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 138, Train average loss 0.033 Test accuracy 98.990\n",
      "[[56.01773598]] [[56.04949885]] [[0.00182523]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0307 \n",
      "Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Round 139, Train average loss 0.033 Test accuracy 98.980\n",
      "[[56.04949885]] [[56.10652469]] [[0.00190055]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0319 \n",
      "Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Round 140, Train average loss 0.033 Test accuracy 98.970\n",
      "[[56.10652469]] [[56.15387666]] [[0.00197211]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0318 \n",
      "Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Round 141, Train average loss 0.033 Test accuracy 98.950\n",
      "[[56.15387666]] [[56.19356298]] [[0.00208721]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0313 \n",
      "Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Round 142, Train average loss 0.033 Test accuracy 98.970\n",
      "[[56.19356298]] [[56.22940482]] [[0.00227305]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0306 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 143, Train average loss 0.032 Test accuracy 99.020\n",
      "[[56.22940482]] [[56.27510974]] [[0.00259645]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0318 \n",
      "Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Round 144, Train average loss 0.032 Test accuracy 98.940\n",
      "[[56.27510974]] [[56.31470865]] [[0.00211172]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0308 \n",
      "Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Round 145, Train average loss 0.032 Test accuracy 98.970\n",
      "[[56.31470865]] [[56.35961445]] [[0.00205422]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0299 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 146, Train average loss 0.032 Test accuracy 99.030\n",
      "[[56.35961445]] [[56.40487741]] [[0.00163267]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0301 \n",
      "Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Round 147, Train average loss 0.032 Test accuracy 98.980\n",
      "[[56.40487741]] [[56.44484951]] [[0.00221348]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0298 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 148, Train average loss 0.032 Test accuracy 98.990\n",
      "[[56.44484951]] [[56.48828381]] [[0.0018552]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0309 \n",
      "Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Round 149, Train average loss 0.031 Test accuracy 98.980\n",
      "[[56.48828381]] [[56.52893761]] [[0.00190144]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0308 \n",
      "Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Round 150, Train average loss 0.031 Test accuracy 98.970\n",
      "[[56.52893761]] [[56.56855595]] [[0.00180158]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0307 \n",
      "Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Round 151, Train average loss 0.031 Test accuracy 98.950\n",
      "[[56.56855595]] [[56.61122247]] [[0.00192448]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0310 \n",
      "Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Round 152, Train average loss 0.031 Test accuracy 98.940\n",
      "[[56.61122247]] [[56.66370376]] [[0.00199466]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0304 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 153, Train average loss 0.031 Test accuracy 98.990\n",
      "[[56.66370376]] [[56.70335313]] [[0.00196504]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0302 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 154, Train average loss 0.031 Test accuracy 99.030\n",
      "[[56.70335313]] [[56.73153737]] [[0.00189432]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0305 \n",
      "Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Round 155, Train average loss 0.031 Test accuracy 98.980\n",
      "[[56.73153737]] [[56.78181767]] [[0.00209288]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0306 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 156, Train average loss 0.031 Test accuracy 98.990\n",
      "[[56.78181767]] [[56.80741067]] [[0.00287327]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0306 \n",
      "Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Round 157, Train average loss 0.030 Test accuracy 98.950\n",
      "[[56.80741067]] [[56.85845672]] [[0.0029258]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0307 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 158, Train average loss 0.030 Test accuracy 99.030\n",
      "[[56.85845672]] [[56.89155942]] [[0.00243684]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0297 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 159, Train average loss 0.030 Test accuracy 99.030\n",
      "[[56.89155942]] [[56.93584196]] [[0.00195019]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0306 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 160, Train average loss 0.030 Test accuracy 99.020\n",
      "[[56.93584196]] [[56.95560235]] [[0.00268954]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0299 \n",
      "Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Round 161, Train average loss 0.030 Test accuracy 99.040\n",
      "[[56.95560235]] [[56.9913893]] [[0.00199399]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0306 \n",
      "Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Round 162, Train average loss 0.030 Test accuracy 98.980\n",
      "[[56.9913893]] [[57.02947807]] [[0.00195944]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0305 \n",
      "Accuracy: 9900/10000 (99.00%)\n",
      "\n",
      "Round 163, Train average loss 0.029 Test accuracy 99.000\n",
      "[[57.02947807]] [[57.06769001]] [[0.00210127]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0300 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 164, Train average loss 0.029 Test accuracy 99.020\n",
      "[[57.06769001]] [[57.10728732]] [[0.0026093]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0299 \n",
      "Accuracy: 9901/10000 (99.01%)\n",
      "\n",
      "Round 165, Train average loss 0.029 Test accuracy 99.010\n",
      "[[57.10728732]] [[57.15115532]] [[0.00208723]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0303 \n",
      "Accuracy: 9901/10000 (99.01%)\n",
      "\n",
      "Round 166, Train average loss 0.029 Test accuracy 99.010\n",
      "[[57.15115532]] [[57.18095279]] [[0.00242573]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0303 \n",
      "Accuracy: 9900/10000 (99.00%)\n",
      "\n",
      "Round 167, Train average loss 0.029 Test accuracy 99.000\n",
      "[[57.18095279]] [[57.22258624]] [[0.00208886]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0297 \n",
      "Accuracy: 9900/10000 (99.00%)\n",
      "\n",
      "Round 168, Train average loss 0.029 Test accuracy 99.000\n",
      "[[57.22258624]] [[57.26553942]] [[0.00175607]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0294 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 169, Train average loss 0.029 Test accuracy 99.020\n",
      "[[57.26553942]] [[57.30637387]] [[0.00198635]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0306 \n",
      "Accuracy: 9901/10000 (99.01%)\n",
      "\n",
      "Round 170, Train average loss 0.028 Test accuracy 99.010\n",
      "[[57.30637387]] [[57.33873643]] [[0.00211221]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0301 \n",
      "Accuracy: 9896/10000 (98.96%)\n",
      "\n",
      "Round 171, Train average loss 0.029 Test accuracy 98.960\n",
      "[[57.33873643]] [[57.36907516]] [[0.00151825]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0305 \n",
      "Accuracy: 9892/10000 (98.92%)\n",
      "\n",
      "Round 172, Train average loss 0.028 Test accuracy 98.920\n",
      "[[57.36907516]] [[57.40738936]] [[0.00205632]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0295 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 173, Train average loss 0.028 Test accuracy 98.990\n",
      "[[57.40738936]] [[57.44918303]] [[0.00221092]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0295 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 174, Train average loss 0.028 Test accuracy 99.060\n",
      "[[57.44918303]] [[57.48131229]] [[0.00197407]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0295 \n",
      "Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "Round 175, Train average loss 0.028 Test accuracy 98.990\n",
      "[[57.48131229]] [[57.52092271]] [[0.00220378]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0297 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 176, Train average loss 0.028 Test accuracy 99.020\n",
      "[[57.52092271]] [[57.55760228]] [[0.00130036]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0295 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 177, Train average loss 0.028 Test accuracy 99.020\n",
      "[[57.55760228]] [[57.59906327]] [[0.00177451]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0290 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 178, Train average loss 0.028 Test accuracy 99.030\n",
      "[[57.59906327]] [[57.62111343]] [[0.00196974]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0286 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 179, Train average loss 0.028 Test accuracy 99.030\n",
      "[[57.62111343]] [[57.65718678]] [[0.00191884]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0288 \n",
      "Accuracy: 9900/10000 (99.00%)\n",
      "\n",
      "Round 180, Train average loss 0.027 Test accuracy 99.000\n",
      "[[57.65718678]] [[57.68283739]] [[0.00204367]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0297 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 181, Train average loss 0.027 Test accuracy 99.030\n",
      "[[57.68283739]] [[57.70996839]] [[0.00220721]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0289 \n",
      "Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Round 182, Train average loss 0.027 Test accuracy 99.030\n",
      "[[57.70996839]] [[57.74074607]] [[0.00214605]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0292 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 183, Train average loss 0.027 Test accuracy 99.060\n",
      "[[57.74074607]] [[57.77928777]] [[0.00166295]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0291 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 184, Train average loss 0.027 Test accuracy 99.050\n",
      "[[57.77928777]] [[57.82171312]] [[0.00193518]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0295 \n",
      "Accuracy: 9901/10000 (99.01%)\n",
      "\n",
      "Round 185, Train average loss 0.027 Test accuracy 99.010\n",
      "[[57.82171312]] [[57.86235169]] [[0.00209351]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0301 \n",
      "Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Round 186, Train average loss 0.027 Test accuracy 98.970\n",
      "[[57.86235169]] [[57.8982825]] [[0.00193207]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0293 \n",
      "Accuracy: 9901/10000 (99.01%)\n",
      "\n",
      "Round 187, Train average loss 0.027 Test accuracy 99.010\n",
      "[[57.8982825]] [[57.9259136]] [[0.00248989]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0286 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 188, Train average loss 0.027 Test accuracy 99.090\n",
      "[[57.9259136]] [[57.96728753]] [[0.00177572]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0297 \n",
      "Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Round 189, Train average loss 0.027 Test accuracy 99.020\n",
      "[[57.96728753]] [[58.01055005]] [[0.00159908]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0291 \n",
      "Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Round 190, Train average loss 0.026 Test accuracy 99.040\n",
      "[[58.01055005]] [[58.04605874]] [[0.00159276]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0291 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 191, Train average loss 0.026 Test accuracy 99.070\n",
      "[[58.04605874]] [[58.08659231]] [[0.00188255]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0286 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 192, Train average loss 0.026 Test accuracy 99.050\n",
      "[[58.08659231]] [[58.11374963]] [[0.0017024]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0284 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 193, Train average loss 0.026 Test accuracy 99.070\n",
      "[[58.11374963]] [[58.14929494]] [[0.00144601]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 194, Train average loss 0.026 Test accuracy 99.110\n",
      "[[58.14929494]] [[58.19361004]] [[0.00176981]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0297 \n",
      "Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Round 195, Train average loss 0.026 Test accuracy 98.970\n",
      "[[58.19361004]] [[58.23453875]] [[0.00189228]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0292 \n",
      "Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Round 196, Train average loss 0.026 Test accuracy 99.040\n",
      "[[58.23453875]] [[58.27323952]] [[0.00183138]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0291 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 197, Train average loss 0.026 Test accuracy 99.050\n",
      "[[58.27323952]] [[58.31023657]] [[0.0017373]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0296 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 198, Train average loss 0.026 Test accuracy 99.070\n",
      "[[58.31023657]] [[58.33983795]] [[0.00185575]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0290 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 199, Train average loss 0.026 Test accuracy 99.080\n",
      "[[58.33983795]] [[58.34172591]] [[5.77855884e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0287 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 200, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.34172591]] [[58.34405704]] [[2.50586026e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0287 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 201, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.34405704]] [[58.34355149]] [[5.53287492e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0286 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 202, Train average loss 0.022 Test accuracy 99.050\n",
      "[[58.34355149]] [[58.34396962]] [[3.78163399e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0286 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 203, Train average loss 0.022 Test accuracy 99.070\n",
      "[[58.34396962]] [[58.34491213]] [[2.55016089e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0287 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 204, Train average loss 0.022 Test accuracy 99.050\n",
      "[[58.34491213]] [[58.34606629]] [[3.06858581e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0285 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 205, Train average loss 0.022 Test accuracy 99.050\n",
      "[[58.34606629]] [[58.34657207]] [[4.19197421e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0283 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 206, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.34657207]] [[58.34716003]] [[3.53733799e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 207, Train average loss 0.022 Test accuracy 99.050\n",
      "[[58.34716003]] [[58.34648863]] [[3.15188457e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0283 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 208, Train average loss 0.022 Test accuracy 99.050\n",
      "[[58.34648863]] [[58.34738062]] [[3.86373982e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 209, Train average loss 0.022 Test accuracy 99.050\n",
      "[[58.34738062]] [[58.34747241]] [[3.51239943e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 210, Train average loss 0.022 Test accuracy 99.080\n",
      "[[58.34747241]] [[58.34872671]] [[3.7921674e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0280 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 211, Train average loss 0.022 Test accuracy 99.090\n",
      "[[58.34872671]] [[58.34897024]] [[2.24791565e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 212, Train average loss 0.022 Test accuracy 99.070\n",
      "[[58.34897024]] [[58.35087352]] [[3.39922759e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 213, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.35087352]] [[58.35046464]] [[2.39358778e-05]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 214, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.35046464]] [[58.35027896]] [[3.1409324e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 215, Train average loss 0.022 Test accuracy 99.070\n",
      "[[58.35027896]] [[58.3510858]] [[3.2222828e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 216, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.3510858]] [[58.35129837]] [[2.58269174e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 217, Train average loss 0.022 Test accuracy 99.070\n",
      "[[58.35129837]] [[58.35224524]] [[3.44575794e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 218, Train average loss 0.022 Test accuracy 99.060\n",
      "[[58.35224524]] [[58.35241334]] [[2.7015588e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Round 219, Train average loss 0.021 Test accuracy 99.050\n",
      "[[58.35241334]] [[58.35346123]] [[3.77708664e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 220, Train average loss 0.021 Test accuracy 99.070\n",
      "[[58.35346123]] [[58.35308062]] [[2.6079277e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0283 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 221, Train average loss 0.021 Test accuracy 99.070\n",
      "[[58.35308062]] [[58.35388421]] [[2.22159092e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0284 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 222, Train average loss 0.021 Test accuracy 99.080\n",
      "[[58.35388421]] [[58.35540123]] [[2.03569362e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0283 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 223, Train average loss 0.021 Test accuracy 99.070\n",
      "[[58.35540123]] [[58.3562555]] [[3.3204664e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0286 \n",
      "Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Round 224, Train average loss 0.021 Test accuracy 99.040\n",
      "[[58.3562555]] [[58.35671961]] [[2.90773559e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0285 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 225, Train average loss 0.021 Test accuracy 99.060\n",
      "[[58.35671961]] [[58.35754882]] [[3.17293855e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0282 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 226, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.35754882]] [[58.35783796]] [[3.80654646e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 227, Train average loss 0.021 Test accuracy 99.070\n",
      "[[58.35783796]] [[58.35846865]] [[2.81488754e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0280 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 228, Train average loss 0.021 Test accuracy 99.080\n",
      "[[58.35846865]] [[58.36049627]] [[2.9555121e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Round 229, Train average loss 0.021 Test accuracy 99.060\n",
      "[[58.36049627]] [[58.36217705]] [[2.50447839e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 230, Train average loss 0.021 Test accuracy 99.070\n",
      "[[58.36217705]] [[58.36156597]] [[3.03011043e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 231, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.36156597]] [[58.36162143]] [[2.94987297e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 232, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.36162143]] [[58.36313454]] [[3.27040593e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0281 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 233, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.36313454]] [[58.36265696]] [[3.06470884e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0280 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 234, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.36265696]] [[58.36429735]] [[3.42517124e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 235, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.36429735]] [[58.36609531]] [[2.25068128e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 236, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.36609531]] [[58.36678172]] [[2.66619372e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 237, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.36678172]] [[58.36737044]] [[2.64131335e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 238, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.36737044]] [[58.36588738]] [[4.03346434e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 239, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.36588738]] [[58.36686464]] [[2.43311971e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 240, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.36686464]] [[58.3671648]] [[2.51182497e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 241, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.3671648]] [[58.36879752]] [[2.32600411e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 242, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.36879752]] [[58.36911833]] [[2.30159962e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 243, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.36911833]] [[58.37016185]] [[2.96562917e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 244, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.37016185]] [[58.37240031]] [[3.96288038e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 245, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.37240031]] [[58.37444125]] [[3.44944132e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 246, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.37444125]] [[58.37417109]] [[3.98839599e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 247, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.37417109]] [[58.37621263]] [[3.3264926e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 248, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.37621263]] [[58.37474317]] [[4.47394266e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0280 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 249, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.37474317]] [[58.37506085]] [[3.02047482e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 250, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.37506085]] [[58.37505229]] [[6.18068692e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "Round 251, Train average loss 0.021 Test accuracy 99.140\n",
      "[[58.37505229]] [[58.37733251]] [[1.96437798e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 252, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.37733251]] [[58.37870222]] [[2.59001336e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 253, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.37870222]] [[58.37929411]] [[3.2907845e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 254, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.37929411]] [[58.38080868]] [[1.81061079e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "Round 255, Train average loss 0.021 Test accuracy 99.140\n",
      "[[58.38080868]] [[58.38308153]] [[4.41882179e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 256, Train average loss 0.021 Test accuracy 99.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58.38308153]] [[58.38492997]] [[2.82603907e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 257, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.38492997]] [[58.38749153]] [[3.87675864e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 258, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.38749153]] [[58.38897841]] [[4.80548872e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 259, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.38897841]] [[58.39024448]] [[2.88624555e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 260, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.39024448]] [[58.39047226]] [[5.56998852e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "Round 261, Train average loss 0.021 Test accuracy 99.140\n",
      "[[58.39047226]] [[58.39164283]] [[2.54904815e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 262, Train average loss 0.021 Test accuracy 99.130\n",
      "[[58.39164283]] [[58.39400649]] [[3.21431345e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 263, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.39400649]] [[58.3939231]] [[3.06217141e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 264, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.3939231]] [[58.39489123]] [[3.50951503e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 265, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.39489123]] [[58.39746444]] [[2.60801877e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 266, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.39746444]] [[58.39804629]] [[2.82622161e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 267, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.39804629]] [[58.39901164]] [[4.03721259e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 268, Train average loss 0.021 Test accuracy 99.130\n",
      "[[58.39901164]] [[58.39890729]] [[2.0471957e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 269, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.39890729]] [[58.39938997]] [[2.68286246e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 270, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.39938997]] [[58.40075439]] [[2.27138896e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0280 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 271, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.40075439]] [[58.40200119]] [[3.15102208e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 272, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.40200119]] [[58.40364241]] [[2.03337824e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 273, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.40364241]] [[58.40507775]] [[2.09314306e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 274, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.40507775]] [[58.40631462]] [[2.83948664e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 275, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.40631462]] [[58.40553994]] [[3.11516459e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 276, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.40553994]] [[58.4069663]] [[2.5674451e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 277, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.4069663]] [[58.40753669]] [[2.12917749e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 278, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.40753669]] [[58.40915602]] [[2.37115363e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 279, Train average loss 0.021 Test accuracy 99.120\n",
      "[[58.40915602]] [[58.41122939]] [[2.67801637e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 280, Train average loss 0.021 Test accuracy 99.080\n",
      "[[58.41122939]] [[58.41158699]] [[3.71913781e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 281, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.41158699]] [[58.41424289]] [[2.466414e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 282, Train average loss 0.021 Test accuracy 99.090\n",
      "[[58.41424289]] [[58.41635846]] [[1.84176361e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 283, Train average loss 0.021 Test accuracy 99.130\n",
      "[[58.41635846]] [[58.4172877]] [[2.55340027e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "Round 284, Train average loss 0.021 Test accuracy 99.140\n",
      "[[58.4172877]] [[58.41839538]] [[2.73481036e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 285, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.41839538]] [[58.41925643]] [[2.90668941e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 286, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.41925643]] [[58.42098634]] [[2.98136113e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 287, Train average loss 0.021 Test accuracy 99.110\n",
      "[[58.42098634]] [[58.42260741]] [[2.84905469e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 288, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.42260741]] [[58.42383122]] [[3.53056394e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 289, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.42383122]] [[58.42612483]] [[2.34084812e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 290, Train average loss 0.021 Test accuracy 99.080\n",
      "[[58.42612483]] [[58.42684997]] [[2.58328708e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 291, Train average loss 0.021 Test accuracy 99.100\n",
      "[[58.42684997]] [[58.42852586]] [[1.98284379e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 292, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.42852586]] [[58.42996852]] [[2.60530991e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 293, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.42996852]] [[58.43174951]] [[1.99055683e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 294, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.43174951]] [[58.43154221]] [[4.00059124e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 295, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.43154221]] [[58.43347212]] [[1.82390368e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 296, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.43347212]] [[58.43611572]] [[2.95093627e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 297, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.43611572]] [[58.43581825]] [[3.2696954e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 298, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.43581825]] [[58.43689443]] [[4.36551652e-05]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 299, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.43689443]] [[58.43963805]] [[5.33391809e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 300, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.43963805]] [[58.44009529]] [[3.29595092e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 301, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.44009529]] [[58.44032859]] [[3.5355035e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 302, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.44032859]] [[58.4417986]] [[2.30422943e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 303, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.4417986]] [[58.44265496]] [[2.26021397e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 304, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.44265496]] [[58.44436224]] [[2.11066231e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 305, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.44436224]] [[58.44552815]] [[2.75196393e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 306, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.44552815]] [[58.44761027]] [[2.29115283e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 307, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.44761027]] [[58.44826718]] [[3.11797271e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 308, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.44826718]] [[58.44948817]] [[1.9902439e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 309, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.44948817]] [[58.45069902]] [[5.06226962e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 310, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.45069902]] [[58.45259249]] [[2.34253206e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 311, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.45259249]] [[58.45393098]] [[1.95065657e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 312, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.45393098]] [[58.45490059]] [[3.58351743e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 313, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.45490059]] [[58.45366996]] [[4.29037969e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 314, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.45366996]] [[58.45522667]] [[2.58540438e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 315, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.45522667]] [[58.45769243]] [[1.74625262e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 316, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.45769243]] [[58.45768497]] [[2.85445565e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 317, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.45768497]] [[58.45933369]] [[2.52971515e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 318, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.45933369]] [[58.45949548]] [[3.40039682e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 319, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.45949548]] [[58.46161024]] [[1.90647243e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 320, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.46161024]] [[58.46390147]] [[2.74977289e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 321, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.46390147]] [[58.4664259]] [[2.36653173e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "Round 322, Train average loss 0.020 Test accuracy 99.140\n",
      "[[58.4664259]] [[58.46652561]] [[7.43438601e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 323, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.46652561]] [[58.46823118]] [[4.43430733e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 324, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.46823118]] [[58.46972117]] [[3.59037017e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0277 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 325, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.46972117]] [[58.47214483]] [[4.41363398e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 326, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.47214483]] [[58.47231968]] [[2.22748278e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 327, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.47231968]] [[58.47410464]] [[2.52974199e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 328, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.47410464]] [[58.47489916]] [[3.6623977e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 329, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.47489916]] [[58.47628408]] [[3.69880211e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 330, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.47628408]] [[58.47633406]] [[2.93503356e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 331, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.47633406]] [[58.47833894]] [[1.91422467e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 332, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.47833894]] [[58.4794867]] [[1.71587166e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 333, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.4794867]] [[58.48052528]] [[2.71037701e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 334, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.48052528]] [[58.48270122]] [[4.82928107e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Round 335, Train average loss 0.020 Test accuracy 99.070\n",
      "[[58.48270122]] [[58.48379537]] [[2.87984605e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 336, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.48379537]] [[58.48453152]] [[4.78915969e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 337, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.48453152]] [[58.48533077]] [[3.92693347e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 338, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.48533077]] [[58.48719964]] [[2.34512259e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 339, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.48719964]] [[58.4884196]] [[4.93972057e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 340, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.4884196]] [[58.48972287]] [[3.65708742e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 341, Train average loss 0.020 Test accuracy 99.110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58.48972287]] [[58.49159535]] [[3.35325633e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 342, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.49159535]] [[58.49268464]] [[2.77065617e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 343, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.49268464]] [[58.49498174]] [[3.45589242e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 344, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.49498174]] [[58.49731654]] [[2.76338578e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 345, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.49731654]] [[58.49895153]] [[4.00444439e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 346, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.49895153]] [[58.49928697]] [[2.53705535e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 347, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.49928697]] [[58.50052099]] [[2.2746926e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 348, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.50052099]] [[58.50304352]] [[2.0279049e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 349, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.50304352]] [[58.50496571]] [[1.8504808e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 350, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.50496571]] [[58.50617404]] [[3.64791962e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 351, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.50617404]] [[58.50758782]] [[2.38921729e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 352, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.50758782]] [[58.50826907]] [[1.67108478e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 353, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.50826907]] [[58.50972709]] [[2.73254283e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 354, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.50972709]] [[58.50986443]] [[3.51748392e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 355, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.50986443]] [[58.51230634]] [[2.11584741e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 356, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.51230634]] [[58.51388012]] [[4.66119803e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 357, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.51388012]] [[58.51608522]] [[2.81398661e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 358, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.51608522]] [[58.51728779]] [[2.05799218e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 359, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.51728779]] [[58.5183381]] [[3.55702269e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 360, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.5183381]] [[58.52014673]] [[4.12899558e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 361, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.52014673]] [[58.52150414]] [[2.9733111e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 362, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.52150414]] [[58.52369949]] [[2.28477849e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 363, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.52369949]] [[58.52559006]] [[1.94875087e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 364, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.52559006]] [[58.52697035]] [[3.33588365e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 365, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.52697035]] [[58.52855736]] [[1.35078425e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 366, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.52855736]] [[58.52932499]] [[4.11987773e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 367, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.52932499]] [[58.52994864]] [[2.95668881e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 368, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.52994864]] [[58.53038263]] [[3.31804822e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0271 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 369, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.53038263]] [[58.53039488]] [[3.99566229e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 370, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.53039488]] [[58.53320461]] [[2.08356787e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0271 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 371, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.53320461]] [[58.53346997]] [[4.63565261e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 372, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.53346997]] [[58.53386892]] [[2.58711788e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 373, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.53386892]] [[58.53583984]] [[1.66494846e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 374, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.53583984]] [[58.53568042]] [[6.63355144e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 375, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.53568042]] [[58.53884833]] [[3.36091133e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 376, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.53884833]] [[58.5395261]] [[4.28890493e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 377, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.5395261]] [[58.53825569]] [[3.16179538e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0273 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 378, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.53825569]] [[58.53925007]] [[3.78178106e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 379, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.53925007]] [[58.53997928]] [[3.58205834e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 380, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.53997928]] [[58.54095268]] [[3.89965091e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0278 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 381, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.54095268]] [[58.54183865]] [[4.1357794e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0279 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 382, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.54183865]] [[58.54274856]] [[4.09877984e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0276 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 383, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.54274856]] [[58.5425236]] [[2.87590515e-05]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Round 384, Train average loss 0.020 Test accuracy 99.080\n",
      "[[58.5425236]] [[58.54550408]] [[2.00561622e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 385, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.54550408]] [[58.54673146]] [[2.85666431e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 386, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.54673146]] [[58.54868766]] [[2.78501244e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 387, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.54868766]] [[58.54962099]] [[2.44124916e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 388, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.54962099]] [[58.5519081]] [[2.99361735e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 389, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.5519081]] [[58.55346753]] [[3.126159e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 390, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.55346753]] [[58.55490986]] [[3.04955523e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0272 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 391, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.55490986]] [[58.55723343]] [[2.0878436e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 392, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.55723343]] [[58.55958985]] [[2.51266075e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 393, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.55958985]] [[58.56190345]] [[2.13202714e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "Round 394, Train average loss 0.020 Test accuracy 99.100\n",
      "[[58.56190345]] [[58.56318204]] [[2.66092152e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 395, Train average loss 0.020 Test accuracy 99.110\n",
      "[[58.56318204]] [[58.56382724]] [[2.52810971e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Round 396, Train average loss 0.020 Test accuracy 99.120\n",
      "[[58.56382724]] [[58.56561491]] [[1.88267772e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0275 \n",
      "Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Round 397, Train average loss 0.020 Test accuracy 99.090\n",
      "[[58.56561491]] [[58.56679746]] [[3.42375089e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "Round 398, Train average loss 0.020 Test accuracy 99.130\n",
      "[[58.56679746]] [[58.56759553]] [[3.13580186e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.0274 \n",
      "Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Round 399, Train average loss 0.020 Test accuracy 99.110\n"
     ]
    }
   ],
   "source": [
    "from models.Nets import *\n",
    "import pickle\n",
    "\n",
    "p = 0\n",
    "N = 40\n",
    "K = 8\n",
    "\n",
    "N_trials = 1\n",
    "Max_iter = 400\n",
    "\n",
    "lr_array = [0.03]\n",
    "\n",
    "acc_test_arr  = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "loss_test_arr = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "\n",
    "\n",
    "\n",
    "P_random = []\n",
    "\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "\n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print()\n",
    "        print('Learning Rate =',args.lr)\n",
    "        print()\n",
    "        net_glob = CNNMnist2(args)\n",
    "        net_glob = net_glob.cuda()\n",
    "        print(net_glob)\n",
    "\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "        \n",
    "        w_glob_array = []\n",
    "        w_locals_array = []\n",
    "        \n",
    "        for iter in range(Max_iter): #args.epochs\n",
    "            \n",
    "            if iter >= 200:\n",
    "                args.lr = lr_array[lr_idx] * 0.1\n",
    "            elif iter >= 300:\n",
    "                args.lr = lr_array[lr_idx] * 0.01\n",
    "            \n",
    "            w_locals, loss_locals = [], []\n",
    "            w_locals_all = []\n",
    "            \n",
    "#             u = np.random.binomial(1, 1-p, size=(N))\n",
    "            u = np.ones((N,))\n",
    "            for u_idx in range(N):\n",
    "                p_sel = p_per_user[u_idx]\n",
    "                u[u_idx] = np.random.binomial(1, 1-p_sel, size=1)[0]\n",
    "            \n",
    "            result = np.where(u == 1)\n",
    "\n",
    "            ###############################\n",
    "            # 1. Random Selection\n",
    "            ###############################\n",
    "            idxs_users = np.random.choice(result[0], K, replace=False)\n",
    "\n",
    "            p_tmp = np.zeros(N)\n",
    "            p_tmp[idxs_users] = 1\n",
    "\n",
    "            P_random.append(p_tmp)\n",
    "\n",
    "#             print('Learning Rate =',args.lr)\n",
    "        #     idxs_users = np.random.choice(range(N), K, replace=False)\n",
    "            for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "                \n",
    "                w_locals_all.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "                \n",
    "                if idx in idxs_users:\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            # update global weights\n",
    "            w_glob = FedAvg(w_locals)\n",
    "            \n",
    "            \n",
    "            w_locals_array.append(w_locals_all)\n",
    "            w_glob_array.append(w_glob)\n",
    "            \n",
    "            ModelDiff_tensor(net_glob.state_dict(), w_glob_array[iter])     \n",
    "            \n",
    "            # copy weight to net_glob\n",
    "            if iter < 500:\n",
    "                print('net_glob is updated !!')\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "#             else:\n",
    "#                 net_glob.load_state_dict(w_glob_prev)\n",
    "\n",
    "            # print loss\n",
    "            loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "\n",
    "    #         loss_train.append(loss_avg)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test_arr[lr_idx][trial_idx][iter]  = acc_test\n",
    "            loss_test_arr[lr_idx][trial_idx][iter] = loss_test\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Train average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "            #print(loss_train)\n",
    "            \n",
    "# filehandler = open(\"./save_models/MNIST_CNN_N40_K8_w_locals_array_Maxiter\"+str(Max_iter),\"wb\")\n",
    "# pickle.dump(w_locals_array, filehandler)\n",
    "\n",
    "# filehandler = open(\"./save_models/MNIST_CNN_N40_K8_w_glob_array_Maxiter\"+str(Max_iter),\"wb\")\n",
    "# pickle.dump(w_glob_array, filehandler)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "filehandler = open(\"./save_models/MNIST_CNN_N40_K8_w_locals_array_Maxiter\"+str(Max_iter),\"wb\")\n",
    "pickle.dump(w_locals_array, filehandler)\n",
    "\n",
    "filehandler = open(\"./save_models/MNIST_CNN_N40_K8_w_glob_array_Maxiter\"+str(Max_iter),\"wb\")\n",
    "pickle.dump(w_glob_array, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((P_random[322]))\n",
    "filehandler = open(\"./save_models/MNIST_CNN_N40_K8_P_random_Maxiter\"+str(Max_iter),\"wb\")\n",
    "pickle.dump(P_random, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "w_locals_array = pickle.load(open('./save_models/MNIST_CNN_N40_K8_w_locals_array_Maxiter400','rb'))\n",
    "w_glob_array = pickle.load(open('./save_models/MNIST_CNN_N40_K8_w_glob_array_Maxiter400','rb'))\n",
    "P_random = pickle.load(open('./save_models/MNIST_CNN_N40_K8_P_random_Maxiter400','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56.56109619]] [[56.51433861]] [[0.05067915]]\n",
      "[[58.37620237]] [[58.36980459]] [[0.00092563]]\n",
      "[[58.50454272]] [[58.49848268]] [[0.00094133]]\n",
      "[[58.37240031]] [[58.53568042]] [[0.01456869]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00024958]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle.load(\"./save_models/MNIST_CNN_N40_K8_w_glob_iter\"+str(iter))\n",
    "ModelDiff_tensor(w_locals_array[150][0], w_locals_array[150][5])\n",
    "ModelDiff_tensor(w_locals_array[250][0], w_locals_array[250][5])\n",
    "ModelDiff_tensor(w_locals_array[350][0], w_locals_array[350][5])     \n",
    "ModelDiff_tensor(w_glob_array[245], w_glob_array[375])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 400\n",
    "\n",
    "w_locals_array_np = np.zeros((L,N,d))\n",
    "w_glob_array_np = np.zeros((L,d))\n",
    "\n",
    "for iter in range(L): #args.epochs\n",
    "    \n",
    "    w_glob_tmp = copy.deepcopy(w_glob_array[iter])\n",
    "    \n",
    "    stt_pos = 0\n",
    "    for k in w_glob_tmp.keys():\n",
    "        tmp1 = w_glob_tmp[k].cpu().detach().numpy()\n",
    "        cur_shape = tmp1.shape\n",
    "        _d = np.prod(cur_shape)\n",
    "        \n",
    "        end_pos = stt_pos + _d\n",
    "        \n",
    "        w_glob_array_np[iter,stt_pos:end_pos] = np.reshape(tmp1,(_d,))        \n",
    "    \n",
    "        for idx in range(N):\n",
    "            tmp2 = w_locals_array[iter][idx][k].cpu().detach().numpy()\n",
    "            w_locals_array_np[iter,idx,stt_pos:end_pos] = np.reshape(tmp2,(_d,))\n",
    "            \n",
    "        stt_pos = end_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.8765838  37.47843597 38.66248373 39.54896333 40.24146293 40.83750199\n",
      " 41.33655191 41.76925118 42.18807902 42.54481473 42.9997951  43.68273335\n",
      " 44.13894055 44.46103311 44.78261366 45.07753541 45.35815317 45.58488351\n",
      " 45.78121686 46.03758191 46.25158061 46.47551128 46.66920614 46.83617932\n",
      " 47.04861328 47.20860954 47.40166073 47.55154515 47.71714026 47.86301587\n",
      " 48.00863094 48.16383455 48.31917437 48.47412916 48.59726655 48.71206464\n",
      " 48.81858409 48.98309915 49.08103544 49.23745251 49.36386867 49.47925616\n",
      " 49.58778393 49.68000209 49.80159044 49.90893545 50.01404213 50.12880001\n",
      " 50.22898025 50.34369762 50.44773747 50.5400683  50.65107065 50.73906565\n",
      " 50.82717315 50.933598   51.00581416 51.09103072 51.16728968 51.25442835\n",
      " 51.33691942 51.4295133  51.51808119 51.60576848 51.68676353 51.7752395\n",
      " 51.84914442 51.92333564 52.00294821 52.08720675 52.16482222 52.25905784\n",
      " 52.32642653 52.41950816 52.49013918 52.54808064 52.61112321 52.68634194\n",
      " 52.76015192 52.81975942 52.88555191 52.94899943 53.00741049 53.07977094\n",
      " 53.15293003 53.21817781 53.28876663 53.36846683 53.44562147 53.51390911\n",
      " 53.57028157 53.63102161 53.66020115 53.73942407 53.82149529 53.8557533\n",
      " 53.91472353 53.97170234 54.03071897 54.07799499 54.14709715 54.20768752\n",
      " 54.24901651 54.3032854  54.34032111 54.39171547 54.4684961  54.52597311\n",
      " 54.58761386 54.61531777 54.6724563  54.73556806 54.78588066 54.8288201\n",
      " 54.87689393 54.9288816  54.98946403 55.02799319 55.08206741 55.1289632\n",
      " 55.17598129 55.22687878 55.27644447 55.33036135 55.38913898 55.44001354\n",
      " 55.48176634 55.54030559 55.58119324 55.61847687 55.66730445 55.69842745\n",
      " 55.74960194 55.80196774 55.83963387 55.88007671 55.92427498 55.97214062\n",
      " 56.01773598 56.04949885 56.10652469 56.15387666 56.19356298 56.22940482\n",
      " 56.27510974 56.31470865 56.35961445 56.40487741 56.44484951 56.48828381\n",
      " 56.52893761 56.56855595 56.61122247 56.66370376 56.70335313 56.73153737\n",
      " 56.78181767 56.80741067 56.85845672 56.89155942 56.93584196 56.95560235\n",
      " 56.9913893  57.02947807 57.06769001 57.10728732 57.15115532 57.18095279\n",
      " 57.22258624 57.26553942 57.30637387 57.33873643 57.36907516 57.40738936\n",
      " 57.44918303 57.48131229 57.52092271 57.55760228 57.59906327 57.62111343\n",
      " 57.65718678 57.68283739 57.70996839 57.74074607 57.77928777 57.82171312\n",
      " 57.86235169 57.8982825  57.9259136  57.96728753 58.01055005 58.04605874\n",
      " 58.08659231 58.11374963 58.14929494 58.19361004 58.23453875 58.27323952\n",
      " 58.31023657 58.33983795 58.34172591 58.34405704 58.34355149 58.34396962\n",
      " 58.34491213 58.34606629 58.34657207 58.34716003 58.34648863 58.34738062\n",
      " 58.34747241 58.34872671 58.34897024 58.35087352 58.35046464 58.35027896\n",
      " 58.3510858  58.35129837 58.35224524 58.35241334 58.35346123 58.35308062\n",
      " 58.35388421 58.35540123 58.3562555  58.35671961 58.35754882 58.35783796\n",
      " 58.35846865 58.36049627 58.36217705 58.36156597 58.36162143 58.36313454\n",
      " 58.36265696 58.36429735 58.36609531 58.36678172 58.36737044 58.36588738\n",
      " 58.36686464 58.3671648  58.36879752 58.36911833 58.37016185 58.37240031\n",
      " 58.37444125 58.37417109 58.37621263 58.37474317 58.37506085 58.37505229\n",
      " 58.37733251 58.37870222 58.37929411 58.38080868 58.38308153 58.38492997\n",
      " 58.38749153 58.38897841 58.39024448 58.39047226 58.39164283 58.39400649\n",
      " 58.3939231  58.39489123 58.39746444 58.39804629 58.39901164 58.39890729\n",
      " 58.39938997 58.40075439 58.40200119 58.40364241 58.40507775 58.40631462\n",
      " 58.40553994 58.4069663  58.40753669 58.40915602 58.41122939 58.41158699\n",
      " 58.41424289 58.41635846 58.4172877  58.41839538 58.41925643 58.42098634\n",
      " 58.42260741 58.42383122 58.42612483 58.42684997 58.42852586 58.42996852\n",
      " 58.43174951 58.43154221 58.43347212 58.43611572 58.43581825 58.43689443\n",
      " 58.43963805 58.44009529 58.44032859 58.4417986  58.44265496 58.44436224\n",
      " 58.44552815 58.44761027 58.44826718 58.44948817 58.45069902 58.45259249\n",
      " 58.45393098 58.45490059 58.45366996 58.45522667 58.45769243 58.45768497\n",
      " 58.45933369 58.45949548 58.46161024 58.46390147 58.4664259  58.46652561\n",
      " 58.46823118 58.46972117 58.47214483 58.47231968 58.47410464 58.47489916\n",
      " 58.47628408 58.47633406 58.47833894 58.4794867  58.48052528 58.48270122\n",
      " 58.48379537 58.48453152 58.48533077 58.48719964 58.4884196  58.48972287\n",
      " 58.49159535 58.49268464 58.49498174 58.49731654 58.49895153 58.49928697\n",
      " 58.50052099 58.50304352 58.50496571 58.50617404 58.50758782 58.50826907\n",
      " 58.50972709 58.50986443 58.51230634 58.51388012 58.51608522 58.51728779\n",
      " 58.5183381  58.52014673 58.52150414 58.52369949 58.52559006 58.52697035\n",
      " 58.52855736 58.52932499 58.52994864 58.53038263 58.53039488 58.53320461\n",
      " 58.53346997 58.53386892 58.53583984 58.53568042 58.53884833 58.5395261\n",
      " 58.53825569 58.53925007 58.53997928 58.54095268 58.54183865 58.54274856\n",
      " 58.5425236  58.54550408 58.54673146 58.54868766 58.54962099 58.5519081\n",
      " 58.55346753 58.55490986 58.55723343 58.55958985 58.56190345 58.56318204\n",
      " 58.56382724 58.56561491 58.56679746 58.56759553]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pow_model = np.zeros((L,))\n",
    "\n",
    "for i in range(L):\n",
    "    pow_model[i] = np.sum(w_glob_array_np[i,:]**2)\n",
    "\n",
    "print(pow_model)\n",
    "plt.plot(pow_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36.37815305]] [[36.46391591]] [[2.69061187]]\n",
      "[[46.74763326]] [[48.76243624]] [[0.41891381]]\n",
      "[[46.43291559]] [[50.04760274]] [[0.90370628]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01946262]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelDiff_np(w_locals_array_np[0,0,:], w_locals_array_np[0,5,:])\n",
    "\n",
    "ModelDiff_np(w_locals_array_np[45,10,:], w_locals_array_np[59,10,:])\n",
    "\n",
    "ModelDiff_np(w_glob_array_np[45,:], w_glob_array_np[75,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 40)\n"
     ]
    }
   ],
   "source": [
    "P_random = np.array(P_random)\n",
    "\n",
    "print(np.shape(P_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40) (40, 62346)\n",
      "(1, 62346)\n",
      "following two vector should be almost the same\n",
      "[[49.90893548]] [[49.90893545]] [[1.09473454e-13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.19346401e-15]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "K=8\n",
    "target_iter = 45\n",
    "\n",
    "A_tmp = np.reshape(P_random[target_iter,:],(1,N))\n",
    "B_tmp = w_locals_array_np[target_iter,:,:]\n",
    "\n",
    "print(np.shape(A_tmp), np.shape(B_tmp))\n",
    "\n",
    "w_glob_tmp_np = (1/K) * np.matmul(A_tmp,B_tmp)\n",
    "\n",
    "print(np.shape(w_glob_tmp_np))\n",
    "\n",
    "print('following two vector should be almost the same')\n",
    "ModelDiff_np(w_glob_tmp_np, w_glob_array_np[target_iter,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_glob_const_array_np = np.zeros((50,d))\n",
    "\n",
    "for iter in range(150,200):\n",
    "    sel_tmp = P_random[iter,:]\n",
    "    sel_tmp = np.reshape(sel_tmp, (1,N))\n",
    "    \n",
    "    w_glob_const_array_np[iter-150,:] = np.matmul(sel_tmp, w_locals_array_np[150,:,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 40)\n",
      "40\n",
      "[[58.39033656]] [[58.63134631]] [[0.03932753]]\n",
      "[[58.38185212]] [[58.65902873]] [[0.04925647]]\n",
      "[[58.392595]] [[58.49128102]] [[0.01400947]]\n",
      "[[58.38710484]] [[58.30686787]] [[0.07833325]]\n",
      "[[58.39155459]] [[58.29052493]] [[0.11848116]]\n",
      "[[58.38382129]] [[58.34737319]] [[0.00989908]]\n",
      "[[58.39094244]] [[58.77321383]] [[0.13749505]]\n",
      "[[58.39323288]] [[58.77108605]] [[0.07594316]]\n",
      "[[58.38997573]] [[58.37442465]] [[0.064123]]\n",
      "[[58.38517098]] [[58.27534052]] [[0.10182096]]\n",
      "[[58.3923289]] [[58.35601891]] [[0.01166528]]\n",
      "[[58.39190088]] [[58.792869]] [[0.3205879]]\n",
      "[[58.3928183]] [[58.61128737]] [[0.14335802]]\n",
      "[[58.38822846]] [[58.44549601]] [[0.00593101]]\n",
      "[[58.39146831]] [[58.42311279]] [[0.01241602]]\n",
      "[[58.38884893]] [[58.39442967]] [[0.00855837]]\n",
      "[[58.39234214]] [[58.3654324]] [[0.00358714]]\n",
      "[[58.39141851]] [[58.30290422]] [[0.01409152]]\n",
      "[[58.39303516]] [[58.39551805]] [[0.01863328]]\n",
      "[[58.39019939]] [[58.39020028]] [[0.01906681]]\n",
      "[[58.39024512]] [[58.32666808]] [[0.02948972]]\n",
      "[[58.38771114]] [[58.50083901]] [[0.01428613]]\n",
      "[[58.3931349]] [[58.70179555]] [[0.0599584]]\n",
      "[[58.39423714]] [[58.66059196]] [[0.04922513]]\n",
      "[[58.39192751]] [[58.42540601]] [[0.00707365]]\n",
      "[[58.38850116]] [[58.37504295]] [[0.05269878]]\n",
      "[[58.39210785]] [[58.44265971]] [[0.01648819]]\n",
      "[[58.3911127]] [[58.38326607]] [[0.00459731]]\n",
      "[[58.39008461]] [[58.28883266]] [[0.04982055]]\n",
      "[[58.39273969]] [[58.29403684]] [[0.01697666]]\n",
      "[[58.39107393]] [[58.29819178]] [[0.08259687]]\n",
      "[[58.38969253]] [[58.35762906]] [[0.1451223]]\n",
      "[[58.38538319]] [[58.66517641]] [[0.05110022]]\n",
      "[[58.39157829]] [[58.61946647]] [[0.04459694]]\n",
      "[[58.38887126]] [[58.41066314]] [[0.00111395]]\n",
      "[[58.38828309]] [[58.52138437]] [[0.07123105]]\n",
      "[[58.38812277]] [[58.52121476]] [[0.12777462]]\n",
      "[[58.39138668]] [[58.40106041]] [[0.00814966]]\n",
      "[[58.39183706]] [[58.29086563]] [[0.01418701]]\n",
      "\n",
      "\n",
      "[[58.39033656]] [[58.504752]] [[0.01359243]]\n",
      "[[58.38185212]] [[58.77893066]] [[0.08695866]]\n",
      "[[58.392595]] [[58.55249053]] [[0.02254636]]\n",
      "[[58.38710484]] [[58.44674895]] [[0.02302392]]\n",
      "[[58.39155459]] [[58.25438504]] [[0.21663192]]\n",
      "[[58.38382129]] [[58.37842389]] [[0.07183495]]\n",
      "[[58.39094244]] [[58.44908794]] [[0.0804002]]\n",
      "[[58.39323288]] [[59.22078012]] [[0.315216]]\n",
      "[[58.38997573]] [[58.55782591]] [[0.0737304]]\n",
      "[[58.38517098]] [[58.24849123]] [[0.11169976]]\n",
      "[[58.3923289]] [[58.31209584]] [[0.10139108]]\n",
      "[[58.39190088]] [[58.5908189]] [[0.11146548]]\n",
      "[[58.3928183]] [[59.144953]] [[0.68102032]]\n",
      "[[58.38822846]] [[58.48838121]] [[0.0156135]]\n",
      "[[58.39146831]] [[58.40947709]] [[0.00312111]]\n",
      "[[58.38884893]] [[58.45225994]] [[0.03828482]]\n",
      "[[58.39234214]] [[58.37921673]] [[0.02109343]]\n",
      "[[58.39141851]] [[58.37752064]] [[0.01283718]]\n",
      "[[58.39303516]] [[58.24892343]] [[0.03449415]]\n",
      "[[58.39019939]] [[58.68594412]] [[0.14930393]]\n",
      "[[58.39024512]] [[58.24505028]] [[0.04112288]]\n",
      "[[58.38771114]] [[58.42788948]] [[0.03389309]]\n",
      "[[58.3931349]] [[58.62124408]] [[0.04195066]]\n",
      "[[58.39423714]] [[58.79516202]] [[0.09131604]]\n",
      "[[58.39192751]] [[58.57188737]] [[0.04984216]]\n",
      "[[58.38850116]] [[58.32292273]] [[0.00735278]]\n",
      "[[58.39210785]] [[58.54915052]] [[0.21622613]]\n",
      "[[58.3911127]] [[58.58653597]] [[0.06855541]]\n",
      "[[58.39008461]] [[58.2816093]] [[0.04292776]]\n",
      "[[58.39273969]] [[58.30969223]] [[0.06967874]]\n",
      "[[58.39107393]] [[58.33918512]] [[0.0241624]]\n",
      "[[58.38969253]] [[58.5481076]] [[0.4396235]]\n",
      "[[58.38538319]] [[58.32997079]] [[0.01131809]]\n",
      "[[58.39157829]] [[59.20346818]] [[0.29237457]]\n",
      "[[58.38887126]] [[58.26250089]] [[0.02340959]]\n",
      "[[58.38828309]] [[58.61078269]] [[0.03016446]]\n",
      "[[58.38812277]] [[58.58551538]] [[0.25957166]]\n",
      "[[58.39138668]] [[58.51136513]] [[0.05188511]]\n",
      "[[58.39183706]] [[58.33464737]] [[0.00753108]]\n",
      "[[58.38960275]] [[58.25229892]] [[0.02636499]]\n",
      "0.0008961564528154216 0.0017183910176061339\n"
     ]
    }
   ],
   "source": [
    "# Pseudo Inversion\n",
    "P_random_tmp = P_random[260:300,:]\n",
    "\n",
    "PT = P_random_tmp.transpose()\n",
    "\n",
    "print(np.shape(PT))\n",
    "\n",
    "PTP = np.matmul(P_random_tmp.transpose(), P_random_tmp)\n",
    "\n",
    "print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "# print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "Pw_glob = np.matmul(PT, w_glob_array_np[260:300,:])\n",
    "w_recon_np = K * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "# ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "l2_diff = np.zeros((N))\n",
    "for i in range(N-1):\n",
    "    l2_diff[i] = ModelDiff_np(w_locals_array_np[260,i,:], (w_recon_np[i]+w_recon_np[i+1])/2)\n",
    "print()\n",
    "print()\n",
    "l2_diff_ = np.zeros((N))\n",
    "for i in range(N):\n",
    "    l2_diff_[i] = ModelDiff_np(w_locals_array_np[260,i,:], w_recon_np[i])\n",
    "\n",
    "print(np.sum(l2_diff)/N, np.sum(l2_diff_)/N)\n",
    "# w_recon_np = K * np.matmul(pinv, w_glob_array_np[10:50,:])\n",
    "# u, s, vh = np.linalg.svd(PTP, full_matrices=True)\n",
    "\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24  8  1  3  2  0  1  0  0  1]\n",
      "[5.34515133e-05 1.21438048e-03 2.37530944e-03 3.53623840e-03\n",
      " 4.69716737e-03 5.85809633e-03 7.01902529e-03 8.17995426e-03\n",
      " 9.34088322e-03 1.05018122e-02 1.16627411e-02]\n"
     ]
    }
   ],
   "source": [
    "hist, bin_edges = np.histogram(l2_diff_)\n",
    "\n",
    "print(hist)\n",
    "print(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=l2_diff_, bins=20, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('My Very Own Histogram')\n",
    "# plt.text(23, 45, r'$mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "plt.savefig('./plots/error_histo.png',dpi=300, bbox_inches = \"tight\")\n",
    "# plt.savefig('./plots/error_histo.eps', format='eps',dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.zeros((N,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "[[-0.12592153  0.19073155 -0.33113567 ... -0.58713839 -0.23970895\n",
      "  -0.02077015]\n",
      " [ 0.02196346  0.00931958  0.08302567 ... -0.12958761  0.02791788\n",
      "  -0.21650339]\n",
      " [-0.08537564  0.23366442 -0.14108336 ... -0.34402047 -0.13685602\n",
      "  -0.15667227]\n",
      " ...\n",
      " [ 0.11000394 -0.72209218  0.40505864 ...  0.73575671  0.14326287\n",
      "   0.23235877]\n",
      " [-0.29289279  0.53368763 -0.42441197 ... -0.17325909 -0.86651045\n",
      "  -0.28190613]\n",
      " [ 0.00099611  0.0685103  -0.33417039 ... -0.74486814  0.17281757\n",
      "  -0.30962181]]\n",
      "[[ 1.00000000e+00  4.71844785e-16 -1.55431223e-15 ...  3.78169718e-16\n",
      "  -1.22124533e-15 -1.11022302e-16]\n",
      " [-5.55111512e-17  1.00000000e+00 -1.02695630e-15 ... -8.32667268e-16\n",
      "   5.27355937e-16  8.53483950e-16]\n",
      " [-1.11022302e-16 -2.49800181e-16  1.00000000e+00 ... -1.11022302e-16\n",
      "  -2.77555756e-16  1.38777878e-15]\n",
      " ...\n",
      " [-3.33066907e-16  9.99200722e-16  1.60982339e-15 ...  1.00000000e+00\n",
      "   0.00000000e+00 -2.27595720e-15]\n",
      " [ 1.11022302e-16 -1.44328993e-15 -3.05311332e-15 ... -2.72004641e-15\n",
      "   1.00000000e+00  4.77395901e-15]\n",
      " [ 1.11022302e-16  2.77555756e-17 -1.04777298e-15 ... -6.66133815e-16\n",
      "  -1.11022302e-16  1.00000000e+00]]\n",
      "(40, 62346)\n"
     ]
    }
   ],
   "source": [
    "P_random_tmp = P_random[20:60,:]\n",
    "\n",
    "\n",
    "\n",
    "print(np.linalg.matrix_rank(P_random_tmp))\n",
    "\n",
    "pinv=np.linalg.pinv(P_random_tmp)\n",
    "\n",
    "print((pinv))\n",
    "print(pinv@P_random_tmp)\n",
    "\n",
    "w_recon_np = K * np.matmul(pinv, w_glob_array_np[20:60,:])\n",
    "# w_recon_const_np = np.matmul(pinv, w_glob_const_array_np[:,:])\n",
    "\n",
    "print(np.shape(w_recon_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.18547819]] [[44.19032681]] [[0.00061653]]\n",
      "[[44.18547819]] [[44.1845069]] [[0.00023136]]\n",
      "[[44.18547819]] [[44.18503901]] [[0.0004239]]\n",
      "[[44.18547819]] [[44.19184781]] [[0.00064832]]\n",
      "[[44.18547819]] [[44.18987874]] [[0.0006569]]\n",
      "[[44.18547819]] [[44.18877092]] [[0.00153757]]\n",
      "[[44.18547819]] [[44.19251795]] [[0.0020055]]\n",
      "[[44.18547819]] [[44.18772894]] [[0.00198513]]\n",
      "[[44.18547819]] [[44.1845114]] [[0.0024794]]\n",
      "[[44.18547819]] [[44.18707573]] [[0.00209496]]\n",
      "[[44.18547819]] [[44.18226602]] [[0.00044381]]\n",
      "[[44.18547819]] [[44.19128346]] [[0.00052433]]\n",
      "[[44.18547819]] [[44.18853467]] [[0.00027977]]\n",
      "[[44.18547819]] [[44.18587752]] [[0.00041754]]\n",
      "[[44.18547819]] [[44.1905458]] [[0.00077721]]\n",
      "[[44.18547819]] [[44.18603927]] [[8.62301717e-05]]\n",
      "[[44.18547819]] [[44.18980338]] [[0.00019002]]\n",
      "[[44.18547819]] [[44.1848371]] [[0.00038909]]\n",
      "[[44.18547819]] [[44.18543366]] [[0.00155818]]\n",
      "[[44.18547819]] [[44.18546118]] [[0.00021386]]\n",
      "[[44.18547819]] [[44.18504224]] [[0.00060313]]\n",
      "[[44.18547819]] [[44.19157661]] [[0.00052304]]\n",
      "[[44.18547819]] [[44.1832109]] [[0.00088487]]\n",
      "[[44.18547819]] [[44.18842412]] [[0.00067768]]\n",
      "[[44.18547819]] [[44.18531983]] [[0.00053883]]\n",
      "[[44.18547819]] [[44.18625627]] [[0.00013143]]\n",
      "[[44.18547819]] [[44.18954621]] [[0.00026745]]\n",
      "[[44.18547819]] [[44.1889229]] [[0.00081721]]\n",
      "[[44.18547819]] [[44.18738833]] [[0.00031645]]\n",
      "[[44.18547819]] [[44.18108369]] [[0.0004951]]\n",
      "[[44.18547819]] [[44.18672151]] [[0.00040179]]\n",
      "[[44.18547819]] [[44.18890105]] [[0.00099285]]\n",
      "[[44.18547819]] [[44.1857391]] [[0.00037738]]\n",
      "[[44.18547819]] [[44.18555598]] [[9.6198767e-05]]\n",
      "[[44.18547819]] [[44.18766365]] [[0.00110139]]\n",
      "[[44.18547819]] [[44.18895997]] [[0.00138601]]\n",
      "[[44.18547819]] [[44.18706096]] [[0.00041952]]\n",
      "[[44.18547819]] [[44.18804319]] [[0.00122089]]\n",
      "[[44.18547819]] [[44.18420711]] [[0.0033392]]\n",
      "[[44.18547819]] [[44.18727285]] [[0.00049053]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    ModelDiff_np(w_locals_array_np[-1,0,:], w_recon_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43.06451082]] [[242.78176678]] [[198.89164699]]\n",
      "[[43.06451082]] [[49.00248279]] [[5.93024574]]\n",
      "[[43.06451082]] [[71.02396262]] [[28.19891518]]\n",
      "[[43.06451082]] [[57.79445856]] [[15.42293756]]\n",
      "[[43.06451082]] [[128.0681452]] [[84.59117114]]\n",
      "[[43.06451082]] [[47.74361787]] [[4.87251176]]\n",
      "[[43.06451082]] [[275.89264727]] [[231.65863392]]\n",
      "[[43.06451082]] [[108.01170228]] [[66.14991779]]\n",
      "[[43.06451082]] [[83.97530446]] [[40.6710493]]\n",
      "[[43.06451082]] [[72.63892956]] [[30.39607074]]\n",
      "[[43.06451082]] [[110.71911139]] [[68.69398992]]\n",
      "[[43.06451082]] [[75.55726117]] [[32.31464345]]\n",
      "[[43.06451082]] [[52.50503152]] [[9.26818622]]\n",
      "[[43.06451082]] [[95.71407425]] [[52.22102977]]\n",
      "[[43.06451082]] [[62.83548657]] [[19.65563078]]\n",
      "[[43.06451082]] [[376.17618668]] [[335.19284061]]\n",
      "[[43.06451082]] [[75.99301597]] [[32.4771955]]\n",
      "[[43.06451082]] [[59.85580472]] [[17.36470655]]\n",
      "[[43.06451082]] [[51.59183333]] [[8.84309868]]\n",
      "[[43.06451082]] [[53.28618831]] [[10.78558961]]\n",
      "[[43.06451082]] [[50.30140388]] [[7.72709301]]\n",
      "[[43.06451082]] [[45.86751296]] [[3.11449368]]\n",
      "[[43.06451082]] [[188.65933055]] [[147.19787549]]\n",
      "[[43.06451082]] [[166.91589762]] [[123.15023873]]\n",
      "[[43.06451082]] [[104.44622031]] [[62.35287232]]\n",
      "[[43.06451082]] [[54.78492388]] [[12.28211346]]\n",
      "[[43.06451082]] [[54.13127643]] [[11.16423894]]\n",
      "[[43.06451082]] [[94.44627584]] [[52.23246241]]\n",
      "[[43.06451082]] [[88.44273184]] [[46.17559797]]\n",
      "[[43.06451082]] [[47.73466411]] [[4.65026231]]\n",
      "[[43.06451082]] [[100.91596816]] [[57.63919418]]\n",
      "[[43.06451082]] [[117.66923804]] [[75.8880073]]\n",
      "[[43.06451082]] [[165.22483968]] [[123.63919996]]\n",
      "[[43.06451082]] [[52.49085544]] [[10.02431728]]\n",
      "[[43.06451082]] [[51.82665379]] [[8.79622059]]\n",
      "[[43.06451082]] [[114.80481258]] [[72.84862482]]\n",
      "[[43.06451082]] [[187.98150643]] [[146.06278116]]\n",
      "[[43.06451082]] [[69.94902433]] [[26.70363155]]\n",
      "[[43.06451082]] [[75.34390272]] [[31.99558083]]\n",
      "[[43.06451082]] [[56.05937744]] [[12.98812982]]\n"
     ]
    }
   ],
   "source": [
    "N_idx = 39\n",
    "\n",
    "for i in range(N):\n",
    "    ModelDiff_np(w_recon_const_np[0], w_recon_np[i])\n",
    "\n",
    "# for iter in range(60,100):\n",
    "#     print('iter=',iter)\n",
    "#     ModelDiff_np(w_recon_np[N_idx], w_locals_array_np[iter,N_idx,:])\n",
    "    \n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.46235478e+00 4.68852291e+00 4.51060146e+00 4.40417201e+00\n",
      " 4.08531140e+00 3.91543416e+00 3.82145746e+00 3.62875652e+00\n",
      " 3.55702060e+00 3.44484337e+00 3.31225541e+00 3.14160056e+00\n",
      " 2.98825380e+00 2.83726526e+00 2.76915884e+00 2.56257634e+00\n",
      " 2.49441954e+00 2.35429766e+00 2.26961498e+00 2.14687796e+00\n",
      " 2.04901805e+00 1.98527167e+00 1.83084679e+00 1.72387391e+00\n",
      " 1.67487440e+00 1.51330718e+00 1.44416214e+00 1.24052550e+00\n",
      " 1.17100295e+00 9.51581599e-01 9.04429617e-01 8.79298746e-01\n",
      " 8.24320020e-01 6.69990656e-01 5.34550525e-01 4.24449771e-01\n",
      " 2.75195221e-01 2.01619422e-01 1.02146994e-01 6.59999616e-04]\n"
     ]
    }
   ],
   "source": [
    "u, s, vh = np.linalg.svd(P_random_tmp, full_matrices=True)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_recon = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
