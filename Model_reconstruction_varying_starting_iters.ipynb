{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, cifar_noniid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# from sympy import * \n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 40  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep=1 #\"the number of local epochs: E\"\n",
    "    local_bs=100 #\"local batch size: B\"\n",
    "    bs=100 #\"test batch size\"\n",
    "    lr=0.03 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    weight_decay = 5e-4\n",
    "    opt = 'SGD' #'ADAM'\n",
    "    loss = 'Cross'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# load dataset and split users\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "\n",
    "dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "\n",
    "img_size = dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18397\n",
      "tensor(3) tensor(4) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "print(dict_users[0][0])\n",
    "\n",
    "print(dataset_train.train_labels[dict_users[0][0]],dataset_train.train_labels[dict_users[0][797]],dataset_train.train_labels[dict_users[0][1499]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62346\n"
     ]
    }
   ],
   "source": [
    "from models.Nets import *\n",
    "from utils.functions import *\n",
    "\n",
    "net_glob = CNNMnist3(args)\n",
    "net_glob = net_glob.cuda()\n",
    "\n",
    "net_glob.train()\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "d = tensor_dim(w_glob)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3243 \n",
      "Accuracy: 8773/10000 (87.73%)\n",
      "\n",
      "\n",
      "Learning Rate = 0.03\n",
      "\n",
      "[[45.09035491]] [[45.09110784]] [[7.45988775e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3231 \n",
      "Accuracy: 8778/10000 (87.78%)\n",
      "\n",
      "Round   0, Train average loss 0.334 Test accuracy 87.780\n",
      "[[45.09110784]] [[45.09169235]] [[6.09502215e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3222 \n",
      "Accuracy: 8779/10000 (87.79%)\n",
      "\n",
      "Round   1, Train average loss 0.333 Test accuracy 87.790\n",
      "[[45.09169235]] [[45.09271597]] [[6.7700812e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3210 \n",
      "Accuracy: 8780/10000 (87.80%)\n",
      "\n",
      "Round   2, Train average loss 0.332 Test accuracy 87.800\n",
      "[[45.09271597]] [[45.09363479]] [[1.13266961e-05]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3200 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round   3, Train average loss 0.331 Test accuracy 87.840\n",
      "[[45.09363479]] [[45.09427869]] [[1.81836677e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3199 \n",
      "Accuracy: 8785/10000 (87.85%)\n",
      "\n",
      "Round   4, Train average loss 0.330 Test accuracy 87.850\n",
      "[[45.09427869]] [[45.09484086]] [[5.27011147e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3194 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round   5, Train average loss 0.330 Test accuracy 87.880\n",
      "[[45.09484086]] [[45.09554307]] [[5.85972822e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3191 \n",
      "Accuracy: 8787/10000 (87.87%)\n",
      "\n",
      "Round   6, Train average loss 0.330 Test accuracy 87.870\n",
      "[[45.09554307]] [[45.09613239]] [[3.35694455e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3188 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "Round   7, Train average loss 0.329 Test accuracy 87.880\n",
      "[[45.09613239]] [[45.09677264]] [[1.45013055e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3184 \n",
      "Accuracy: 8789/10000 (87.89%)\n",
      "\n",
      "Round   8, Train average loss 0.329 Test accuracy 87.890\n",
      "[[45.09677264]] [[45.09764605]] [[3.71363509e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3180 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round   9, Train average loss 0.329 Test accuracy 87.910\n",
      "[[45.09764605]] [[45.09879341]] [[5.65195902e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3173 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round  10, Train average loss 0.328 Test accuracy 87.910\n",
      "[[45.09879341]] [[45.09951995]] [[2.62399835e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3168 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  11, Train average loss 0.328 Test accuracy 87.920\n",
      "[[45.09951995]] [[45.0999963]] [[1.05674927e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3166 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  12, Train average loss 0.327 Test accuracy 87.920\n",
      "[[45.0999963]] [[45.10004403]] [[9.38514156e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3167 \n",
      "Accuracy: 8791/10000 (87.91%)\n",
      "\n",
      "Round  13, Train average loss 0.327 Test accuracy 87.910\n",
      "[[45.10004403]] [[45.10096206]] [[1.82183579e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3165 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  14, Train average loss 0.327 Test accuracy 87.920\n",
      "[[45.10096206]] [[45.1015461]] [[4.24783116e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3160 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "Round  15, Train average loss 0.327 Test accuracy 87.940\n",
      "[[45.1015461]] [[45.10234596]] [[3.83663849e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3154 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round  16, Train average loss 0.326 Test accuracy 87.930\n",
      "[[45.10234596]] [[45.10347424]] [[6.26030216e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3147 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  17, Train average loss 0.326 Test accuracy 87.920\n",
      "[[45.10347424]] [[45.10425865]] [[1.00335266e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3147 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  18, Train average loss 0.325 Test accuracy 87.920\n",
      "[[45.10425865]] [[45.10520832]] [[9.31694034e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3146 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round  19, Train average loss 0.325 Test accuracy 87.930\n",
      "[[45.10520832]] [[45.10624351]] [[5.83105668e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3143 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round  20, Train average loss 0.325 Test accuracy 87.930\n",
      "[[45.10624351]] [[45.10686365]] [[3.11350666e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3139 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  21, Train average loss 0.325 Test accuracy 87.950\n",
      "[[45.10686365]] [[45.10750171]] [[2.52099638e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3136 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  22, Train average loss 0.325 Test accuracy 87.970\n",
      "[[45.10750171]] [[45.10842412]] [[1.22786841e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3136 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  23, Train average loss 0.324 Test accuracy 87.960\n",
      "[[45.10842412]] [[45.10924439]] [[1.8767965e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3133 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  24, Train average loss 0.324 Test accuracy 87.950\n",
      "[[45.10924439]] [[45.10993425]] [[1.06926569e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3132 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  25, Train average loss 0.324 Test accuracy 87.950\n",
      "[[45.10993425]] [[45.110485]] [[1.51258794e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3129 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  26, Train average loss 0.324 Test accuracy 87.960\n",
      "[[45.110485]] [[45.11075378]] [[1.34032414e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3127 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  27, Train average loss 0.324 Test accuracy 87.960\n",
      "[[45.11075378]] [[45.11130653]] [[1.3481948e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3128 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  28, Train average loss 0.324 Test accuracy 87.950\n",
      "[[45.11130653]] [[45.11202293]] [[1.18127607e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3127 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  29, Train average loss 0.324 Test accuracy 87.920\n",
      "[[45.11202293]] [[45.1129094]] [[3.46707959e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3124 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round  30, Train average loss 0.324 Test accuracy 87.930\n",
      "[[45.1129094]] [[45.11365925]] [[4.10674596e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3124 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "Round  31, Train average loss 0.323 Test accuracy 87.940\n",
      "[[45.11365925]] [[45.11394053]] [[2.72768665e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3122 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round  32, Train average loss 0.323 Test accuracy 87.930\n",
      "[[45.11394053]] [[45.11479854]] [[1.86146878e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3121 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "Round  33, Train average loss 0.323 Test accuracy 87.950\n",
      "[[45.11479854]] [[45.11576511]] [[3.18900697e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3120 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  34, Train average loss 0.323 Test accuracy 87.970\n",
      "[[45.11576511]] [[45.11654344]] [[1.3652134e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3119 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  35, Train average loss 0.323 Test accuracy 87.970\n",
      "[[45.11654344]] [[45.11747893]] [[1.91123045e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3118 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  36, Train average loss 0.323 Test accuracy 87.990\n",
      "[[45.11747893]] [[45.11804864]] [[2.80357892e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3116 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  37, Train average loss 0.323 Test accuracy 87.960\n",
      "[[45.11804864]] [[45.11906838]] [[2.03425381e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3113 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  38, Train average loss 0.322 Test accuracy 87.970\n",
      "[[45.11906838]] [[45.11996363]] [[3.13675102e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3114 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  39, Train average loss 0.322 Test accuracy 87.970\n",
      "[[45.11996363]] [[45.12086925]] [[3.13628178e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3112 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "Round  40, Train average loss 0.322 Test accuracy 88.010\n",
      "[[45.12086925]] [[45.12131624]] [[9.63934542e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3112 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "Round  41, Train average loss 0.322 Test accuracy 88.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.12131624]] [[45.12191299]] [[2.76885027e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3110 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "Round  42, Train average loss 0.322 Test accuracy 88.000\n",
      "[[45.12191299]] [[45.1225356]] [[1.7641923e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3109 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  43, Train average loss 0.322 Test accuracy 88.020\n",
      "[[45.1225356]] [[45.12314256]] [[1.78639095e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3109 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  44, Train average loss 0.322 Test accuracy 88.040\n",
      "[[45.12314256]] [[45.12386051]] [[6.10796902e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3109 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  45, Train average loss 0.322 Test accuracy 88.060\n",
      "[[45.12386051]] [[45.1243911]] [[7.93524311e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3109 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "Round  46, Train average loss 0.322 Test accuracy 88.080\n",
      "[[45.1243911]] [[45.1248363]] [[1.19467278e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3109 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  47, Train average loss 0.322 Test accuracy 88.040\n",
      "[[45.1248363]] [[45.12531634]] [[1.04200111e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3107 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  48, Train average loss 0.322 Test accuracy 88.040\n",
      "[[45.12531634]] [[45.1257949]] [[3.72336595e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3107 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  49, Train average loss 0.322 Test accuracy 88.030\n",
      "[[45.1257949]] [[45.12611222]] [[8.36362355e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  50, Train average loss 0.322 Test accuracy 88.040\n",
      "[[45.12611222]] [[45.12686712]] [[2.01353245e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  51, Train average loss 0.322 Test accuracy 88.030\n",
      "[[45.12686712]] [[45.12770381]] [[1.53280182e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  52, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.12770381]] [[45.12814192]] [[2.199346e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  53, Train average loss 0.322 Test accuracy 88.020\n",
      "[[45.12814192]] [[45.12892658]] [[1.17038752e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  54, Train average loss 0.322 Test accuracy 88.030\n",
      "[[45.12892658]] [[45.12967026]] [[1.7993932e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3105 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  55, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.12967026]] [[45.13024347]] [[2.18462048e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3102 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  56, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.13024347]] [[45.13089084]] [[1.33052064e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3102 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  57, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13089084]] [[45.13146491]] [[1.94769004e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3102 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  58, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.13146491]] [[45.13235118]] [[1.82872219e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3101 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  59, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.13235118]] [[45.13292827]] [[8.30160414e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3101 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  60, Train average loss 0.321 Test accuracy 88.020\n",
      "[[45.13292827]] [[45.13371067]] [[2.37543461e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3099 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  61, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.13371067]] [[45.13446364]] [[3.04411569e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3097 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  62, Train average loss 0.321 Test accuracy 88.060\n",
      "[[45.13446364]] [[45.13488169]] [[1.49978429e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3098 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  63, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13488169]] [[45.13532438]] [[2.07948894e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3098 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  64, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13532438]] [[45.13551772]] [[7.6851447e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3097 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  65, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13551772]] [[45.13604474]] [[1.11345799e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3096 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  66, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.13604474]] [[45.13658828]] [[6.37056715e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  67, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.13658828]] [[45.1373178]] [[2.30415384e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3095 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  68, Train average loss 0.321 Test accuracy 88.040\n",
      "[[45.1373178]] [[45.1379835]] [[2.09235654e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3094 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  69, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.1379835]] [[45.13884958]] [[7.36339963e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3094 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  70, Train average loss 0.321 Test accuracy 88.050\n",
      "[[45.13884958]] [[45.13967005]] [[1.53753938e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3094 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  71, Train average loss 0.321 Test accuracy 88.030\n",
      "[[45.13967005]] [[45.14052485]] [[4.0564565e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  72, Train average loss 0.321 Test accuracy 88.070\n",
      "[[45.14052485]] [[45.14074882]] [[2.12044581e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  73, Train average loss 0.320 Test accuracy 88.030\n",
      "[[45.14074882]] [[45.14124907]] [[2.10837091e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  74, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.14124907]] [[45.14194438]] [[2.3778042e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  75, Train average loss 0.320 Test accuracy 88.040\n",
      "[[45.14194438]] [[45.14198278]] [[1.08466567e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  76, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14198278]] [[45.14248239]] [[8.65299054e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3092 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  77, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14248239]] [[45.14322636]] [[1.86742436e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  78, Train average loss 0.320 Test accuracy 88.030\n",
      "[[45.14322636]] [[45.14353347]] [[1.65447085e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  79, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14353347]] [[45.1441906]] [[1.46123622e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  80, Train average loss 0.320 Test accuracy 88.020\n",
      "[[45.1441906]] [[45.14466703]] [[1.85197587e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3091 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  81, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14466703]] [[45.14544476]] [[6.83541579e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  82, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14544476]] [[45.14605728]] [[1.67245986e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  83, Train average loss 0.320 Test accuracy 88.030\n",
      "[[45.14605728]] [[45.14667216]] [[6.62917001e-06]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  84, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14667216]] [[45.14737413]] [[1.79875675e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  85, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14737413]] [[45.14803688]] [[1.7906218e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  86, Train average loss 0.320 Test accuracy 88.060\n",
      "[[45.14803688]] [[45.14860971]] [[2.7037977e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  87, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.14860971]] [[45.14921592]] [[1.26442054e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3090 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "Round  88, Train average loss 0.320 Test accuracy 88.070\n",
      "[[45.14921592]] [[45.15007039]] [[2.61342893e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  89, Train average loss 0.320 Test accuracy 88.040\n",
      "[[45.15007039]] [[45.15079049]] [[3.80771889e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  90, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.15079049]] [[45.15145119]] [[1.94334318e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  91, Train average loss 0.320 Test accuracy 88.030\n",
      "[[45.15145119]] [[45.15172493]] [[1.19695014e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  92, Train average loss 0.320 Test accuracy 88.040\n",
      "[[45.15172493]] [[45.15232314]] [[1.57860952e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "Round  93, Train average loss 0.320 Test accuracy 88.060\n",
      "[[45.15232314]] [[45.15269387]] [[2.07923677e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  94, Train average loss 0.320 Test accuracy 88.040\n",
      "[[45.15269387]] [[45.15355415]] [[1.78386399e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  95, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.15355415]] [[45.15417018]] [[1.39610638e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3088 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "Round  96, Train average loss 0.320 Test accuracy 88.040\n",
      "[[45.15417018]] [[45.15429529]] [[2.10336369e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "Round  97, Train average loss 0.320 Test accuracy 88.030\n",
      "[[45.15429529]] [[45.15467915]] [[1.2638814e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3085 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  98, Train average loss 0.320 Test accuracy 88.050\n",
      "[[45.15467915]] [[45.15564284]] [[3.12717452e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.3086 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "Round  99, Train average loss 0.320 Test accuracy 88.050\n",
      "[[13.87172472]] [[11.94624342]] [[29.8305869]]\n",
      "[[13.92615851]] [[13.26935915]] [[30.57539328]]\n",
      "[[13.97772527]] [[22.20032428]] [[38.33187894]]\n",
      "[[14.06636927]] [[3.56399887]] [[22.8436626]]\n",
      "[[14.33246937]] [[10.32941848]] [[27.06831129]]\n",
      "[[14.47430968]] [[11.48506731]] [[40.73674459]]\n",
      "[[15.18436865]] [[6.57961132]] [[30.17724696]]\n",
      "[[15.56844884]] [[2.84225588]] [[19.01908774]]\n",
      "[[15.55244357]] [[7.27872477]] [[26.1201705]]\n",
      "[[15.70677159]] [[11.07783969]] [[36.2113686]]\n",
      "[[16.073882]] [[5.14303676]] [[19.55532372]]\n",
      "[[15.93851655]] [[2.07122858]] [[18.18214149]]\n",
      "[[15.882704]] [[1.83948774]] [[11.35012335]]\n",
      "[[15.57819283]] [[3.57079815]] [[25.18327144]]\n",
      "[[15.8334602]] [[8.32574908]] [[25.84274834]]\n",
      "[[15.86692529]] [[7.51981144]] [[30.26825635]]\n",
      "[[16.10025298]] [[12.27019222]] [[22.55145467]]\n",
      "[[15.77302496]] [[1.96657122]] [[19.64807171]]\n",
      "[[15.89982338]] [[1.82612031]] [[22.42709056]]\n",
      "[[16.12163045]] [[11.42887109]] [[32.39132833]]\n",
      "[[16.33519681]] [[6.10247306]] [[18.71255153]]\n",
      "[[16.11951953]] [[4.94115291]] [[15.90430301]]\n",
      "[[15.86551414]] [[2.40662209]] [[20.25369277]]\n",
      "[[15.92622043]] [[3.67852114]] [[26.08725112]]\n",
      "[[16.16370385]] [[2.09576075]] [[17.72156818]]\n",
      "[[16.14265192]] [[2.96467235]] [[17.87752388]]\n",
      "[[16.0717574]] [[2.62703532]] [[17.96627987]]\n",
      "[[16.00874236]] [[2.64246182]] [[14.12690355]]\n",
      "[[15.82246492]] [[2.31530109]] [[19.10765519]]\n",
      "[[15.87129518]] [[6.795476]] [[22.13616012]]\n",
      "[[15.79446771]] [[8.04922208]] [[37.75577431]]\n",
      "[[16.47041785]] [[5.34626584]] [[10.66783968]]\n",
      "[[15.94667555]] [[3.64847881]] [[25.64040548]]\n",
      "[[16.1675371]] [[6.25045366]] [[29.00975078]]\n",
      "[[16.50879293]] [[2.67581827]] [[17.1657072]]\n",
      "[[16.40441741]] [[3.74601168]] [[24.16291272]]\n",
      "[[16.53522507]] [[5.49501468]] [[15.00045514]]\n",
      "[[16.18637708]] [[3.98713747]] [[22.7769282]]\n",
      "[[16.27141135]] [[6.148032]] [[34.48035291]]\n",
      "[[16.79835916]] [[6.14711229]] [[30.89838441]]\n",
      "[[17.13285291]] [[1.8893117]] [[16.60611721]]\n",
      "[[17.00804322]] [[5.42694653]] [[22.08529832]]\n",
      "[[16.94247591]] [[3.4578169]] [[13.5267011]]\n",
      "[[16.6559749]] [[3.50132627]] [[22.70713791]]\n",
      "[[16.80336727]] [[11.97161929]] [[42.48049706]]\n",
      "[[17.42889192]] [[1.55530765]] [[20.68831507]]\n",
      "[[17.47341238]] [[2.34155866]] [[14.63569077]]\n",
      "[[17.20952283]] [[2.04232217]] [[11.99071137]]\n",
      "[[16.91655708]] [[7.29779726]] [[29.72526478]]\n",
      "[[17.1894698]] [[1.63927022]] [[21.65427315]]\n",
      "[[17.28419705]] [[3.9465236]] [[20.34976393]]\n",
      "[[17.26540596]] [[3.00429157]] [[23.39033173]]\n",
      "[[17.4699189]] [[4.31071815]] [[10.76698407]]\n",
      "[[16.99138555]] [[2.29395954]] [[26.27650016]]\n",
      "[[17.30103777]] [[3.52681068]] [[27.71744657]]\n",
      "[[17.66336107]] [[4.28185614]] [[22.103652]]\n",
      "[[17.60681152]] [[2.60782045]] [[13.87101143]]\n",
      "[[17.32059604]] [[3.81747248]] [[12.91858515]]\n",
      "[[16.95957795]] [[3.58429549]] [[26.3969724]]\n",
      "[[17.22014887]] [[1.62711441]] [[18.18984088]]\n",
      "[[17.19858682]] [[4.65585183]] [[26.14003953]]\n",
      "[[17.37835405]] [[5.96646675]] [[25.00802598]]\n",
      "[[17.42653]] [[2.93957722]] [[11.76626645]]\n",
      "[[17.08001604]] [[4.07579832]] [[28.58441309]]\n",
      "[[17.42793945]] [[1.50628836]] [[13.04232346]]\n",
      "[[17.17887638]] [[2.18237766]] [[15.06993241]]\n",
      "[[16.9624408]] [[1.24863116]] [[18.21572528]]\n",
      "[[16.96501094]] [[4.51614153]] [[24.82524079]]\n",
      "[[17.11384422]] [[4.10101882]] [[22.01457811]]\n",
      "[[17.16222588]] [[1.44322633]] [[18.29486264]]\n",
      "[[17.15884771]] [[3.01357718]] [[25.42709148]]\n",
      "[[17.43636498]] [[7.95065473]] [[21.53910438]]\n",
      "[[17.25989715]] [[4.15607379]] [[10.58053295]]\n",
      "[[16.77913497]] [[4.13240699]] [[25.25525874]]\n",
      "[[16.97170987]] [[4.66049623]] [[22.1569368]]\n",
      "[[16.95200196]] [[2.12594472]] [[20.32899325]]\n",
      "[[16.99631354]] [[1.69598615]] [[13.4023419]]\n",
      "[[16.79415993]] [[3.66015175]] [[21.50651228]]\n",
      "[[16.8447331]] [[3.24276287]] [[16.51564257]]\n",
      "[[16.66896663]] [[2.86402299]] [[22.5463994]]\n",
      "[[16.82462226]] [[3.6298727]] [[18.41787663]]\n",
      "[[16.76395539]] [[1.33974149]] [[22.73222202]]\n",
      "[[16.95645361]] [[3.27802133]] [[29.03504796]]\n",
      "[[17.33804228]] [[12.99317323]] [[40.54430029]]\n",
      "[[17.78828329]] [[3.52556323]] [[28.85645783]]\n",
      "[[18.11664212]] [[3.50961873]] [[21.39834621]]\n",
      "[[18.1065033]] [[5.29944349]] [[34.8215919]]\n",
      "[[18.6193805]] [[2.47826425]] [[16.97554087]]\n",
      "[[18.39983094]] [[5.12232071]] [[28.42245788]]\n",
      "[[18.58064188]] [[7.46312903]] [[37.77861155]]\n",
      "[[19.10453732]] [[3.80895263]] [[26.22416806]]\n",
      "[[19.33142908]] [[2.34602227]] [[14.85403289]]\n",
      "[[19.01744762]] [[3.09407466]] [[21.78336016]]\n",
      "[[18.98009998]] [[4.07530408]] [[24.60051241]]\n",
      "[[19.11652281]] [[3.49637341]] [[28.72462464]]\n",
      "[[19.42575831]] [[2.7363685]] [[25.13579697]]\n",
      "[[19.53999957]] [[4.12259282]] [[11.96930972]]\n",
      "[[19.01094992]] [[2.47720755]] [[23.70734245]]\n",
      "(100, 40)\n",
      "[19. 15. 27. 17. 22. 26. 16. 19. 21. 21. 15. 23. 24. 15. 17. 23. 25. 17.\n",
      " 21. 19. 27. 17. 20. 16. 23. 16. 12. 20. 21. 17. 25. 19. 19. 27. 19. 14.\n",
      " 25. 25. 12. 24.]\n",
      "(40, 50)\n",
      "40\n",
      "[[16.27141135]] [[9.57641062]] [[13.73546208]]\n",
      "[[31.03174697]] [[22.07209182]] [[23.41818703]]\n",
      "[[61.15133208]] [[18.11293173]] [[22.82488007]]\n",
      "[[15.31488027]] [[8.43685279]] [[9.7169646]]\n",
      "[[25.98951871]] [[13.02913633]] [[10.51219879]]\n",
      "[[22.35394123]] [[12.14128935]] [[16.75851725]]\n",
      "[[36.36567784]] [[11.98103449]] [[8.48979084]]\n",
      "[[2.28572182]] [[8.74990283]] [[7.39351591]]\n",
      "[[26.95750298]] [[17.11818411]] [[13.75393225]]\n",
      "[[33.37993459]] [[15.9609338]] [[14.20730051]]\n",
      "[[24.22545026]] [[6.78597686]] [[7.23043747]]\n",
      "[[3.77917783]] [[4.3022454]] [[9.98542276]]\n",
      "[[25.19654489]] [[15.51384646]] [[19.79009433]]\n",
      "[[43.15313751]] [[21.01888415]] [[21.51240241]]\n",
      "[[41.7405996]] [[14.52905339]] [[16.52069043]]\n",
      "[[21.99263878]] [[19.83342504]] [[22.75152049]]\n",
      "[[46.43920844]] [[19.21010057]] [[14.88387447]]\n",
      "[[24.71282824]] [[5.83026727]] [[8.43831679]]\n",
      "[[2.76953601]] [[8.26462574]] [[7.32438917]]\n",
      "[[30.37822609]] [[20.11781429]] [[37.67480429]]\n",
      "[[91.45564014]] [[22.2676585]] [[44.84817509]]\n",
      "[[36.26114982]] [[20.55106246]] [[35.29269429]]\n",
      "[[80.73230885]] [[32.55494119]] [[28.99166823]]\n",
      "[[44.27162339]] [[16.66263045]] [[32.73135525]]\n",
      "[[60.94067226]] [[55.80296203]] [[0.73549232]]\n",
      "[[56.69250882]] [[14.73274209]] [[26.48428252]]\n",
      "[[22.41816491]] [[27.49732604]] [[9.82204643]]\n",
      "[[58.98239255]] [[21.64663582]] [[23.41042218]]\n",
      "[[29.00567511]] [[14.37954725]] [[12.95471603]]\n",
      "[[28.5033954]] [[18.74674133]] [[22.50189932]]\n",
      "[[58.06166]] [[15.29186602]] [[15.47466603]]\n",
      "[[3.00532832]] [[14.78372537]] [[11.63260664]]\n",
      "[[52.10832896]] [[39.16319368]] [[40.25904893]]\n",
      "[[115.71244188]] [[27.18245531]] [[41.34817408]]\n",
      "[[25.79525382]] [[14.28403113]] [[17.30843046]]\n",
      "[[45.99260237]] [[13.4664122]] [[19.32176314]]\n",
      "[[16.43974532]] [[10.22226319]] [[12.80553156]]\n",
      "[[29.17347449]] [[18.06894026]] [[17.40549608]]\n",
      "[[43.12331644]] [[41.17276807]] [[40.37195845]]\n",
      "[[102.572546]] [[41.17276807]] [[34.45161015]]\n",
      "\n",
      "\n",
      "[[16.91655708]] [[18.49513439]] [[0.27453718]]\n",
      "[[30.03319691]] [[28.73951525]] [[0.2382609]]\n",
      "[[60.49307392]] [[57.10190608]] [[0.36753546]]\n",
      "[[15.80743107]] [[18.67616556]] [[0.19990898]]\n",
      "[[26.23123417]] [[22.10416325]] [[0.29004727]]\n",
      "[[22.68671126]] [[19.8118083]] [[0.14609638]]\n",
      "[[38.3997018]] [[36.13555514]] [[0.26163076]]\n",
      "[[2.34729812]] [[5.01419344]] [[0.89234455]]\n",
      "[[26.09045092]] [[26.86438823]] [[0.08940677]]\n",
      "[[35.76901419]] [[35.29037332]] [[0.07895121]]\n",
      "[[24.3876725]] [[26.01763013]] [[0.16446297]]\n",
      "[[3.8577929]] [[4.04065004]] [[0.78491452]]\n",
      "[[25.5273799]] [[22.20756918]] [[0.17958639]]\n",
      "[[45.39531593]] [[45.08336304]] [[0.3063855]]\n",
      "[[41.45006009]] [[41.71384441]] [[0.78247416]]\n",
      "[[22.16164899]] [[19.95159146]] [[0.62180991]]\n",
      "[[49.21022811]] [[55.50817346]] [[0.40869109]]\n",
      "[[25.03126044]] [[20.83439662]] [[0.582852]]\n",
      "[[2.95748589]] [[2.56627133]] [[0.33102677]]\n",
      "[[29.81461497]] [[29.44669567]] [[0.16240782]]\n",
      "[[82.82978405]] [[83.04443122]] [[0.34061438]]\n",
      "[[39.16468428]] [[39.60866724]] [[0.16528715]]\n",
      "[[74.56398571]] [[75.2144237]] [[0.37105598]]\n",
      "[[43.62996026]] [[42.28759678]] [[0.20168687]]\n",
      "[[60.21479621]] [[55.23242656]] [[0.23250978]]\n",
      "[[55.97346865]] [[57.1745139]] [[0.41321796]]\n",
      "[[23.71966391]] [[22.30787452]] [[0.33563501]]\n",
      "[[59.54534611]] [[57.0628422]] [[0.09475505]]\n",
      "[[30.7666807]] [[30.5370002]] [[0.32201592]]\n",
      "[[28.46629101]] [[22.60455771]] [[1.66384439]]\n",
      "[[57.32270426]] [[55.09382882]] [[0.10472248]]\n",
      "[[3.11937476]] [[3.89575076]] [[0.23494779]]\n",
      "[[51.74759584]] [[50.66192383]] [[0.30792052]]\n",
      "[[105.5136467]] [[111.8779921]] [[0.19069811]]\n",
      "[[26.15290238]] [[20.24840496]] [[2.39088203]]\n",
      "[[46.31319716]] [[37.69047245]] [[0.65614886]]\n",
      "[[16.41149293]] [[20.10116172]] [[0.98110191]]\n",
      "[[28.94236507]] [[29.45297914]] [[0.48461052]]\n",
      "[[43.33869625]] [[44.89110594]] [[0.83592094]]\n",
      "[[94.08575756]] [[104.10692682]] [[0.48027158]]\n",
      "0.8121159702452111\n",
      "0.031952986720251056\n",
      "25.415964315176556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2927 \n",
      "Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "\n",
      "Learning Rate = 0.0001507537688442211\n",
      "\n",
      "[[45.48138806]] [[45.48157731]] [[5.56252472e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2925 \n",
      "Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "Round   0, Train average loss 0.302 Test accuracy 88.410\n",
      "[[45.48157731]] [[45.48184891]] [[9.80735562e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2921 \n",
      "Accuracy: 8843/10000 (88.43%)\n",
      "\n",
      "Round   1, Train average loss 0.302 Test accuracy 88.430\n",
      "[[45.48184891]] [[45.4820536]] [[6.37124916e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2918 \n",
      "Accuracy: 8844/10000 (88.44%)\n",
      "\n",
      "Round   2, Train average loss 0.302 Test accuracy 88.440\n",
      "[[45.4820536]] [[45.48224545]] [[5.78491025e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2916 \n",
      "Accuracy: 8844/10000 (88.44%)\n",
      "\n",
      "Round   3, Train average loss 0.301 Test accuracy 88.440\n",
      "[[45.48224545]] [[45.48242691]] [[5.63486257e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2913 \n",
      "Accuracy: 8846/10000 (88.46%)\n",
      "\n",
      "Round   4, Train average loss 0.301 Test accuracy 88.460\n",
      "[[45.48242691]] [[45.48263833]] [[7.5020941e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2910 \n",
      "Accuracy: 8847/10000 (88.47%)\n",
      "\n",
      "Round   5, Train average loss 0.301 Test accuracy 88.470\n",
      "[[45.48263833]] [[45.4828391]] [[1.19390156e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2905 \n",
      "Accuracy: 8847/10000 (88.47%)\n",
      "\n",
      "Round   6, Train average loss 0.301 Test accuracy 88.470\n",
      "[[45.4828391]] [[45.4831048]] [[1.0347065e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2902 \n",
      "Accuracy: 8846/10000 (88.46%)\n",
      "\n",
      "Round   7, Train average loss 0.300 Test accuracy 88.460\n",
      "[[45.4831048]] [[45.48332111]] [[6.2099375e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2899 \n",
      "Accuracy: 8848/10000 (88.48%)\n",
      "\n",
      "Round   8, Train average loss 0.300 Test accuracy 88.480\n",
      "[[45.48332111]] [[45.48355079]] [[4.00694283e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2898 \n",
      "Accuracy: 8849/10000 (88.49%)\n",
      "\n",
      "Round   9, Train average loss 0.300 Test accuracy 88.490\n",
      "[[45.48355079]] [[45.48374764]] [[2.23199033e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2897 \n",
      "Accuracy: 8850/10000 (88.50%)\n",
      "\n",
      "Round  10, Train average loss 0.299 Test accuracy 88.500\n",
      "[[45.48374764]] [[45.48393284]] [[2.56958463e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2897 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round  11, Train average loss 0.299 Test accuracy 88.510\n",
      "[[45.48393284]] [[45.48412648]] [[7.33485467e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2895 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round  12, Train average loss 0.299 Test accuracy 88.510\n",
      "[[45.48412648]] [[45.48431241]] [[3.00084944e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2894 \n",
      "Accuracy: 8850/10000 (88.50%)\n",
      "\n",
      "Round  13, Train average loss 0.299 Test accuracy 88.500\n",
      "[[45.48431241]] [[45.48447907]] [[7.40517108e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2892 \n",
      "Accuracy: 8852/10000 (88.52%)\n",
      "\n",
      "Round  14, Train average loss 0.299 Test accuracy 88.520\n",
      "[[45.48447907]] [[45.48471505]] [[3.0469403e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2891 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round  15, Train average loss 0.299 Test accuracy 88.510\n",
      "[[45.48471505]] [[45.48489621]] [[2.25621044e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2892 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round  16, Train average loss 0.299 Test accuracy 88.510\n",
      "[[45.48489621]] [[45.48510312]] [[6.40317312e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2891 \n",
      "Accuracy: 8850/10000 (88.50%)\n",
      "\n",
      "Round  17, Train average loss 0.299 Test accuracy 88.500\n",
      "[[45.48510312]] [[45.48530193]] [[3.32955793e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2890 \n",
      "Accuracy: 8851/10000 (88.51%)\n",
      "\n",
      "Round  18, Train average loss 0.299 Test accuracy 88.510\n",
      "[[45.48530193]] [[45.4855931]] [[8.11896929e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2887 \n",
      "Accuracy: 8852/10000 (88.52%)\n",
      "\n",
      "Round  19, Train average loss 0.299 Test accuracy 88.520\n",
      "[[45.4855931]] [[45.48585591]] [[5.73089921e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2885 \n",
      "Accuracy: 8855/10000 (88.55%)\n",
      "\n",
      "Round  20, Train average loss 0.299 Test accuracy 88.550\n",
      "[[45.48585591]] [[45.48610011]] [[4.87219119e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2884 \n",
      "Accuracy: 8854/10000 (88.54%)\n",
      "\n",
      "Round  21, Train average loss 0.298 Test accuracy 88.540\n",
      "[[45.48610011]] [[45.48632444]] [[3.46388738e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2882 \n",
      "Accuracy: 8853/10000 (88.53%)\n",
      "\n",
      "Round  22, Train average loss 0.298 Test accuracy 88.530\n",
      "[[45.48632444]] [[45.48656264]] [[1.54979371e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2878 \n",
      "Accuracy: 8855/10000 (88.55%)\n",
      "\n",
      "Round  23, Train average loss 0.298 Test accuracy 88.550\n",
      "[[45.48656264]] [[45.48685234]] [[2.27021937e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2877 \n",
      "Accuracy: 8855/10000 (88.55%)\n",
      "\n",
      "Round  24, Train average loss 0.298 Test accuracy 88.550\n",
      "[[45.48685234]] [[45.48705287]] [[6.03451428e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2875 \n",
      "Accuracy: 8857/10000 (88.57%)\n",
      "\n",
      "Round  25, Train average loss 0.298 Test accuracy 88.570\n",
      "[[45.48705287]] [[45.48724191]] [[1.55585072e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2875 \n",
      "Accuracy: 8858/10000 (88.58%)\n",
      "\n",
      "Round  26, Train average loss 0.297 Test accuracy 88.580\n",
      "[[45.48724191]] [[45.48754295]] [[5.01910895e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2874 \n",
      "Accuracy: 8859/10000 (88.59%)\n",
      "\n",
      "Round  27, Train average loss 0.297 Test accuracy 88.590\n",
      "[[45.48754295]] [[45.48781986]] [[8.1246875e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2871 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round  28, Train average loss 0.297 Test accuracy 88.610\n",
      "[[45.48781986]] [[45.4880552]] [[4.25121769e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2869 \n",
      "Accuracy: 8860/10000 (88.60%)\n",
      "\n",
      "Round  29, Train average loss 0.297 Test accuracy 88.600\n",
      "[[45.4880552]] [[45.48832179]] [[6.37073997e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2868 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round  30, Train average loss 0.297 Test accuracy 88.610\n",
      "[[45.48832179]] [[45.48853383]] [[8.32178582e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2866 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round  31, Train average loss 0.297 Test accuracy 88.610\n",
      "[[45.48853383]] [[45.48888679]] [[4.64595185e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2865 \n",
      "Accuracy: 8860/10000 (88.60%)\n",
      "\n",
      "Round  32, Train average loss 0.297 Test accuracy 88.600\n",
      "[[45.48888679]] [[45.48915176]] [[6.78894374e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2863 \n",
      "Accuracy: 8860/10000 (88.60%)\n",
      "\n",
      "Round  33, Train average loss 0.296 Test accuracy 88.600\n",
      "[[45.48915176]] [[45.48937693]] [[2.26180609e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2863 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round  34, Train average loss 0.296 Test accuracy 88.610\n",
      "[[45.48937693]] [[45.48964882]] [[3.97420563e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2861 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round  35, Train average loss 0.296 Test accuracy 88.610\n",
      "[[45.48964882]] [[45.48993174]] [[4.49475644e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2861 \n",
      "Accuracy: 8860/10000 (88.60%)\n",
      "\n",
      "Round  36, Train average loss 0.296 Test accuracy 88.600\n",
      "[[45.48993174]] [[45.49012412]] [[3.62415469e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2861 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "Round  37, Train average loss 0.296 Test accuracy 88.620\n",
      "[[45.49012412]] [[45.49042587]] [[3.81347114e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2860 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "Round  38, Train average loss 0.296 Test accuracy 88.620\n",
      "[[45.49042587]] [[45.49070082]] [[8.54145404e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2858 \n",
      "Accuracy: 8863/10000 (88.63%)\n",
      "\n",
      "Round  39, Train average loss 0.296 Test accuracy 88.630\n",
      "[[45.49070082]] [[45.49081844]] [[6.154137e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2857 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "Round  40, Train average loss 0.296 Test accuracy 88.620\n",
      "[[45.49081844]] [[45.49097241]] [[4.83338634e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2856 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  41, Train average loss 0.296 Test accuracy 88.640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.49097241]] [[45.49119247]] [[6.95548268e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2854 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  42, Train average loss 0.296 Test accuracy 88.660\n",
      "[[45.49119247]] [[45.49139769]] [[1.16353926e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2854 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  43, Train average loss 0.295 Test accuracy 88.660\n",
      "[[45.49139769]] [[45.49163626]] [[5.45916096e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2854 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  44, Train average loss 0.295 Test accuracy 88.660\n",
      "[[45.49163626]] [[45.49182292]] [[2.44483517e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2853 \n",
      "Accuracy: 8868/10000 (88.68%)\n",
      "\n",
      "Round  45, Train average loss 0.295 Test accuracy 88.680\n",
      "[[45.49182292]] [[45.49201079]] [[3.04954343e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2853 \n",
      "Accuracy: 8868/10000 (88.68%)\n",
      "\n",
      "Round  46, Train average loss 0.295 Test accuracy 88.680\n",
      "[[45.49201079]] [[45.49225352]] [[7.38718251e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2851 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  47, Train average loss 0.295 Test accuracy 88.660\n",
      "[[45.49225352]] [[45.49248485]] [[4.17158657e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2850 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  48, Train average loss 0.295 Test accuracy 88.670\n",
      "[[45.49248485]] [[45.49269434]] [[4.3690991e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2850 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  49, Train average loss 0.295 Test accuracy 88.660\n",
      "[[45.49269434]] [[45.49289765]] [[1.97961963e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2849 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  50, Train average loss 0.295 Test accuracy 88.670\n",
      "[[45.49289765]] [[45.49309998]] [[8.80912219e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2848 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round  51, Train average loss 0.295 Test accuracy 88.700\n",
      "[[45.49309998]] [[45.49324996]] [[8.30933221e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2846 \n",
      "Accuracy: 8869/10000 (88.69%)\n",
      "\n",
      "Round  52, Train average loss 0.295 Test accuracy 88.690\n",
      "[[45.49324996]] [[45.49350728]] [[1.02067955e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2844 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  53, Train average loss 0.295 Test accuracy 88.660\n",
      "[[45.49350728]] [[45.49367706]] [[1.68608661e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2844 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  54, Train average loss 0.295 Test accuracy 88.660\n",
      "[[45.49367706]] [[45.49383519]] [[3.2737968e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2843 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  55, Train average loss 0.295 Test accuracy 88.670\n",
      "[[45.49383519]] [[45.4940121]] [[4.74931574e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2842 \n",
      "Accuracy: 8869/10000 (88.69%)\n",
      "\n",
      "Round  56, Train average loss 0.294 Test accuracy 88.690\n",
      "[[45.4940121]] [[45.49428138]] [[9.18574105e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2842 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  57, Train average loss 0.294 Test accuracy 88.670\n",
      "[[45.49428138]] [[45.49442288]] [[7.9421129e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2841 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  58, Train average loss 0.294 Test accuracy 88.660\n",
      "[[45.49442288]] [[45.49459538]] [[2.73247213e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2841 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  59, Train average loss 0.294 Test accuracy 88.670\n",
      "[[45.49459538]] [[45.49481616]] [[1.15856553e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2839 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  60, Train average loss 0.294 Test accuracy 88.670\n",
      "[[45.49481616]] [[45.49506623]] [[6.03786997e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2838 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  61, Train average loss 0.294 Test accuracy 88.660\n",
      "[[45.49506623]] [[45.49530943]] [[2.38840818e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2838 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  62, Train average loss 0.294 Test accuracy 88.660\n",
      "[[45.49530943]] [[45.49555582]] [[4.58847658e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2837 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  63, Train average loss 0.294 Test accuracy 88.660\n",
      "[[45.49555582]] [[45.49581528]] [[5.65289016e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2837 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  64, Train average loss 0.294 Test accuracy 88.670\n",
      "[[45.49581528]] [[45.49604623]] [[1.24545159e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2837 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  65, Train average loss 0.294 Test accuracy 88.670\n",
      "[[45.49604623]] [[45.49624514]] [[1.63794132e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2837 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  66, Train average loss 0.294 Test accuracy 88.660\n",
      "[[45.49624514]] [[45.496508]] [[4.79309859e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2837 \n",
      "Accuracy: 8863/10000 (88.63%)\n",
      "\n",
      "Round  67, Train average loss 0.294 Test accuracy 88.630\n",
      "[[45.496508]] [[45.49677632]] [[3.47388876e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2836 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  68, Train average loss 0.294 Test accuracy 88.640\n",
      "[[45.49677632]] [[45.4969473]] [[2.92843509e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2836 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  69, Train average loss 0.294 Test accuracy 88.650\n",
      "[[45.4969473]] [[45.49722774]] [[3.46519987e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2835 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  70, Train average loss 0.294 Test accuracy 88.640\n",
      "[[45.49722774]] [[45.49738739]] [[5.91050615e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2835 \n",
      "Accuracy: 8863/10000 (88.63%)\n",
      "\n",
      "Round  71, Train average loss 0.294 Test accuracy 88.630\n",
      "[[45.49738739]] [[45.49762671]] [[4.31974813e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2834 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  72, Train average loss 0.294 Test accuracy 88.650\n",
      "[[45.49762671]] [[45.49783907]] [[4.25655549e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2834 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  73, Train average loss 0.294 Test accuracy 88.640\n",
      "[[45.49783907]] [[45.49798969]] [[1.98582038e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2834 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  74, Train average loss 0.294 Test accuracy 88.640\n",
      "[[45.49798969]] [[45.49821596]] [[1.84307255e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  75, Train average loss 0.294 Test accuracy 88.650\n",
      "[[45.49821596]] [[45.49843158]] [[4.54580131e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  76, Train average loss 0.294 Test accuracy 88.650\n",
      "[[45.49843158]] [[45.4986254]] [[2.20799946e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2832 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  77, Train average loss 0.293 Test accuracy 88.660\n",
      "[[45.4986254]] [[45.49884736]] [[4.42529087e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2832 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  78, Train average loss 0.293 Test accuracy 88.650\n",
      "[[45.49884736]] [[45.49901402]] [[4.5257115e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  79, Train average loss 0.293 Test accuracy 88.640\n",
      "[[45.49901402]] [[45.49927847]] [[7.77341631e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  80, Train average loss 0.293 Test accuracy 88.650\n",
      "[[45.49927847]] [[45.499523]] [[2.15497357e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  81, Train average loss 0.293 Test accuracy 88.650\n",
      "[[45.499523]] [[45.49969608]] [[2.3952556e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2832 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  82, Train average loss 0.293 Test accuracy 88.660\n",
      "[[45.49969608]] [[45.49988907]] [[4.81497881e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2831 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  83, Train average loss 0.293 Test accuracy 88.660\n",
      "[[45.49988907]] [[45.5000605]] [[2.62766146e-07]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2831 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  84, Train average loss 0.293 Test accuracy 88.660\n",
      "[[45.5000605]] [[45.50029114]] [[1.61990698e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2831 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  85, Train average loss 0.293 Test accuracy 88.670\n",
      "[[45.50029114]] [[45.50048457]] [[4.15560733e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2831 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  86, Train average loss 0.293 Test accuracy 88.670\n",
      "[[45.50048457]] [[45.50063723]] [[5.91993909e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2830 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round  87, Train average loss 0.293 Test accuracy 88.670\n",
      "[[45.50063723]] [[45.5007503]] [[2.6436785e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2831 \n",
      "Accuracy: 8868/10000 (88.68%)\n",
      "\n",
      "Round  88, Train average loss 0.293 Test accuracy 88.680\n",
      "[[45.5007503]] [[45.5011069]] [[4.90694293e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2830 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  89, Train average loss 0.293 Test accuracy 88.660\n",
      "[[45.5011069]] [[45.501285]] [[4.36719375e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2829 \n",
      "Accuracy: 8869/10000 (88.69%)\n",
      "\n",
      "Round  90, Train average loss 0.293 Test accuracy 88.690\n",
      "[[45.501285]] [[45.50147561]] [[4.41640733e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2829 \n",
      "Accuracy: 8869/10000 (88.69%)\n",
      "\n",
      "Round  91, Train average loss 0.293 Test accuracy 88.690\n",
      "[[45.50147561]] [[45.50170696]] [[4.43961218e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2828 \n",
      "Accuracy: 8869/10000 (88.69%)\n",
      "\n",
      "Round  92, Train average loss 0.293 Test accuracy 88.690\n",
      "[[45.50170696]] [[45.50192054]] [[4.6974589e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2828 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round  93, Train average loss 0.293 Test accuracy 88.700\n",
      "[[45.50192054]] [[45.50213275]] [[5.31287318e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2827 \n",
      "Accuracy: 8871/10000 (88.71%)\n",
      "\n",
      "Round  94, Train average loss 0.293 Test accuracy 88.710\n",
      "[[45.50213275]] [[45.50234714]] [[4.05393229e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2826 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round  95, Train average loss 0.293 Test accuracy 88.700\n",
      "[[45.50234714]] [[45.50258492]] [[1.40857467e-06]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2826 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round  96, Train average loss 0.293 Test accuracy 88.700\n",
      "[[45.50258492]] [[45.50282295]] [[7.33857677e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2825 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round  97, Train average loss 0.293 Test accuracy 88.700\n",
      "[[45.50282295]] [[45.50300915]] [[2.53899787e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2825 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round  98, Train average loss 0.293 Test accuracy 88.700\n",
      "[[45.50300915]] [[45.50310787]] [[4.52892163e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2824 \n",
      "Accuracy: 8872/10000 (88.72%)\n",
      "\n",
      "Round  99, Train average loss 0.293 Test accuracy 88.720\n",
      "[[1.46291267]] [[1.9222417]] [[2.53213833]]\n",
      "[[1.43830361]] [[1.24876483]] [[4.01046121]]\n",
      "[[1.45735295]] [[1.13384241]] [[3.97002019]]\n",
      "[[1.47724152]] [[1.10443306]] [[4.22722012]]\n",
      "[[1.50052477]] [[1.47041044]] [[4.15163026]]\n",
      "[[1.51548201]] [[2.34004705]] [[5.46681178]]\n",
      "[[1.5384378]] [[2.02802475]] [[4.44792565]]\n",
      "[[1.54929004]] [[1.21714775]] [[2.82166579]]\n",
      "[[1.54300032]] [[0.7853608]] [[3.51175462]]\n",
      "[[1.56156892]] [[0.4374701]] [[1.82509972]]\n",
      "[[1.55673748]] [[0.50363859]] [[2.67181778]]\n",
      "[[1.56821575]] [[1.43763152]] [[3.60899536]]\n",
      "[[1.57208559]] [[0.58816649]] [[3.03725462]]\n",
      "[[1.58771784]] [[1.45141353]] [[5.34959472]]\n",
      "[[1.63038091]] [[0.5972003]] [[1.69395332]]\n",
      "[[1.61787826]] [[0.44221725]] [[1.57916261]]\n",
      "[[1.61215415]] [[1.25502193]] [[3.56795372]]\n",
      "[[1.61879741]] [[0.65259335]] [[1.25610924]]\n",
      "[[1.59741427]] [[1.59131798]] [[3.24597198]]\n",
      "[[1.59038697]] [[1.12325625]] [[3.47393945]]\n",
      "[[1.59785238]] [[0.95494947]] [[3.82002606]]\n",
      "[[1.61885129]] [[0.67892193]] [[3.50733485]]\n",
      "[[1.63621455]] [[3.03759566]] [[5.58237288]]\n",
      "[[1.64300498]] [[0.444963]] [[3.05742074]]\n",
      "[[1.65672905]] [[1.1827648]] [[4.03984184]]\n",
      "[[1.67579456]] [[0.30494674]] [[1.59499169]]\n",
      "[[1.66912993]] [[0.98374535]] [[3.50245422]]\n",
      "[[1.68238274]] [[1.59243875]] [[4.35358635]]\n",
      "[[1.69521272]] [[0.83323867]] [[3.68367075]]\n",
      "[[1.71241866]] [[1.24866503]] [[3.90729988]]\n",
      "[[1.72651668]] [[1.63107002]] [[3.65052997]]\n",
      "[[1.72692483]] [[0.91060656]] [[4.19977486]]\n",
      "[[1.75132402]] [[1.33063297]] [[4.08018643]]\n",
      "[[1.76534025]] [[0.44331399]] [[1.63585935]]\n",
      "[[1.75322547]] [[0.7789443]] [[3.7653858]]\n",
      "[[1.77285565]] [[0.88097226]] [[2.49485152]]\n",
      "[[1.76732292]] [[0.71033432]] [[1.45996481]]\n",
      "[[1.75030812]] [[0.74744034]] [[2.60563751]]\n",
      "[[1.74776905]] [[1.67412499]] [[4.08379766]]\n",
      "[[1.75320626]] [[1.20621085]] [[0.96955523]]\n",
      "[[1.71443869]] [[0.94734372]] [[3.61868746]]\n",
      "[[1.73018379]] [[1.3632746]] [[2.66566505]]\n",
      "[[1.71833506]] [[0.2280537]] [[2.21093756]]\n",
      "[[1.72447252]] [[1.06999555]] [[3.53963208]]\n",
      "[[1.73569]] [[0.47918769]] [[2.44891727]]\n",
      "[[1.74151595]] [[0.59771051]] [[2.82811265]]\n",
      "[[1.74782532]] [[1.44788777]] [[3.28183982]]\n",
      "[[1.74515376]] [[0.81763097]] [[3.53455771]]\n",
      "[[1.75998065]] [[0.85634342]] [[3.24867455]]\n",
      "[[1.77269113]] [[0.38800545]] [[2.15532873]]\n",
      "[[1.77013939]] [[1.72658795]] [[4.49561237]]\n",
      "[[1.7857078]] [[1.62862911]] [[5.15990589]]\n",
      "[[1.8146017]] [[2.00053192]] [[4.61172321]]\n",
      "[[1.82804237]] [[0.33047298]] [[2.62586889]]\n",
      "[[1.83587056]] [[0.64166417]] [[3.24185091]]\n",
      "[[1.84704589]] [[0.93086589]] [[3.51199755]]\n",
      "[[1.85937396]] [[1.80040525]] [[2.29500034]]\n",
      "[[1.83510602]] [[1.55665413]] [[3.68377581]]\n",
      "[[1.83706654]] [[0.53556454]] [[2.33651835]]\n",
      "[[1.83697461]] [[2.27078843]] [[3.32848185]]\n",
      "[[1.81957741]] [[1.18342251]] [[4.08483835]]\n",
      "[[1.83582297]] [[0.468128]] [[3.07931157]]\n",
      "[[1.84770135]] [[0.89934141]] [[3.71612477]]\n",
      "[[1.86507003]] [[1.10796647]] [[3.69190849]]\n",
      "[[1.87932597]] [[0.24410851]] [[2.18338822]]\n",
      "[[1.88114469]] [[0.3210365]] [[2.23292241]]\n",
      "[[1.88400531]] [[0.93944732]] [[3.31172859]]\n",
      "[[1.88958264]] [[0.6808822]] [[3.91586659]]\n",
      "[[1.91029015]] [[0.57397328]] [[2.05293502]]\n",
      "[[1.89935553]] [[0.67917918]] [[3.54635654]]\n",
      "[[1.91522104]] [[1.15845921]] [[4.18455317]]\n",
      "[[1.93609198]] [[0.84667063]] [[4.29370814]]\n",
      "[[1.96111037]] [[0.83428488]] [[2.82938381]]\n",
      "[[1.95991881]] [[0.38922079]] [[3.07479861]]\n",
      "[[1.97557413]] [[0.36124222]] [[1.79651418]]\n",
      "[[1.96476534]] [[0.89097706]] [[4.2848089]]\n",
      "[[1.98986289]] [[0.43276789]] [[2.36892378]]\n",
      "[[1.98830541]] [[0.86735701]] [[3.07230394]]\n",
      "[[1.99255979]] [[0.88703945]] [[1.77460638]]\n",
      "[[1.97316353]] [[1.5235896]] [[1.59887347]]\n",
      "[[1.93688582]] [[0.42237482]] [[3.12888757]]\n",
      "[[1.95128257]] [[0.4694701]] [[1.75765742]]\n",
      "[[1.93932512]] [[0.94373585]] [[3.37074455]]\n",
      "[[1.94472917]] [[0.51502165]] [[2.45870967]]\n",
      "[[1.94273747]] [[0.31750177]] [[2.89162324]]\n",
      "[[1.95503337]] [[0.81449904]] [[3.68016154]]\n",
      "[[1.96815744]] [[1.16030806]] [[3.38556279]]\n",
      "[[1.97288777]] [[0.51816099]] [[2.03753392]]\n",
      "[[1.96559323]] [[0.96176081]] [[4.38563829]]\n",
      "[[1.98924528]] [[0.85596998]] [[3.92747922]]\n",
      "[[2.00757206]] [[0.86561584]] [[3.8307781]]\n",
      "[[2.02293771]] [[0.87016399]] [[2.72667638]]\n",
      "[[2.01673967]] [[0.92070194]] [[3.93821466]]\n",
      "[[2.03545156]] [[1.04132314]] [[3.32998771]]\n",
      "[[2.03544241]] [[0.79457073]] [[4.0934697]]\n",
      "[[2.05902966]] [[2.76080636]] [[7.02662015]]\n",
      "[[2.10093903]] [[1.43836105]] [[5.81201592]]\n",
      "[[2.14251491]] [[0.49764358]] [[2.98894277]]\n",
      "(100, 40)\n",
      "[19. 26. 18. 20. 19. 29. 19. 20. 18. 26. 24. 12. 22. 23. 14. 28. 15. 16.\n",
      " 25. 19. 22. 21. 21. 20. 24. 24. 23. 17. 19. 14. 28. 10. 19. 17. 13. 23.\n",
      " 13. 23. 20. 17.]\n",
      "(40, 50)\n",
      "40\n",
      "[[1.74776905]] [[3.17420381]] [[4.57213376]]\n",
      "[[14.33263522]] [[5.85508393]] [[7.04559564]]\n",
      "[[10.15031565]] [[3.93351515]] [[3.58479579]]\n",
      "[[4.79836552]] [[2.18475396]] [[2.3006482]]\n",
      "[[4.66847897]] [[2.53974251]] [[2.05459432]]\n",
      "[[4.59475396]] [[2.13544807]] [[3.18128358]]\n",
      "[[3.53644622]] [[1.08722406]] [[0.90151581]]\n",
      "[[0.25230807]] [[3.25916241]] [[2.96146539]]\n",
      "[[13.43318854]] [[6.10805689]] [[5.02259355]]\n",
      "[[8.96700036]] [[3.46695187]] [[3.51914754]]\n",
      "[[4.49291671]] [[0.85991602]] [[1.85522963]]\n",
      "[[0.46953683]] [[1.14212925]] [[1.66970515]]\n",
      "[[5.12807323]] [[3.35140773]] [[4.70420115]]\n",
      "[[4.98158493]] [[4.11687425]] [[5.25456014]]\n",
      "[[16.56733335]] [[6.04819052]] [[5.52337233]]\n",
      "[[3.97260872]] [[3.13385334]] [[3.51072154]]\n",
      "[[9.55701916]] [[2.86256898]] [[3.9026116]]\n",
      "[[4.73127745]] [[1.18879852]] [[1.42027711]]\n",
      "[[0.41792994]] [[3.66478757]] [[3.20887355]]\n",
      "[[14.18074119]] [[4.3289881]] [[9.28724569]]\n",
      "[[12.442545]] [[3.9535485]] [[6.07821392]]\n",
      "[[8.95184774]] [[4.31683777]] [[7.55417341]]\n",
      "[[14.12090637]] [[7.56752846]] [[6.20231076]]\n",
      "[[17.52519356]] [[4.59593591]] [[11.43655285]]\n",
      "[[10.15891566]] [[12.42213667]] [[0.50661627]]\n",
      "[[10.27018233]] [[3.24648337]] [[4.17472193]]\n",
      "[[4.25293947]] [[4.32415214]] [[2.34961554]]\n",
      "[[9.52694835]] [[3.42089678]] [[4.10247706]]\n",
      "[[2.66127085]] [[1.44860597]] [[2.27540473]]\n",
      "[[4.28779774]] [[3.27497231]] [[3.90665079]]\n",
      "[[8.20023127]] [[2.60784796]] [[2.10479516]]\n",
      "[[0.40554706]] [[5.67819545]] [[4.83197536]]\n",
      "[[20.77841956]] [[7.75309109]] [[10.19735989]]\n",
      "[[16.91601921]] [[4.65165242]] [[5.95353313]]\n",
      "[[4.40940637]] [[3.22768074]] [[2.7843862]]\n",
      "[[8.1255187]] [[3.4809759]] [[3.8320358]]\n",
      "[[6.74888038]] [[2.21799265]] [[5.64622381]]\n",
      "[[11.80047227]] [[3.16910114]] [[5.93582073]]\n",
      "[[6.68747408]] [[5.25538936]] [[5.4738226]]\n",
      "[[15.73793932]] [[5.25538936]] [[5.98334113]]\n",
      "\n",
      "\n",
      "[[1.75998065]] [[1.49574416]] [[0.48593706]]\n",
      "[[13.51188577]] [[11.8548289]] [[0.19038476]]\n",
      "[[10.87931985]] [[10.75613227]] [[0.02007158]]\n",
      "[[4.67504687]] [[5.03501065]] [[0.18178326]]\n",
      "[[4.62374352]] [[5.09045301]] [[0.21741337]]\n",
      "[[4.48362046]] [[3.94959896]] [[0.67500506]]\n",
      "[[3.81834336]] [[3.2804882]] [[0.06223479]]\n",
      "[[0.25880546]] [[0.4358394]] [[0.04920481]]\n",
      "[[12.6086211]] [[12.0222426]] [[0.04133203]]\n",
      "[[8.54668235]] [[8.86988435]] [[0.10824724]]\n",
      "[[4.45192454]] [[4.34682612]] [[0.11071804]]\n",
      "[[0.48366937]] [[1.17133178]] [[0.56681313]]\n",
      "[[5.02890413]] [[4.43383538]] [[0.45705607]]\n",
      "[[5.35301639]] [[7.45546884]] [[0.24951211]]\n",
      "[[14.38836772]] [[14.8704725]] [[0.17836984]]\n",
      "[[3.93774906]] [[5.25497401]] [[0.47646182]]\n",
      "[[9.33952496]] [[8.84777131]] [[0.18434543]]\n",
      "[[4.67771556]] [[4.14438812]] [[0.10783131]]\n",
      "[[0.42840145]] [[0.57154214]] [[0.15822782]]\n",
      "[[13.4215943]] [[12.30723018]] [[0.04562838]]\n",
      "[[12.00592836]] [[12.15234905]] [[0.01553914]]\n",
      "[[8.63806789]] [[7.56043675]] [[0.25011346]]\n",
      "[[13.6799914]] [[14.01315195]] [[0.05225007]]\n",
      "[[15.28690843]] [[13.43642174]] [[0.08889784]]\n",
      "[[11.13104636]] [[12.79823158]] [[0.34197438]]\n",
      "[[11.10801077]] [[12.27349688]] [[0.42603655]]\n",
      "[[4.06145163]] [[4.00597553]] [[0.54169522]]\n",
      "[[8.90687808]] [[8.97909678]] [[0.25002337]]\n",
      "[[2.89136376]] [[3.62026935]] [[0.06678939]]\n",
      "[[4.17333678]] [[5.27658964]] [[0.82716263]]\n",
      "[[8.81911064]] [[8.99691523]] [[0.13552874]]\n",
      "[[0.41575015]] [[0.69774945]] [[0.30204074]]\n",
      "[[18.30792326]] [[18.89400543]] [[0.0483645]]\n",
      "[[16.21491042]] [[16.07600526]] [[0.12182251]]\n",
      "[[4.30895309]] [[3.95284639]] [[0.77913411]]\n",
      "[[7.59758478]] [[9.73987639]] [[1.10750089]]\n",
      "[[6.45986058]] [[4.0552893]] [[0.82420786]]\n",
      "[[10.07081785]] [[7.59946313]] [[0.75157856]]\n",
      "[[6.26121935]] [[5.92470301]] [[0.03247947]]\n",
      "[[15.17941156]] [[15.58179869]] [[0.29968515]]\n",
      "1.4079102206081178\n",
      "0.11078796871905436\n",
      "12.708150865897878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2744 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "\n",
      "Learning Rate = 7.518796992481203e-05\n",
      "\n",
      "[[44.6769072]] [[44.67697507]] [[9.96456891e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2744 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   0, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67697507]] [[44.67711372]] [[1.56162072e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2744 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round   1, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67711372]] [[44.67723666]] [[2.5177517e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2744 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   2, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67723666]] [[44.67725428]] [[2.87945734e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2743 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round   3, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67725428]] [[44.67736526]] [[1.85719645e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2743 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   4, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67736526]] [[44.67747926]] [[1.25213291e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2743 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   5, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67747926]] [[44.67760345]] [[1.15714169e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2743 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   6, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67760345]] [[44.67768095]] [[2.2249644e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2743 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   7, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67768095]] [[44.67780749]] [[1.87297929e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2742 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   8, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67780749]] [[44.67796394]] [[7.2251927e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2742 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round   9, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67796394]] [[44.67809503]] [[1.28078415e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2742 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  10, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67809503]] [[44.67820906]] [[8.49244237e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2742 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  11, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67820906]] [[44.67830984]] [[9.34761312e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  12, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67830984]] [[44.67846274]] [[1.08141878e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  13, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67846274]] [[44.67858936]] [[2.84060358e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  14, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67858936]] [[44.67872853]] [[4.89537029e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2742 \n",
      "Accuracy: 8895/10000 (88.95%)\n",
      "\n",
      "Round  15, Train average loss 0.286 Test accuracy 88.950\n",
      "[[44.67872853]] [[44.67883792]] [[1.40505757e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2742 \n",
      "Accuracy: 8895/10000 (88.95%)\n",
      "\n",
      "Round  16, Train average loss 0.286 Test accuracy 88.950\n",
      "[[44.67883792]] [[44.67889733]] [[1.90244723e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  17, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67889733]] [[44.67895782]] [[8.2823251e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  18, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67895782]] [[44.67900121]] [[1.83815378e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  19, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67900121]] [[44.67911386]] [[6.52111376e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  20, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67911386]] [[44.67919239]] [[1.10208079e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8895/10000 (88.95%)\n",
      "\n",
      "Round  21, Train average loss 0.286 Test accuracy 88.950\n",
      "[[44.67919239]] [[44.67930877]] [[1.90300696e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8895/10000 (88.95%)\n",
      "\n",
      "Round  22, Train average loss 0.286 Test accuracy 88.950\n",
      "[[44.67930877]] [[44.67944849]] [[1.41760161e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8895/10000 (88.95%)\n",
      "\n",
      "Round  23, Train average loss 0.286 Test accuracy 88.950\n",
      "[[44.67944849]] [[44.67962498]] [[1.09128102e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  24, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67962498]] [[44.67976986]] [[1.39816315e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2741 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  25, Train average loss 0.286 Test accuracy 88.960\n",
      "[[44.67976986]] [[44.67981816]] [[1.8750756e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  26, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67981816]] [[44.67984149]] [[1.03972439e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  27, Train average loss 0.286 Test accuracy 88.970\n",
      "[[44.67984149]] [[44.67996844]] [[1.3754192e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  28, Train average loss 0.285 Test accuracy 88.970\n",
      "[[44.67996844]] [[44.68015045]] [[2.57367879e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  29, Train average loss 0.285 Test accuracy 88.970\n",
      "[[44.68015045]] [[44.68027413]] [[7.58057652e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  30, Train average loss 0.285 Test accuracy 88.970\n",
      "[[44.68027413]] [[44.68044826]] [[2.62722143e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  31, Train average loss 0.285 Test accuracy 88.970\n",
      "[[44.68044826]] [[44.68056208]] [[1.70096156e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8897/10000 (88.97%)\n",
      "\n",
      "Round  32, Train average loss 0.285 Test accuracy 88.970\n",
      "[[44.68056208]] [[44.68064728]] [[1.99117155e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8898/10000 (88.98%)\n",
      "\n",
      "Round  33, Train average loss 0.285 Test accuracy 88.980\n",
      "[[44.68064728]] [[44.68083088]] [[1.94595783e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  34, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68083088]] [[44.68089051]] [[1.48437065e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  35, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68089051]] [[44.68097868]] [[3.25267831e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8898/10000 (88.98%)\n",
      "\n",
      "Round  36, Train average loss 0.285 Test accuracy 88.980\n",
      "[[44.68097868]] [[44.68106529]] [[1.08973532e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2740 \n",
      "Accuracy: 8898/10000 (88.98%)\n",
      "\n",
      "Round  37, Train average loss 0.285 Test accuracy 88.980\n",
      "[[44.68106529]] [[44.68109233]] [[2.1027946e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  38, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68109233]] [[44.68124081]] [[8.64767665e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  39, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68124081]] [[44.6813506]] [[1.66239183e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  40, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.6813506]] [[44.68148215]] [[2.02310294e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  41, Train average loss 0.285 Test accuracy 88.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.68148215]] [[44.68159025]] [[2.0156775e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  42, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68159025]] [[44.68170866]] [[2.21063397e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  43, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68170866]] [[44.68178475]] [[1.05683804e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  44, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68178475]] [[44.68190554]] [[8.63106107e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  45, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68190554]] [[44.6820379]] [[8.23555846e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  46, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.6820379]] [[44.68213022]] [[2.2118483e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  47, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68213022]] [[44.68222912]] [[1.30667232e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  48, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68222912]] [[44.68235061]] [[1.68114876e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  49, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68235061]] [[44.68250281]] [[1.89910229e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  50, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68250281]] [[44.68260042]] [[1.6719266e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  51, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68260042]] [[44.68275176]] [[2.86562045e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  52, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68275176]] [[44.68289496]] [[5.925302e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  53, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68289496]] [[44.68298596]] [[6.80385841e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  54, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68298596]] [[44.68310882]] [[1.39018795e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  55, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68310882]] [[44.68325645]] [[2.63044205e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  56, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68325645]] [[44.68327133]] [[7.60430445e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  57, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68327133]] [[44.68333758]] [[7.72793152e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8899/10000 (88.99%)\n",
      "\n",
      "Round  58, Train average loss 0.285 Test accuracy 88.990\n",
      "[[44.68333758]] [[44.68344374]] [[8.23639499e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  59, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68344374]] [[44.68346598]] [[6.6410407e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  60, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68346598]] [[44.68354918]] [[2.01404025e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  61, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68354918]] [[44.68363824]] [[2.37756332e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  62, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68363824]] [[44.68372465]] [[1.35047939e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  63, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68372465]] [[44.68383866]] [[8.0606326e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  64, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68383866]] [[44.68395734]] [[5.02929197e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  65, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68395734]] [[44.6840035]] [[1.15698189e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  66, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.6840035]] [[44.6840774]] [[1.05449157e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  67, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.6840774]] [[44.68419132]] [[7.90735826e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  68, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68419132]] [[44.68430684]] [[1.28112058e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  69, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68430684]] [[44.68441008]] [[2.08473206e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  70, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68441008]] [[44.68449479]] [[1.67491567e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2738 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  71, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68449479]] [[44.6845665]] [[1.27437867e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  72, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.6845665]] [[44.68470012]] [[1.39665204e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  73, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68470012]] [[44.68477945]] [[2.56952433e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  74, Train average loss 0.285 Test accuracy 89.030\n",
      "[[44.68477945]] [[44.68490102]] [[1.24006318e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  75, Train average loss 0.285 Test accuracy 89.030\n",
      "[[44.68490102]] [[44.68494332]] [[9.83092318e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  76, Train average loss 0.285 Test accuracy 89.040\n",
      "[[44.68494332]] [[44.68500861]] [[1.49884537e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8906/10000 (89.06%)\n",
      "\n",
      "Round  77, Train average loss 0.285 Test accuracy 89.060\n",
      "[[44.68500861]] [[44.68510578]] [[1.04110638e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  78, Train average loss 0.285 Test accuracy 89.040\n",
      "[[44.68510578]] [[44.68518627]] [[9.77842162e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  79, Train average loss 0.285 Test accuracy 89.040\n",
      "[[44.68518627]] [[44.68527856]] [[1.23976847e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  80, Train average loss 0.285 Test accuracy 89.030\n",
      "[[44.68527856]] [[44.68536409]] [[1.12767075e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  81, Train average loss 0.285 Test accuracy 89.040\n",
      "[[44.68536409]] [[44.68551837]] [[1.6008184e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  82, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.68551837]] [[44.68556763]] [[8.56458526e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  83, Train average loss 0.285 Test accuracy 89.040\n",
      "[[44.68556763]] [[44.68570646]] [[1.32191062e-07]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  84, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.68570646]] [[44.68581456]] [[6.33825274e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  85, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.68581456]] [[44.68591723]] [[7.38411844e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  86, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68591723]] [[44.68603206]] [[1.42853803e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  87, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68603206]] [[44.68613079]] [[2.1250738e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  88, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.68613079]] [[44.68619298]] [[9.41366869e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  89, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68619298]] [[44.68627311]] [[5.907431e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  90, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68627311]] [[44.68640041]] [[1.46086829e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  91, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68640041]] [[44.68649594]] [[1.22422518e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2736 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  92, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.68649594]] [[44.68662869]] [[1.34064112e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  93, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.68662869]] [[44.6867844]] [[1.4469728e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  94, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.6867844]] [[44.6868736]] [[9.9789275e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  95, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.6868736]] [[44.68692402]] [[2.02339998e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  96, Train average loss 0.285 Test accuracy 89.020\n",
      "[[44.68692402]] [[44.6870465]] [[1.68654764e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  97, Train average loss 0.285 Test accuracy 89.000\n",
      "[[44.6870465]] [[44.68704068]] [[8.65525513e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  98, Train average loss 0.285 Test accuracy 89.010\n",
      "[[44.68704068]] [[44.68715036]] [[1.29638468e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2735 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  99, Train average loss 0.285 Test accuracy 89.000\n",
      "[[0.94785417]] [[0.30607766]] [[1.65050137]]\n",
      "[[0.95288653]] [[0.49347933]] [[1.60650755]]\n",
      "[[0.95515023]] [[0.56437364]] [[1.27535163]]\n",
      "[[0.95079483]] [[0.3640105]] [[1.28177742]]\n",
      "[[0.94906799]] [[0.24541805]] [[1.37153672]]\n",
      "[[0.95196941]] [[0.22679977]] [[1.13165299]]\n",
      "[[0.95088737]] [[0.43609302]] [[1.1580879]]\n",
      "[[0.94793105]] [[0.36710394]] [[1.73799293]]\n",
      "[[0.9526022]] [[0.14161378]] [[0.8686229]]\n",
      "[[0.94927867]] [[0.25103369]] [[0.89392857]]\n",
      "[[0.94657974]] [[0.16645187]] [[1.06720097]]\n",
      "[[0.94605951]] [[1.83213217]] [[3.8775132]]\n",
      "[[0.9597812]] [[0.21195808]] [[1.01513457]]\n",
      "[[0.95837897]] [[0.5567583]] [[1.08112217]]\n",
      "[[0.95241573]] [[0.95949258]] [[2.28593454]]\n",
      "[[0.95744593]] [[0.27539128]] [[0.838952]]\n",
      "[[0.95172108]] [[0.37287966]] [[1.42857125]]\n",
      "[[0.95286088]] [[0.16233357]] [[1.14705598]]\n",
      "[[0.95243217]] [[0.36027814]] [[1.79156521]]\n",
      "[[0.95840964]] [[0.12781383]] [[0.77948295]]\n",
      "[[0.95461132]] [[0.21600783]] [[1.45309051]]\n",
      "[[0.95844102]] [[0.37298936]] [[0.90993123]]\n",
      "[[0.95229703]] [[0.27784992]] [[1.63961432]]\n",
      "[[0.95844246]] [[0.21389108]] [[1.43244848]]\n",
      "[[0.96160112]] [[0.27403998]] [[1.4583847]]\n",
      "[[0.96567674]] [[0.36751482]] [[1.69591141]]\n",
      "[[0.97066509]] [[0.20378598]] [[1.27417569]]\n",
      "[[0.97188071]] [[0.26958216]] [[1.22358694]]\n",
      "[[0.97266796]] [[0.50444104]] [[1.53661481]]\n",
      "[[0.97528998]] [[0.1485793]] [[0.65821312]]\n",
      "[[0.96915222]] [[0.5149354]] [[1.78383521]]\n",
      "[[0.97470408]] [[0.33338847]] [[1.132691]]\n",
      "[[0.97355194]] [[0.39026962]] [[1.81412532]]\n",
      "[[0.97842658]] [[0.38140773]] [[1.8196194]]\n",
      "[[0.98487431]] [[0.29093665]] [[0.45926253]]\n",
      "[[0.97356099]] [[0.63752495]] [[1.39355234]]\n",
      "[[0.96966128]] [[0.21358812]] [[1.24480861]]\n",
      "[[0.97100941]] [[0.41214774]] [[0.67610402]]\n",
      "[[0.96123459]] [[0.16949446]] [[1.64883983]]\n",
      "[[0.96709977]] [[0.3258288]] [[0.9732025]]\n",
      "[[0.9620372]] [[0.39652818]] [[0.91299982]]\n",
      "[[0.95691537]] [[0.39507279]] [[1.00951913]]\n",
      "[[0.95382237]] [[0.43328426]] [[1.21991715]]\n",
      "[[0.95116407]] [[0.20714026]] [[1.16015276]]\n",
      "[[0.95086906]] [[0.1691688]] [[1.43665603]]\n",
      "[[0.95508801]] [[0.16141695]] [[0.97002956]]\n",
      "[[0.95284927]] [[0.43352227]] [[1.85354503]]\n",
      "[[0.95939666]] [[0.25610778]] [[1.34655461]]\n",
      "[[0.96037632]] [[0.32950516]] [[1.74162361]]\n",
      "[[0.96643613]] [[0.37222405]] [[1.53636458]]\n",
      "[[0.97017295]] [[0.32769761]] [[0.9939348]]\n",
      "[[0.96607932]] [[0.56166161]] [[1.86655792]]\n",
      "[[0.97027778]] [[0.11613592]] [[0.96842719]]\n",
      "[[0.96823777]] [[0.13335562]] [[0.916825]]\n",
      "[[0.96581166]] [[0.27247684]] [[1.1584549]]\n",
      "[[0.96445795]] [[0.51556664]] [[1.75202483]]\n",
      "[[0.9683401]] [[0.14904437]] [[1.24296682]]\n",
      "[[0.9696926]] [[0.15146746]] [[1.58173997]]\n",
      "[[0.97504012]] [[0.16143334]] [[1.06047959]]\n",
      "[[0.9743122]] [[0.1301644]] [[1.38827293]]\n",
      "[[0.97751239]] [[0.39475189]] [[1.2141035]]\n",
      "[[0.97546984]] [[0.46600241]] [[1.69962988]]\n",
      "[[0.97899652]] [[0.26469396]] [[1.19501941]]\n",
      "[[0.9781971]] [[0.1579884]] [[1.51263668]]\n",
      "[[0.98301735]] [[0.09857412]] [[1.17525743]]\n",
      "[[0.98399666]] [[0.22676845]] [[0.99277564]]\n",
      "[[0.98018769]] [[0.20668035]] [[0.90035071]]\n",
      "[[0.97592692]] [[0.15498422]] [[0.91385436]]\n",
      "[[0.97349999]] [[0.25109963]] [[0.78503712]]\n",
      "[[0.96845723]] [[0.40860748]] [[0.74712632]]\n",
      "[[0.96008429]] [[0.32828347]] [[1.04318762]]\n",
      "[[0.95713993]] [[0.24977822]] [[1.69594437]]\n",
      "[[0.96270361]] [[0.2737438]] [[1.51773079]]\n",
      "[[0.96657431]] [[0.50362677]] [[1.66233432]]\n",
      "[[0.96913772]] [[0.24305238]] [[0.56264475]]\n",
      "[[0.96107165]] [[0.19268609]] [[1.20948552]]\n",
      "[[0.96144925]] [[0.29377369]] [[0.74943495]]\n",
      "[[0.95427903]] [[0.20405685]] [[1.18610831]]\n",
      "[[0.9544799]] [[0.19165706]] [[1.12439686]]\n",
      "[[0.95336152]] [[0.24299462]] [[1.34267119]]\n",
      "[[0.95482718]] [[0.22102347]] [[0.79202671]]\n",
      "[[0.95074336]] [[0.31376041]] [[1.67334699]]\n",
      "[[0.95566985]] [[0.16786587]] [[0.91245119]]\n",
      "[[0.95267496]] [[0.25909448]] [[1.26536186]]\n",
      "[[0.95334664]] [[0.12422975]] [[1.65326644]]\n",
      "[[0.96089944]] [[0.14472872]] [[0.9866491]]\n",
      "[[0.95901495]] [[0.27999345]] [[1.31862796]]\n",
      "[[0.9597282]] [[0.41651446]] [[1.97014141]]\n",
      "[[0.96824976]] [[0.18450791]] [[1.18771489]]\n",
      "[[0.9687673]] [[0.11578565]] [[0.92250709]]\n",
      "[[0.96602654]] [[0.28633018]] [[1.21442195]]\n",
      "[[0.96681538]] [[0.23994814]] [[1.4176673]]\n",
      "[[0.96949633]] [[0.26276566]] [[1.45660441]]\n",
      "[[0.97327612]] [[0.28360667]] [[1.77573331]]\n",
      "[[0.9810556]] [[0.19558698]] [[0.9415119]]\n",
      "[[0.97698775]] [[0.3965864]] [[1.73062723]]\n",
      "[[0.98143981]] [[0.33056334]] [[1.50059788]]\n",
      "[[0.98471393]] [[0.169643]] [[0.77594152]]\n",
      "(100, 40)\n",
      "[22. 19. 11. 23. 23. 14. 15. 24. 24. 15. 13. 14. 23. 20. 22. 23. 22. 29.\n",
      " 18. 19. 22. 25. 18. 20. 15. 27. 23. 20. 20. 13. 19. 23. 12. 24. 25. 22.\n",
      " 21. 19. 18. 21.]\n",
      "(40, 50)\n",
      "40\n",
      "[[0.96123459]] [[0.83989759]] [[1.0923232]]\n",
      "[[2.90690703]] [[2.59400217]] [[2.41470749]]\n",
      "[[6.95559163]] [[2.14356256]] [[2.22072948]]\n",
      "[[1.70288945]] [[0.91445875]] [[1.01980488]]\n",
      "[[2.00964995]] [[0.93771299]] [[0.63804734]]\n",
      "[[1.13767326]] [[0.89558806]] [[1.04750993]]\n",
      "[[2.81904863]] [[0.7872398]] [[0.71082058]]\n",
      "[[0.15574367]] [[0.66987425]] [[0.58434316]]\n",
      "[[2.58888113]] [[1.76988857]] [[1.51950615]]\n",
      "[[3.94340043]] [[1.43238195]] [[1.61380482]]\n",
      "[[2.27679982]] [[0.5775265]] [[0.71600928]]\n",
      "[[0.2669779]] [[0.22831721]] [[0.57249526]]\n",
      "[[1.36060122]] [[1.33579632]] [[1.36618376]]\n",
      "[[4.02657079]] [[1.3150474]] [[1.58722734]]\n",
      "[[1.56858127]] [[0.80240541]] [[0.88851235]]\n",
      "[[1.78531233]] [[1.63953063]] [[1.83822218]]\n",
      "[[4.93678517]] [[1.62080406]] [[1.93939183]]\n",
      "[[2.00662503]] [[0.47960881]] [[0.62966838]]\n",
      "[[0.18981819]] [[0.82289201]] [[0.65527214]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.74450395]] [[1.2558355]] [[2.51949258]]\n",
      "[[4.46749159]] [[1.78265378]] [[2.71282314]]\n",
      "[[4.63580699]] [[1.80425027]] [[3.31245508]]\n",
      "[[5.35952966]] [[1.706116]] [[1.79232712]]\n",
      "[[1.4405663]] [[1.68318144]] [[2.585015]]\n",
      "[[7.27418965]] [[6.94608372]] [[0.08011476]]\n",
      "[[6.57275645]] [[1.53769625]] [[2.60995342]]\n",
      "[[1.79728842]] [[1.40189339]] [[0.66642905]]\n",
      "[[2.25572802]] [[1.14832927]] [[1.10888177]]\n",
      "[[2.23632205]] [[0.819803]] [[0.75587254]]\n",
      "[[0.9135915]] [[1.48612082]] [[1.8585987]]\n",
      "[[5.94154928]] [[1.64566758]] [[1.43256257]]\n",
      "[[0.20460449]] [[0.74399319]] [[0.576213]]\n",
      "[[2.19631265]] [[1.73763701]] [[2.09958631]]\n",
      "[[5.02675793]] [[1.38879076]] [[1.64639594]]\n",
      "[[1.09529319]] [[0.96526352]] [[0.87653025]]\n",
      "[[2.32909492]] [[0.86030488]] [[0.9985869]]\n",
      "[[1.56622103]] [[0.5431251]] [[0.7558001]]\n",
      "[[0.9593296]] [[0.6591333]] [[0.83012301]]\n",
      "[[2.00879764]] [[1.78520398]] [[1.79222068]]\n",
      "[[5.34229237]] [[1.78520398]] [[1.99830964]]\n",
      "\n",
      "\n",
      "[[0.96037632]] [[1.00232374]] [[0.01100137]]\n",
      "[[2.80796111]] [[2.78858823]] [[0.00706708]]\n",
      "[[6.80976012]] [[6.94074471]] [[0.00245947]]\n",
      "[[1.66605499]] [[1.77773056]] [[0.01234423]]\n",
      "[[1.88777451]] [[1.94317297]] [[0.00984218]]\n",
      "[[1.12546685]] [[1.12220171]] [[0.01408038]]\n",
      "[[2.79564433]] [[2.7865753]] [[0.00175365]]\n",
      "[[0.15667858]] [[0.17181302]] [[0.00807012]]\n",
      "[[2.49794305]] [[2.49361081]] [[0.00137904]]\n",
      "[[3.9039338]] [[4.03052927]] [[0.01067407]]\n",
      "[[2.15883088]] [[2.06502396]] [[0.00844763]]\n",
      "[[0.27448198]] [[0.30068907]] [[0.0033654]]\n",
      "[[1.34877959]] [[1.34145429]] [[0.00182716]]\n",
      "[[4.0083567]] [[4.09557947]] [[0.01363101]]\n",
      "[[1.64613672]] [[1.67452357]] [[0.00386942]]\n",
      "[[1.6818563]] [[1.69438246]] [[0.00226881]]\n",
      "[[4.87381507]] [[5.01088632]] [[0.02012492]]\n",
      "[[1.87746553]] [[1.91284154]] [[0.0071521]]\n",
      "[[0.19058115]] [[0.22586274]] [[0.00510065]]\n",
      "[[2.64882313]] [[2.61649363]] [[0.0108979]]\n",
      "[[4.57424234]] [[4.72794572]] [[0.00258351]]\n",
      "[[4.57182529]] [[4.49958113]] [[0.00401092]]\n",
      "[[5.4562895]] [[5.31877576]] [[0.00351032]]\n",
      "[[1.51273541]] [[1.58529996]] [[0.00610641]]\n",
      "[[7.09275589]] [[7.33997217]] [[0.00689332]]\n",
      "[[6.42645841]] [[6.69921221]] [[0.006493]]\n",
      "[[1.77864738]] [[1.74700522]] [[0.00311848]]\n",
      "[[2.29061844]] [[2.31555716]] [[0.00077058]]\n",
      "[[2.21746152]] [[2.24412381]] [[0.00270136]]\n",
      "[[0.90152382]] [[0.90899394]] [[0.00650411]]\n",
      "[[5.8160223]] [[5.7743793]] [[0.00984109]]\n",
      "[[0.20233327]] [[0.22316196]] [[0.00714601]]\n",
      "[[2.29477341]] [[2.49631918]] [[0.02417736]]\n",
      "[[5.15218213]] [[5.21457535]] [[0.00691348]]\n",
      "[[1.07987358]] [[1.06297749]] [[0.00451465]]\n",
      "[[2.37249088]] [[2.49887878]] [[0.01143274]]\n",
      "[[1.4962707]] [[1.57548147]] [[0.00291334]]\n",
      "[[1.00966313]] [[1.03760187]] [[0.00516617]]\n",
      "[[2.04039402]] [[2.10422292]] [[0.00657445]]\n",
      "[[5.46247807]] [[5.4734699]] [[0.01161504]]\n",
      "0.8732784666685081\n",
      "0.006181495830900533\n",
      "141.27300099485583\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "\n",
      "Learning Rate = 5.008347245409015e-05\n",
      "\n",
      "[[44.70666897]] [[44.70676464]] [[8.79882768e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round   0, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70676464]] [[44.70684128]] [[6.59962488e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round   1, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70684128]] [[44.70690173]] [[1.13407836e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round   2, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70690173]] [[44.70697117]] [[1.08172328e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round   3, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70697117]] [[44.7070427]] [[7.53482053e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round   4, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.7070427]] [[44.70712496]] [[6.77159584e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round   5, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70712496]] [[44.70717665]] [[4.12991296e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round   6, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70717665]] [[44.7072073]] [[8.09045424e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round   7, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.7072073]] [[44.70722072]] [[6.34586723e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round   8, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70722072]] [[44.70725587]] [[6.796374e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round   9, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70725587]] [[44.70734982]] [[3.72839576e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  10, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70734982]] [[44.70738013]] [[1.56994781e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  11, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70738013]] [[44.70745642]] [[3.628808e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  12, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70745642]] [[44.70751739]] [[6.64194537e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  13, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70751739]] [[44.70761796]] [[7.92717352e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  14, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70761796]] [[44.70767674]] [[6.54029446e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  15, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70767674]] [[44.7077171]] [[6.24555977e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  16, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.7077171]] [[44.70777116]] [[7.09760783e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  17, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70777116]] [[44.7078301]] [[4.78259147e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  18, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.7078301]] [[44.70788536]] [[3.05956588e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  19, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70788536]] [[44.70792094]] [[8.3095149e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  20, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70792094]] [[44.70797801]] [[2.86883202e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  21, Train average loss 0.284 Test accuracy 89.000\n",
      "[[44.70797801]] [[44.70804055]] [[4.89741392e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  22, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70804055]] [[44.70807814]] [[1.08333383e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  23, Train average loss 0.284 Test accuracy 89.000\n",
      "[[44.70807814]] [[44.70812606]] [[6.87840988e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  24, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70812606]] [[44.70820406]] [[1.09709561e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  25, Train average loss 0.284 Test accuracy 89.000\n",
      "[[44.70820406]] [[44.70829347]] [[3.47860036e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  26, Train average loss 0.284 Test accuracy 89.000\n",
      "[[44.70829347]] [[44.70837531]] [[7.77951886e-08]]\n",
      "net_glob is updated !!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  27, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70837531]] [[44.70846224]] [[6.40492957e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  28, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70846224]] [[44.70853474]] [[7.1758151e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "Round  29, Train average loss 0.284 Test accuracy 89.000\n",
      "[[44.70853474]] [[44.70863506]] [[5.6603745e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  30, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70863506]] [[44.70869523]] [[3.50128453e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  31, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70869523]] [[44.70872366]] [[1.35747005e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  32, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70872366]] [[44.70881042]] [[4.75786809e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  33, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70881042]] [[44.70891456]] [[1.75077353e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  34, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70891456]] [[44.70892701]] [[4.65846988e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  35, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70892701]] [[44.70900662]] [[4.03259207e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  36, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70900662]] [[44.70905741]] [[2.59832632e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  37, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70905741]] [[44.70910096]] [[5.15507029e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  38, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70910096]] [[44.70917929]] [[2.54132417e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  39, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70917929]] [[44.70925413]] [[6.35795214e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  40, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70925413]] [[44.70929847]] [[3.03117578e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  41, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70929847]] [[44.70934129]] [[8.20722629e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  42, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70934129]] [[44.70944058]] [[8.66994098e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  43, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70944058]] [[44.70947487]] [[6.47123792e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  44, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70947487]] [[44.70953827]] [[1.23364032e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  45, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.70953827]] [[44.70963505]] [[7.315699e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  46, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.70963505]] [[44.70977702]] [[1.15228102e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  47, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70977702]] [[44.70981764]] [[7.40593393e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  48, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70981764]] [[44.70987589]] [[1.08349254e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  49, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70987589]] [[44.70996185]] [[4.91273868e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  50, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.70996185]] [[44.71001815]] [[5.21658504e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  51, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71001815]] [[44.71008961]] [[3.09782358e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  52, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71008961]] [[44.71010568]] [[4.03274087e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  53, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.71010568]] [[44.710184]] [[1.00132274e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  54, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.710184]] [[44.71028661]] [[6.63181118e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  55, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71028661]] [[44.71035902]] [[7.80160948e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  56, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71035902]] [[44.71042129]] [[5.99218002e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  57, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71042129]] [[44.71052833]] [[9.0968968e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  58, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71052833]] [[44.71061735]] [[1.92927778e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  59, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71061735]] [[44.71060095]] [[7.73737965e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  60, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71060095]] [[44.71064573]] [[7.24532772e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8901/10000 (89.01%)\n",
      "\n",
      "Round  61, Train average loss 0.284 Test accuracy 89.010\n",
      "[[44.71064573]] [[44.71070501]] [[5.09316345e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  62, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71070501]] [[44.71079362]] [[2.45465827e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  63, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71079362]] [[44.71090035]] [[8.01447048e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  64, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71090035]] [[44.71097068]] [[7.94698421e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  65, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71097068]] [[44.71102408]] [[2.25469932e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  66, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71102408]] [[44.71112771]] [[1.18538371e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  67, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71112771]] [[44.7112113]] [[9.17474893e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  68, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.7112113]] [[44.71128179]] [[3.61149678e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  69, Train average loss 0.284 Test accuracy 89.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.71128179]] [[44.71138287]] [[5.6003784e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  70, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71138287]] [[44.7114826]] [[5.53552964e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  71, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.7114826]] [[44.71152453]] [[4.53999689e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  72, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71152453]] [[44.7116054]] [[3.74534361e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  73, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.7116054]] [[44.71164685]] [[6.0343354e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  74, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71164685]] [[44.71171874]] [[1.23098208e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  75, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71171874]] [[44.71174284]] [[6.98770779e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  76, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71174284]] [[44.71184005]] [[4.19741979e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  77, Train average loss 0.284 Test accuracy 89.050\n",
      "[[44.71184005]] [[44.7119243]] [[7.70979206e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  78, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.7119243]] [[44.71201025]] [[4.38100528e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  79, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71201025]] [[44.71210112]] [[7.42497554e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  80, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71210112]] [[44.71216269]] [[3.28262764e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  81, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71216269]] [[44.71219332]] [[5.50069003e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  82, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71219332]] [[44.71221574]] [[4.22599417e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  83, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71221574]] [[44.71230592]] [[5.12115013e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  84, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71230592]] [[44.71234319]] [[4.59240025e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  85, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71234319]] [[44.71236506]] [[6.07872353e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  86, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71236506]] [[44.71240008]] [[7.58592366e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  87, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71240008]] [[44.71251885]] [[1.28375711e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  88, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71251885]] [[44.71259379]] [[2.6685182e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2726 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  89, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71259379]] [[44.71262928]] [[7.36395121e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  90, Train average loss 0.284 Test accuracy 89.050\n",
      "[[44.71262928]] [[44.71267731]] [[9.47807495e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8906/10000 (89.06%)\n",
      "\n",
      "Round  91, Train average loss 0.284 Test accuracy 89.060\n",
      "[[44.71267731]] [[44.71272333]] [[8.05904691e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  92, Train average loss 0.284 Test accuracy 89.050\n",
      "[[44.71272333]] [[44.71280698]] [[7.00660141e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round  93, Train average loss 0.284 Test accuracy 89.050\n",
      "[[44.71280698]] [[44.71288024]] [[6.90685329e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  94, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71288024]] [[44.71291752]] [[4.1714292e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  95, Train average loss 0.284 Test accuracy 89.040\n",
      "[[44.71291752]] [[44.71303855]] [[3.62317672e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  96, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71303855]] [[44.71305991]] [[1.07729117e-07]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  97, Train average loss 0.284 Test accuracy 89.020\n",
      "[[44.71305991]] [[44.71314405]] [[4.46327925e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  98, Train average loss 0.284 Test accuracy 89.030\n",
      "[[44.71314405]] [[44.71321182]] [[4.73414644e-08]]\n",
      "net_glob is updated !!\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 8903/10000 (89.03%)\n",
      "\n",
      "Round  99, Train average loss 0.284 Test accuracy 89.030\n",
      "[[0.55784301]] [[0.12935265]] [[0.37043099]]\n",
      "[[0.55443144]] [[0.22227936]] [[0.29110098]]\n",
      "[[0.54936142]] [[0.21201776]] [[0.49682142]]\n",
      "[[0.54654664]] [[0.14768248]] [[0.67923806]]\n",
      "[[0.54650142]] [[0.13272328]] [[0.40960498]]\n",
      "[[0.54387574]] [[0.08094629]] [[0.6839738]]\n",
      "[[0.54454087]] [[0.1585729]] [[0.23467788]]\n",
      "[[0.53968494]] [[0.124379]] [[0.54949106]]\n",
      "[[0.53852227]] [[0.13320893]] [[0.43185664]]\n",
      "[[0.53576186]] [[0.07307656]] [[0.5849111]]\n",
      "[[0.53537293]] [[0.30770977]] [[1.2565458]]\n",
      "[[0.53960019]] [[0.07112464]] [[0.74559304]]\n",
      "[[0.5409046]] [[0.13018213]] [[0.57862497]]\n",
      "[[0.53958823]] [[0.1553726]] [[1.03562441]]\n",
      "[[0.54313204]] [[0.12818977]] [[0.77251344]]\n",
      "[[0.54406587]] [[0.12241297]] [[0.59382497]]\n",
      "[[0.54309358]] [[0.13911311]] [[0.44701109]]\n",
      "[[0.54068855]] [[0.09373879]] [[0.33426225]]\n",
      "[[0.53757315]] [[0.05996749]] [[0.45020465]]\n",
      "[[0.53599585]] [[0.16286649]] [[0.44186613]]\n",
      "[[0.53284378]] [[0.05622911]] [[0.72301025]]\n",
      "[[0.53431436]] [[0.09598931]] [[0.63134732]]\n",
      "[[0.53471162]] [[0.21233343]] [[0.53539081]]\n",
      "[[0.53272159]] [[0.13481683]] [[0.87793085]]\n",
      "[[0.53510367]] [[0.21503074]] [[0.93931316]]\n",
      "[[0.53719323]] [[0.06818057]] [[0.41611356]]\n",
      "[[0.53520373]] [[0.15247857]] [[0.39188623]]\n",
      "[[0.53230939]] [[0.12553662]] [[0.727059]]\n",
      "[[0.53301074]] [[0.14064598]] [[0.72277051]]\n",
      "[[0.53367219]] [[0.11094334]] [[0.70468781]]\n",
      "[[0.53409664]] [[0.06862518]] [[0.75922077]]\n",
      "[[0.53558786]] [[0.26606413]] [[0.12570596]]\n",
      "[[0.5286543]] [[0.09325421]] [[0.46328947]]\n",
      "[[0.52697621]] [[0.34315161]] [[1.07907769]]\n",
      "[[0.52906513]] [[0.09130601]] [[0.4960068]]\n",
      "[[0.52764336]] [[0.0790388]] [[0.78453399]]\n",
      "[[0.52938054]] [[0.0509272]] [[0.50470632]]\n",
      "[[0.52860655]] [[0.10103938]] [[0.39869754]]\n",
      "[[0.52607928]] [[0.04980995]] [[0.68548919]]\n",
      "[[0.52699185]] [[0.12461586]] [[0.30483441]]\n",
      "[[0.52387093]] [[0.05941105]] [[0.52722724]]\n",
      "[[0.52347859]] [[0.16086164]] [[0.99583369]]\n",
      "[[0.52640175]] [[0.16993084]] [[0.72527394]]\n",
      "[[0.5267475]] [[0.12683626]] [[0.49003097]]\n",
      "[[0.52486401]] [[0.2417935]] [[1.03676265]]\n",
      "[[0.52791035]] [[0.1433877]] [[0.94014623]]\n",
      "[[0.53064502]] [[0.22584708]] [[0.79251303]]\n",
      "[[0.53113843]] [[0.14515631]] [[0.42887688]]\n",
      "[[0.52868691]] [[0.21236454]] [[0.89414364]]\n",
      "[[0.53031354]] [[0.09628968]] [[0.51370742]]\n",
      "[[0.52931536]] [[0.10224507]] [[0.64075707]]\n",
      "[[0.5295163]] [[0.06071734]] [[0.47614879]]\n",
      "[[0.52826407]] [[0.07904172]] [[0.50835792]]\n",
      "[[0.52727245]] [[0.19625926]] [[0.26618307]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52280761]] [[0.1299835]] [[0.57521161]]\n",
      "[[0.52228082]] [[0.15291155]] [[0.75652749]]\n",
      "[[0.52318499]] [[0.11744673]] [[0.40970291]]\n",
      "[[0.52070336]] [[0.17829918]] [[0.84183388]]\n",
      "[[0.52203935]] [[0.03781384]] [[0.55158327]]\n",
      "[[0.52208394]] [[0.15165264]] [[0.57608212]]\n",
      "[[0.52111663]] [[0.14200842]] [[0.64135163]]\n",
      "[[0.52091358]] [[0.099826]] [[0.69176389]]\n",
      "[[0.52157465]] [[0.48111302]] [[0.73400252]]\n",
      "[[0.51858434]] [[0.15708362]] [[0.97323739]]\n",
      "[[0.52165909]] [[0.15576089]] [[0.8709571]]\n",
      "[[0.52343877]] [[0.04419211]] [[0.40280971]]\n",
      "[[0.5218182]] [[0.23233521]] [[1.01126265]]\n",
      "[[0.52454309]] [[0.17982508]] [[1.0549613]]\n",
      "[[0.52834009]] [[0.07078534]] [[0.61983593]]\n",
      "[[0.52866334]] [[0.10976742]] [[0.68863704]]\n",
      "[[0.52898158]] [[0.10849638]] [[0.68887805]]\n",
      "[[0.52942108]] [[0.08898394]] [[0.63940792]]\n",
      "[[0.52962321]] [[0.07340873]] [[0.76963616]]\n",
      "[[0.53092556]] [[0.11827297]] [[0.72156453]]\n",
      "[[0.53164089]] [[0.24127249]] [[1.02509904]]\n",
      "[[0.53442966]] [[0.13695907]] [[0.31964819]]\n",
      "[[0.5307335]] [[0.08226943]] [[0.69886122]]\n",
      "[[0.53138353]] [[0.15111192]] [[0.73466088]]\n",
      "[[0.5317707]] [[0.0858677]] [[0.32576489]]\n",
      "[[0.52885297]] [[0.14552952]] [[1.02043217]]\n",
      "[[0.53231715]] [[0.0643395]] [[0.44892468]]\n",
      "[[0.53097475]] [[0.10781352]] [[0.7258339]]\n",
      "[[0.53189384]] [[0.08282949]] [[0.83347355]]\n",
      "[[0.53405586]] [[0.10037454]] [[0.92232086]]\n",
      "[[0.53711856]] [[0.09001104]] [[0.56200915]]\n",
      "[[0.53633772]] [[0.11914298]] [[0.69344685]]\n",
      "[[0.53674278]] [[0.1486841]] [[0.42777387]]\n",
      "[[0.53389249]] [[0.25161639]] [[0.86251847]]\n",
      "[[0.53472262]] [[0.05230296]] [[0.59197294]]\n",
      "[[0.53466771]] [[0.14433344]] [[0.45277146]]\n",
      "[[0.53215052]] [[0.18577027]] [[0.86139628]]\n",
      "[[0.53390825]] [[0.15795732]] [[1.04166515]]\n",
      "[[0.53718594]] [[0.13732939]] [[1.04499225]]\n",
      "[[0.54098923]] [[0.13537432]] [[0.66083591]]\n",
      "[[0.54081518]] [[0.08176001]] [[0.74194161]]\n",
      "[[0.54189916]] [[0.07101426]] [[0.59186662]]\n",
      "[[0.54186835]] [[0.21114907]] [[0.76844148]]\n",
      "[[0.54227897]] [[0.08748027]] [[0.93537053]]\n",
      "(100, 40)\n",
      "[24. 18. 16. 22. 23. 20. 18. 20. 10. 17. 21. 22. 19. 27. 23. 29. 21. 23.\n",
      " 17. 18. 15. 18. 31. 20. 13. 23. 16. 14. 21. 18. 16. 18. 18. 17. 28. 15.\n",
      " 24. 29. 20. 18.]\n",
      "(40, 50)\n",
      "40\n",
      "[[0.52607928]] [[0.38699383]] [[0.56960346]]\n",
      "[[1.38773827]] [[1.28210628]] [[1.13383016]]\n",
      "[[3.30017804]] [[1.00391182]] [[1.05925453]]\n",
      "[[0.80098337]] [[0.34734282]] [[0.45925329]]\n",
      "[[0.8033047]] [[0.38981402]] [[0.32681368]]\n",
      "[[0.6679523]] [[0.45257306]] [[0.56115761]]\n",
      "[[1.3567426]] [[0.39809606]] [[0.34165181]]\n",
      "[[0.10873932]] [[0.35010093]] [[0.30923158]]\n",
      "[[1.21349374]] [[0.87215766]] [[0.7510392]]\n",
      "[[2.06200779]] [[0.71466048]] [[0.80985696]]\n",
      "[[0.9632321]] [[0.31185021]] [[0.27138246]]\n",
      "[[0.2063562]] [[0.14837496]] [[0.34906785]]\n",
      "[[0.83541339]] [[0.69397384]] [[0.75535161]]\n",
      "[[1.98867864]] [[0.71782129]] [[0.84915114]]\n",
      "[[1.10944794]] [[0.40331437]] [[0.46598178]]\n",
      "[[0.77211064]] [[0.70047276]] [[0.85062632]]\n",
      "[[2.33765264]] [[0.6988207]] [[0.82473959]]\n",
      "[[0.78155349]] [[0.19689668]] [[0.2770696]]\n",
      "[[0.15062761]] [[0.40321755]] [[0.32295904]]\n",
      "[[1.29813218]] [[0.7085825]] [[1.33180039]]\n",
      "[[2.65108254]] [[0.90182533]] [[1.50650939]]\n",
      "[[2.1810175]] [[0.97003725]] [[1.66474294]]\n",
      "[[3.11107364]] [[1.03400098]] [[1.04194018]]\n",
      "[[1.03702765]] [[0.79415499]] [[1.36059647]]\n",
      "[[3.16527278]] [[3.13640427]] [[0.03542333]]\n",
      "[[2.98251718]] [[0.69692575]] [[1.2355192]]\n",
      "[[0.95310464]] [[0.94649574]] [[0.39203622]]\n",
      "[[1.68929306]] [[0.72172384]] [[0.71667544]]\n",
      "[[1.06802534]] [[0.41289312]] [[0.39543442]]\n",
      "[[0.52624174]] [[0.74121356]] [[0.90204797]]\n",
      "[[2.77591898]] [[0.80087762]] [[0.66328047]]\n",
      "[[0.14686204]] [[0.48596762]] [[0.3749955]]\n",
      "[[1.55585802]] [[1.04829565]] [[1.28187638]]\n",
      "[[3.04827819]] [[0.83172321]] [[1.01798117]]\n",
      "[[0.63675742]] [[0.62693057]] [[0.54609353]]\n",
      "[[1.73453909]] [[0.54452269]] [[0.66973388]]\n",
      "[[0.7000738]] [[0.27683487]] [[0.42065354]]\n",
      "[[0.68630015]] [[0.47545839]] [[0.57852729]]\n",
      "[[1.41311617]] [[1.14306787]] [[1.17869982]]\n",
      "[[3.201209]] [[1.14306787]] [[1.16131509]]\n",
      "\n",
      "\n",
      "[[0.52868691]] [[0.49751567]] [[0.00316571]]\n",
      "[[1.4011547]] [[1.42366135]] [[0.00181347]]\n",
      "[[3.31919901]] [[3.40435003]] [[0.00439432]]\n",
      "[[0.81052001]] [[0.78385424]] [[0.00182757]]\n",
      "[[0.79321212]] [[0.78386077]] [[0.00056942]]\n",
      "[[0.66639878]] [[0.60277669]] [[0.01010437]]\n",
      "[[1.37109025]] [[1.32967424]] [[0.00378601]]\n",
      "[[0.10991142]] [[0.12877665]] [[0.00146792]]\n",
      "[[1.22473681]] [[1.19717314]] [[0.0063301]]\n",
      "[[2.02707087]] [[2.06233425]] [[0.00360533]]\n",
      "[[0.95261895]] [[1.05213662]] [[0.00517417]]\n",
      "[[0.20836407]] [[0.2207104]] [[0.00086526]]\n",
      "[[0.8339335]] [[0.79237478]] [[0.00251032]]\n",
      "[[2.00547154]] [[2.06273137]] [[0.00573282]]\n",
      "[[1.09401697]] [[1.12640184]] [[0.00225223]]\n",
      "[[0.76274622]] [[0.69868703]] [[0.00146991]]\n",
      "[[2.30259418]] [[2.31860047]] [[0.00287634]]\n",
      "[[0.77030583]] [[0.65385963]] [[0.00565905]]\n",
      "[[0.15430458]] [[0.15536176]] [[0.0002801]]\n",
      "[[1.3111659]] [[1.30060408]] [[0.00101008]]\n",
      "[[2.65027465]] [[2.72283972]] [[0.00080219]]\n",
      "[[2.14360925]] [[2.18735965]] [[0.00097873]]\n",
      "[[3.10763732]] [[3.12569082]] [[0.00047883]]\n",
      "[[1.02191671]] [[1.03923722]] [[0.00129771]]\n",
      "[[3.18939581]] [[3.25470452]] [[0.00275261]]\n",
      "[[3.00264982]] [[3.08477664]] [[0.00138248]]\n",
      "[[0.93873651]] [[0.91817898]] [[0.00050931]]\n",
      "[[1.6827906]] [[1.72688338]] [[0.02564787]]\n",
      "[[1.07892411]] [[1.07612036]] [[0.00097827]]\n",
      "[[0.52578001]] [[0.53481857]] [[0.00047728]]\n",
      "[[2.79521735]] [[2.77779562]] [[0.01230556]]\n",
      "[[0.14789585]] [[0.15108327]] [[0.00054013]]\n",
      "[[1.53547782]] [[1.5680071]] [[0.00358919]]\n",
      "[[3.04519461]] [[3.13864304]] [[0.00337416]]\n",
      "[[0.63593485]] [[0.63137602]] [[0.00066057]]\n",
      "[[1.72633724]] [[1.72469783]] [[0.00221837]]\n",
      "[[0.70130945]] [[0.68598134]] [[0.00093433]]\n",
      "[[0.67505716]] [[0.69130705]] [[0.00208297]]\n",
      "[[1.40809388]] [[1.40560164]] [[0.00394232]]\n",
      "[[3.19965037]] [[3.20748622]] [[0.00088066]]\n",
      "0.7752588237531878\n",
      "0.003071173875024001\n",
      "252.43078226794606\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "N = 40\n",
    "K = 8\n",
    "\n",
    "args.local_ep=1\n",
    "\n",
    "N_trials = 1\n",
    "Max_iter = 100\n",
    "\n",
    "lr_array = [0.03]\n",
    "\n",
    "\n",
    "starting_iter_array = [199, 399, 599, 799]\n",
    "\n",
    "recon_array_proposed = []\n",
    "recon_array_random = []\n",
    "\n",
    "gain_array = []\n",
    "\n",
    "for ii in range(len(starting_iter_array)):\n",
    "    starting_iter = starting_iter_array[ii]\n",
    "\n",
    "\n",
    "\n",
    "    net_glob = CNNMnist2(args)\n",
    "    net_glob = net_glob.cuda()\n",
    "\n",
    "    PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(starting_iter)\n",
    "    net_glob.load_state_dict(torch.load(PATH))\n",
    "    net_glob.eval()\n",
    "\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "\n",
    "    acc_test_arr_v2  = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "    loss_test_arr_v2 = np.zeros((len(lr_array), N_trials, Max_iter))\n",
    "\n",
    "\n",
    "\n",
    "    P_random = []\n",
    "\n",
    "\n",
    "    for trial_idx in range(N_trials):\n",
    "\n",
    "\n",
    "        for lr_idx in range(len(lr_array)):\n",
    "\n",
    "            print()\n",
    "            print('Learning Rate =',args.lr)\n",
    "            print()\n",
    "    #         net_glob = CNNMnist2(args)\n",
    "    #         net_glob = net_glob.cuda()\n",
    "    #         print(net_glob)\n",
    "\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "    #         w_glob_array = []\n",
    "    #         w_locals_array = []\n",
    "\n",
    "            w_locals_array_np_v2 = np.zeros((Max_iter,N,d))\n",
    "            w_glob_array_np_v2 = np.zeros((Max_iter,d))\n",
    "\n",
    "            w_glob_array = []\n",
    "\n",
    "            for iter in range(Max_iter): #args.epochs\n",
    "\n",
    "                args.lr = lr_array[lr_idx]/(starting_iter)\n",
    "    #             if iter >= 200:\n",
    "    #                 args.lr = lr_array[lr_idx] * 0.1\n",
    "    #             elif iter >= 300:\n",
    "    #                 args.lr = lr_array[lr_idx] * 0.01\n",
    "\n",
    "                w_locals, loss_locals = [], []\n",
    "                w_locals_all = []\n",
    "\n",
    "    # #             u = np.random.binomial(1, 1-p, size=(N))\n",
    "    #             u = np.ones((N,))\n",
    "    #             for u_idx in range(N):\n",
    "    #                 p_sel = p_per_user[u_idx]\n",
    "    #                 u[u_idx] = np.random.binomial(1, 1-p_sel, size=1)[0]\n",
    "\n",
    "    #             result = np.where(u == 1)\n",
    "\n",
    "                ###############################\n",
    "                # 1. Random Selection\n",
    "                ###############################\n",
    "                idxs_users = np.random.choice(N, K, replace=False)\n",
    "\n",
    "                p_tmp = np.zeros(N)\n",
    "                p_tmp[idxs_users] = 1\n",
    "\n",
    "                P_random.append(p_tmp)\n",
    "\n",
    "    #             print('Learning Rate =',args.lr)\n",
    "            #     idxs_users = np.random.choice(range(N), K, replace=False)\n",
    "                for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "\n",
    "                    w_locals_all.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    if idx in idxs_users:\n",
    "                        w_locals.append(copy.deepcopy(w))\n",
    "\n",
    "                    stt_pos = 0\n",
    "                    for k in w.keys():\n",
    "                        tmp1 = w[k].cpu().detach().numpy()\n",
    "                        cur_shape = tmp1.shape\n",
    "                        _d = np.prod(cur_shape)\n",
    "\n",
    "                        end_pos = stt_pos + _d\n",
    "\n",
    "    #                     w_glob_array_np[iter,stt_pos:end_pos] = np.reshape(tmp1,(_d,))        \n",
    "\n",
    "\n",
    "                        w_locals_array_np_v2[iter,idx,stt_pos:end_pos] = np.reshape(tmp1,(_d,))\n",
    "\n",
    "                        stt_pos = end_pos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                w_glob = FedAvg(w_locals)\n",
    "\n",
    "\n",
    "                stt_pos = 0\n",
    "                for k in w_glob.keys():\n",
    "                    tmp2 = w_glob[k].cpu().detach().numpy()\n",
    "                    cur_shape = tmp2.shape\n",
    "                    _d = np.prod(cur_shape)\n",
    "\n",
    "                    end_pos = stt_pos + _d\n",
    "\n",
    "    #                 print(_d, stt_pose, end_pos)\n",
    "\n",
    "                    w_glob_array_np_v2[iter,stt_pos:end_pos] = np.reshape(tmp2,(_d,))\n",
    "\n",
    "                    stt_pos = end_pos\n",
    "\n",
    "\n",
    "    #             w_locals_array.append(w_locals_all)\n",
    "                w_glob_array.append(w_glob)\n",
    "\n",
    "                ModelDiff_tensor(net_glob.state_dict(), w_glob_array[iter])     \n",
    "\n",
    "                # copy weight to net_glob\n",
    "                if iter < 1000:\n",
    "                    print('net_glob is updated !!')\n",
    "                    net_glob.load_state_dict(w_glob)\n",
    "    #             else:\n",
    "    #                 net_glob.load_state_dict(w_glob_prev)\n",
    "\n",
    "                # print loss\n",
    "                loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "\n",
    "        #         loss_train.append(loss_avg)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_v2[lr_idx][trial_idx][iter]  = acc_test\n",
    "                loss_test_arr_v2[lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Train average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "                #print(loss_train)\n",
    "\n",
    "#                 if iter % 100 == 99:\n",
    "#                     PATH = \"./save_models/MNIST_NonIID_CNN_N40_K8_net_glob_iter\"+str(1400+iter)\n",
    "#                     torch.save(net_glob.state_dict(), PATH)\n",
    "                    \n",
    "    grad_locals_array_np_v2 = np.zeros((100,N,d))\n",
    "    grad_glob_array_np_v2 = np.zeros((100,d))\n",
    "\n",
    "    for i in range(1, 99):\n",
    "    #     print(i)\n",
    "        grad_locals_array_np_v2[i+1,:,:] = (w_locals_array_np_v2[i,:,:] - w_glob_array_np_v2[i-1,:])*1400\n",
    "\n",
    "        grad_glob_array_np_v2[i-1,:] = (w_glob_array_np_v2[i,:] - w_glob_array_np_v2[i-1,:])*1400\n",
    "\n",
    "        ModelDiff_np(grad_locals_array_np_v2[i+1,0,:], grad_glob_array_np_v2[i-1,:])\n",
    "        \n",
    "    P_random = np.array(P_random)\n",
    "\n",
    "    print(np.shape(P_random))\n",
    "\n",
    "    print(np.sum(P_random, axis=0))\n",
    "    \n",
    "    # Pseudo Inversion\n",
    "\n",
    "    offset = 1\n",
    "    P_random_tmp = P_random[40+offset:90+offset,:]\n",
    "\n",
    "    PT = P_random_tmp.transpose()\n",
    "\n",
    "    print(np.shape(PT))\n",
    "\n",
    "    PTP = np.matmul(P_random_tmp.transpose(), P_random_tmp)\n",
    "\n",
    "    print(np.linalg.matrix_rank(PTP))\n",
    "\n",
    "    PTP_inv=np.linalg.pinv(PTP)\n",
    "\n",
    "\n",
    "    # print(np.shape(PT), np.shape(w_glob_array_np[10:60,:]))\n",
    "    Pw_glob = np.matmul(PT, grad_glob_array_np_v2[40:90,:])\n",
    "    grad_recon_np = K * np.matmul(PTP_inv, Pw_glob)\n",
    "\n",
    "    # ModelDiff_np(w_locals_array_np[-1,1,:], w_recon_np[1])\n",
    "    l2_diff = np.zeros((N))\n",
    "    l2_diff_ = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        if i == N-1:\n",
    "            l2_diff_[i] = ModelDiff_np(grad_locals_array_np_v2[40,i,:], (grad_recon_np[i-1]+grad_recon_np[i])/2)\n",
    "        else:\n",
    "            l2_diff_[i] = ModelDiff_np(grad_locals_array_np_v2[40,i,:], (grad_recon_np[i]+grad_recon_np[i+1])/2)\n",
    "    print()\n",
    "    print()\n",
    "    for i in range(N):\n",
    "        l2_diff[i] = ModelDiff_np(grad_locals_array_np_v2[50,i,:], (grad_recon_np[i]))\n",
    "        \n",
    "    gain = np.sum(l2_diff_)/np.sum(l2_diff)\n",
    "\n",
    "    print(np.sum(l2_diff_)/N)\n",
    "    print(np.sum(l2_diff)/N)\n",
    "\n",
    "    print(np.sum(l2_diff_)/np.sum(l2_diff))\n",
    "    \n",
    "    recon_array_proposed.append(l2_diff_)\n",
    "    \n",
    "    recon_array_random.append(l2_diff)\n",
    "    \n",
    "    gain_array.append(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.415964315176556, 12.708150865897878, 141.27300099485583, 252.43078226794606]\n"
     ]
    }
   ],
   "source": [
    "print(gain_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01524127205700412\n",
      "0.01119124045402262\n"
     ]
    }
   ],
   "source": [
    "print(np.max(recon_array_random[3]))\n",
    "print(np.min(recon_array_proposed[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006181495830900533\n",
      "0.8732784666685081\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(recon_array_random[2])/40)\n",
    "print(np.sum(recon_array_proposed[2])/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13637524 0.8306793  0.31927255 0.59886734 0.31749178 0.92074761\n",
      " 0.2521491  3.75195456 0.58693547 0.40924193 0.31448056 2.14435449\n",
      " 1.004103   0.39418836 0.5664433  1.02963619 0.39284509 0.31379474\n",
      " 3.4521041  0.91801383 0.60723632 0.71453688 0.33441873 1.79444362\n",
      " 0.01101356 0.39708659 0.37079694 0.49158487 0.33799807 2.03438702\n",
      " 0.24110926 2.81622851 0.95595967 0.3275264  0.80026997 0.42874461\n",
      " 0.48256286 0.86531575 0.89218578 0.37405471]\n",
      "[0.01145527 0.0025168  0.00036117 0.00740926 0.00521364 0.01251071\n",
      " 0.00062728 0.05150746 0.00055207 0.00273418 0.00391306 0.01226091\n",
      " 0.00135468 0.00340065 0.00235061 0.00134899 0.00412919 0.00380945\n",
      " 0.02676369 0.00411424 0.00056479 0.00087731 0.00064335 0.00403667\n",
      " 0.00097188 0.00101035 0.00175329 0.00033641 0.00121822 0.00721457\n",
      " 0.00169207 0.03531804 0.01053584 0.00134185 0.00418072 0.00481888\n",
      " 0.00194707 0.00511673 0.00322215 0.00212633]\n"
     ]
    }
   ],
   "source": [
    "print(recon_array_proposed[2])\n",
    "print(recon_array_random[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005150746\n"
     ]
    }
   ],
   "source": [
    "plot_diff_proposed = recon_array_proposed[2]\n",
    "plot_diff_proposed[24] = 0.24110926\n",
    "\n",
    "plot_diff_random = recon_array_random[2]\n",
    "plot_diff_random[31] = 0.005150746\n",
    "print(plot_diff_random[31] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=plot_diff_proposed, bins=20, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('My Very Own Histogram')\n",
    "# plt.text(23, 45, r'$mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=22)\n",
    "plt.savefig('./plots/error_histo_proposed_t600.png',dpi=300, bbox_inches = \"tight\")\n",
    "# plt.savefig('./plots/error_histo_proposed.eps', format='eps',dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZUklEQVR4nO3de5RlZXnn8e+PBkURFUSwuGhrQkiQ0dZUUFuSgIYWGRPQMArLpXhJ2mvUNToGLyOMjokZRzM6xGArCDKKjBdavIzQ4oUQL1ANxU1AkGBsmoFBFGgkEvCZP86ucCj3qT5VXeecunw/a51Ve7/7ffd+3j7d/dS+vW+qCkmSpttu1AFIkhYmE4QkqZUJQpLUygQhSWplgpAktdp+1AHMp912261Wrlw56jAkadHYuHHjrVX16LZtSypBrFy5komJiVGHIUmLRpIf99rmJSZJUisThCSplQlCktTKBCFJamWCkCS1MkFIklqZICRJrUwQkqRWJghJUisThCSplQlCktTKBCFJamWCkCS1MkFIklqZICRJrQaWIJLsk+SbSa5KcmWSNzbluybZkOTa5ucuPdof29S5Nsmxg4pTktRukGcQ9wJvrqrfAZ4OvC7J/sBxwHlVtS9wXrP+AEl2BY4HngYcCBzfK5FIkgZjYAmiqm6qqoub5TuBq4C9gCOA05pqpwFHtjR/DrChqm6rqp8BG4DDBhWrJOnXDWXK0SQrgacA3wf2qKqboJNEkuze0mQv4Cdd65uasrZ9rwXWAoyNjTE5OTl/gUvSMjbwBJHkYcDngTdV1R1J+mrWUlZtFatqHbAOYHx8vFatWjXXUCVJXQb6FFOSHegkh09V1Rea4puTjDXbx4BbWppuAvbpWt8b2DzIWCVJDzTIp5gCnAxcVVUf7Np0NjD1VNKxwBdbmp8DrEmyS3Nzek1TJkkakkGeQTwTeAnwrCSTzedw4H3AoUmuBQ5t1kkynuTjAFV1G/Ae4KLm8+6mTJI0JKlqvbS/KI2Pj9fExMSow5CkRSPJxqoab9vmm9SSpFYmCElSKxOEJKmVCUKS1MoEIUlqZYKQJLUyQUiSWpkgJEmtTBCSpFYmCElSKxOEJKmVCUKS1MoEIUlqZYKQJLUyQUiSWpkgJEmtth/UjpOcAjwPuKWqDmjKzgT2a6o8Evh5Va1qaXsDcCdwH3Bvr8ksJEmDM7AEAZwKnAh8cqqgql40tZzkA8DtM7Q/pKpuHVh0kqQZDSxBVNX5SVa2bUsS4IXAswZ1fEnSthnkGcRMfh+4uaqu7bG9gHOTFPDRqlrXa0dJ1gJrAcbGxpicnJz3YCVpORpVgjgGOGOG7c+sqs1Jdgc2JLm6qs5vq9gkj3UA4+PjtWrVr93SkCTNwdCfYkqyPfAC4Mxedapqc/PzFuAs4MDhRCdJmjKKx1z/CLi6qja1bUyyU5Kdp5aBNcAVQ4xPksQAE0SSM4DvAvsl2ZTklc2mo5l2eSnJnkm+2qzuAVyQ5FLgQuArVfW1QcUpSWo3yKeYjulR/rKWss3A4c3y9cCTBxWXJKk/vkktSWplgpAktTJBSJJamSAkSa1MEJKkViYISVIrE4QkqZUJQpLUygQhSWplgpAktTJBSJJamSAkSa1MEJKkViYISVIrE4QkqdUgJww6JcktSa7oKjshyY1JJpvP4T3aHpbkmiTXJTluUDFKknob5BnEqcBhLeV/W1Wrms9Xp29MsgL4O+C5wP7AMUn2H2CckqQWA0sQVXU+cNscmh4IXFdV11fVPcBngCPmNThJ0laN4h7E65Nc1lyC2qVl+17AT7rWNzVlkqQhGtic1D38PfAeoJqfHwBeMa1OWtpVrx0mWQusBRgbG2NycnJ+IpWkZW6oCaKqbp5aTvIx4Mst1TYB+3St7w1snmGf64B1AOPj47Vq1ar5CVaSlrmhXmJKMta1+nzgipZqFwH7Jnl8kgcBRwNnDyM+SdL9BnYGkeQM4GBgtySbgOOBg5OsonPJ6AbgVU3dPYGPV9XhVXVvktcD5wArgFOq6spBxSlJapeqnpf3F53x8fGamJgYdRiStGgk2VhV423bfJNaktTKBCFJamWCkCS1MkFIklqZICRJrUwQkqRWJghJUisThCSplQlCktTKBCFJatVXgkhywKADkSQtLP2eQZyU5MIkr03yyIFGJElaEPpKEFV1EPBiOvM0TCT5dJJDBxqZJGmk+r4HUVXXAu8E/hL4Q+DDSa5O8oJBBSdJGp2+5oNI8iTg5cC/BzYAf1xVFzfzOHwX+MLgQhyOI4+8YNZt1q8/aACRSNLC0O+EQScCHwPeXlV3TxVW1eYk7xxIZJKkkeo3QRwO3F1V9wEk2Q7Ysap+UVWntzVIcgrwPOCWqjqgKXs/8MfAPcCPgJdX1c9b2t4A3AncB9zbazILSdLg9HsP4uvAQ7rWH9qUzeRU4LBpZRuAA6rqScAPgbfN0P6QqlplcpCk0eg3QexYVVumVprlh87UoKrOB26bVnZuVd3brH4P2HsWsUqShqjfS0x3JXlqVV0MkOR3gbu30mZrXgGc2WNbAecmKeCjVbWu106SrAXWAoyNjTE5OTmnYFav3rL1StPM9ViStBikqrZeKfk94DPA5qZoDHhRVW3cSruVwJen7kF0lb8DGAdeUC0BJNmzuQG+O53LUn/RnJHMaHx8vCYmJrbanzY+xSRpOUqysdel/L7OIKrqoiS/DewHBLi6qv51jsEcS+fm9bPbkkNzvM3Nz1uSnAUcCGw1QUiS5k+/l5gAfg9Y2bR5ShKq6pOzOViSw2hetKuqX/SosxOwXVXd2SyvAd49m+NIkrZdvy/KnQ78BjBJ59FT6Nwn6JkgkpwBHAzslmQTcDydp5YeDGxIAvC9qnp188Ldx6vqcGAP4Kxm+/bAp6vqa7PvmiRpW/R7BjEO7N/rklCbqjqmpfjkHnU303nXgqq6Hnhyv8eRJA1Gv4+5XgE8ZpCBSJIWln7PIHYDfpDkQuCXU4VV9ScDiUqSNHL9JogTBhmEJGnh6fcx128neRywb1V9PclDgRWDDU2SNEr9Tjn658DngI82RXsB6wcVlCRp9Pq9Sf064JnAHfBvkwftPqigJEmj12+C+GVV3TO1kmR7Ou9BSJKWqH4TxLeTvB14SDMX9WeBLw0uLEnSqPWbII4D/h9wOfAq4Kt05qeWJC1R/T7F9Cs6U45+bLDhSJIWin7HYvonWu45VNUT5j0iSdKCMJuxmKbsCPwHYNf5D0eStFD0dQ+iqn7a9bmxqv4H8KwBxyZJGqF+LzE9tWt1OzpnFDsPJCJJ0oLQ7yWmD3Qt3wvcALxw3qORJC0Y/T7FdMigA5EkLSz9XmL6jzNtr6oP9mh3Cp35p2+pqgOasl2BM+lMX3oD8MKq+llL22O5/12L/1pVp/UTqyRpfvT7otw48Bo6g/TtBbwa2J/OfYiZ7kWcChw2rew44Lyq2hc4r1l/gCaJHA88DTgQOD7JLn3GKkmaB7OZMOipVXUnQJITgM9W1Z/N1Kiqzk+yclrxEXTmqgY4DfgW8JfT6jwH2FBVtzXH20An0ZzRZ7ySpG3Ub4J4LHBP1/o9dC4RzcUeVXUTQFXdlKRtVNi9gJ90rW9qyn5NkrXAWoCxsTEmJyfnFNTq1Vtm3Waux5KkxaDfBHE6cGGSs+i8Uf184JMDiwrSUtY6emxVrQPWAYyPj9eqVavmdMATTrhg1m3e+ta5HUuSFoN+X5R7L/By4GfAz4GXV9VfzfGYNycZA2h+3tJSZxOwT9f63sDmOR5PkjQH/d6kBngocEdVfQjYlOTxczzm2cCxzfKxwBdb6pwDrEmyS3Nzek1TJkkakn6nHD2ezo3ktzVFOwD/q492ZwDfBfZLsinJK4H3AYcmuRY4tFknyXiSjwM0N6ffA1zUfN49dcNakjQc/d6DeD7wFOBigKranGSrQ21U1TE9Nj27pe4E8Gdd66cAp/QZnyRpnvV7iemeqiqaG8VJdhpcSJKkhaDfBPG/k3wUeGSSPwe+jpMHSdKS1u9YTP+9mYv6DmA/4F1VtWGgkUmSRmqrCSLJCuCcqvojwKQgScvEVi8xVdV9wC+SPGII8UiSFoh+n2L6F+DyZkyku6YKq+oNA4lKkjRy/SaIrzQfSdIyMWOCSPLYqvpn52KQpOVna/cg1k8tJPn8gGORJC0gW0sQ3aOqPmGQgUiSFpatJYjqsSxJWuK2dpP6yUnuoHMm8ZBmmWa9qurhA41OkjQyMyaIqloxrEAkSQvLbOaDkCQtI/2+B6EBOfLI2U91un79QQOIRJIeyDMISVKroSeIJPslmez63JHkTdPqHJzk9q467xp2nJK03A39ElNVXQOsgn8bKfZG4KyWqv9QVc8bZmySpPuN+hLTs4EfVdWPRxyHJGmaUd+kPho4o8e2ZyS5FNgMvKWqrmyrlGQtsBZgbGyMycnJOQWyevWWWbeZ67EWwnElaWvSmWp6BAdOHkTnP/8nVtXN07Y9HPhVVW1Jcjjwoarad2v7HB8fr4mJiTnFM6qniXyKSdIoJdlYVeNt20Z5iem5wMXTkwNAVd1RVVua5a8COyTZbdgBStJyNsoEcQw9Li8leUySNMsH0onzp0OMTZKWvZHcg0jyUOBQ4FVdZa8GqKqTgKOA1yS5F7gbOLpGdS1MkpapkSSIqvoF8KhpZSd1LZ8InDjsuObK+wiSlqJRP+YqSVqgTBCSpFYmCElSKxOEJKmVCUKS1MoEIUlqZYKQJLUyQUiSWpkgJEmtTBCSpFYmCElSKxOEJKmVCUKS1MoEIUlqZYKQJLUaWYJIckOSy5NMJvm1iaTT8eEk1yW5LMlTRxGnJC1XI5kwqMshVXVrj23PBfZtPk8D/r75KUkagoV8iekI4JPV8T3gkUnGRh2UJC0XozyDKODcJAV8tKrWTdu+F/CTrvVNTdlN3ZWSrAXWAoyNjTE5OTmnYFav3jLrNlPHGlVbgHPP/b+zbr9mzWNm3UbS8pOqGs2Bkz2ranOS3YENwF9U1fld278C/HVVXdCsnwe8tao29trn+Ph4TUz82u2MvmzLvNKjajsf7SUtb0k2VtV427aRXWKqqs3Nz1uAs4ADp1XZBOzTtb43sHk40UmSRpIgkuyUZOepZWANcMW0amcDL22eZno6cHtV3YQkaShGdQ9iD+CsJFMxfLqqvpbk1QBVdRLwVeBw4DrgF8DLRxSrJC1LI0kQVXU98OSW8pO6lgt43TDjkiTdbyE/5ipJGiEThCSplQlCktTKBCFJamWCkCS1MkFIklqZICRJrUwQkqRWJghJUisThCSplQlCktTKBCFJamWCkCS1MkFIklqZICRJrUwQkqRWQ08QSfZJ8s0kVyW5MskbW+ocnOT2JJPN513DjlOSlrtRzCh3L/Dmqrq4mZd6Y5INVfWDafX+oaqeN4L4JEmM4Ayiqm6qqoub5TuBq4C9hh2HJGlmI5mTekqSlcBTgO+3bH5GkkuBzcBbqurKHvtYC6wFGBsbY3Jyck6xrF69ZdZtpo41qrbz0V6SeklVjebAycOAbwPvraovTNv2cOBXVbUlyeHAh6pq363tc3x8vCYmJuYUz5FHXjDrNuvXHzTStvPRXtLylmRjVY23bRvJU0xJdgA+D3xqenIAqKo7qmpLs/xVYIckuw05TEla1kbxFFOAk4GrquqDPeo8pqlHkgPpxPnT4UUpSRrFPYhnAi8BLk8ydTH87cBjAarqJOAo4DVJ7gXuBo6uUV0Lk6RlaugJoqouALKVOicCJw4nIs3Fcr33sVz7reXJN6klSa1MEJKkViYISVIrE4QkqZUJQpLUygQhSWplgpAktTJBSJJamSAkSa1GOty3NBejHD1XGrSF9HfUMwhJUisThCSplQlCktTKBCFJamWCkCS1MkFIklqNak7qw5Jck+S6JMe1bH9wkjOb7d9PsnL4UUrS8jaKOalXAH8HPBfYHzgmyf7Tqr0S+FlV/Sbwt8DfDDdKSdIoziAOBK6rquur6h7gM8AR0+ocAZzWLH8OeHaSGacplSTNr1G8Sb0X8JOu9U3A03rVqap7k9wOPAq4dfrOkqwF1jarW5Jc00cMu7Xta7a2JWUNqW1rP0eVagd43K1+n6P6ruajfZd5+Xu7CNjPWdrGv2OP67VhFAmirSs1hzqdwqp1wLpZBZBMVNX4bNosRvZzabGfS8ti6OcoLjFtAvbpWt8b2NyrTpLtgUcAtw0lOkkSMJoEcRGwb5LHJ3kQcDRw9rQ6ZwPHNstHAd+oqtYzCEnSYAz9ElNzT+H1wDnACuCUqroyybuBiao6GzgZOD3JdXTOHI6e5zBmdUlqEbOfS4v9XFoWfD/jL+aSpDa+SS1JamWCkCS1WvQJYluG7Ujytqb8miTP6XefozCgft6Q5PIkk0kmhtOTmc21n0keleSbSbYkOXFam99t+nldkg8vhJcuB9TPbzX7nGw+uw+nN71tQz8PTbKx+d42JnlWV5ul9H3O1M/Rf59VtWg/dG5y/wh4AvAg4FJg/2l1Xguc1CwfDZzZLO/f1H8w8PhmPyv62edS6Gez7QZgt1F/j/PUz52Ag4BXAydOa3Mh8Aw679f8H+C5S7Sf3wLGR/09zlM/nwLs2SwfANy4RL/Pmfo58u9zsZ9BbMuwHUcAn6mqX1bVPwHXNfvrZ5/DNoh+LkRz7mdV3VVVFwD/0l05yRjw8Kr6bnX+1X0SOHKgvdi6ee/nArUt/bykqqbej7oS2LH5LXypfZ+t/RxK1H1Y7AmibdiOvXrVqap7galhO3q17WefwzaIfkLn7fRzm1PbtYzetvRzpn1u2so+h20Q/ZzyieZyxH9eAJde5quffwpcUlW/ZGl/n939nDLS73MUQ23Mp20ZtqNXeVvSHPWzwIPoJ8Azq2pzc21zQ5Krq+r8bYhzW83rMCxzrD8Mg+gnwIur6sYkOwOfB15C5zfsUdnmfiZ5Ip3RnNfMYp/DNoh+wgL4Phf7GcS2DNvRq20/+xy2QfSTqVPbqroFOIvRX3oaxDAsm5r9zLTPYRvIcDNVdWPz807g0yzy7zPJ3nT+Xr60qn7UVX9JfZ89+rkgvs/FniC2ZdiOs4Gjm+uajwf2pXPzq599Dtu89zPJTs1vJiTZic5vLlcMoS8zmfdhWKrqJuDOJE9vTtFfCnxx/kOflXnvZ5Ltk+zWLO8API9F/H0meSTwFeBtVfWPU5WX2vfZq58L5vsc5R3y+fgAhwM/pPMUwTuasncDf9Is7wh8ls7N2QuBJ3S1fUfT7hq6noRo2+eoP/PdTzpPXFzafK5cIv28gc5vZVvo/Ma2f1M+Tucf14+AE2lGEFhK/aTzdNNG4LLm+/wQzdNqi7GfwDuBu4DJrs/uS+377NXPhfJ9OtSGJKnVYr/EJEkaEBOEJKmVCUKS1MoEIUlqZYKQJLUyQWhBS3JfM9TAFUm+1Dw3Psp43j6P+3pkktd2re+Z5HPztX9pW/mYqxa0JFuq6mHN8mnAD6vqvQshnmnlofPv6Vez2NdK4MtVdcD8Rdi/JNtXZ1yg1vV+22np8gxCi8l36RoELcl/SnJRksuS/Jeu8pc2ZZcmOb0pe1yS85ry85I8tik/NZ05Bb6T5PokRzXlY0nO7zp7+f0k7wMe0pR9KsnKJFcl+QhwMbBPki1dcRyV5NRmeY8kZzUxXZpkNfA+4Dea/b2/2d8VTf0dk3winXkCLklySFP+siRfSPK1JNcm+W9tf1DpzJnw7XQGYjwnnVFQp+YY+Ksk3wbe2PT/g0m+CfxNkl2TrG/+nL6X5ElNuxOSrEtyLqMd30nDNOo3EP34mekDbGl+rqDzJuphzfoaOpO+h84vOl8G/gB4Ip03xndr6u3a/PwScGyz/ApgfbN8arPf7ei8kXxdU/5m7n8jdgWwc3c8zfJK4FfA06fH2ywfBZzaLJ8JvKlrf49o2l8xbX9XdB3/E83ybwP/TOdt3JcB1zftdwR+DOwz7c9sB+A7wKOb9RcBpzTL3wI+0lX31ObPbmqOkP8JHN8sPwuYbJZPoPNm70NG/XfCz/A+i300Vy19D0kySec/z43AhqZ8TfO5pFl/GJ1xpp4MfK6qbgWoqqkB7p4BvKBZPh3o/s17fXUuDf0gyR5N2UXAKc04OOurarJHfD+uqu/10Y9n0Rk3iKq6D7g9yS4z1D+Izn/WVNXVSX4M/Faz7byquh0gyQ+Ax/HA4ab3ozP5zIbOlS9WADd1bT9z2rE+28Q0ddw/bY77jXRmsHtEs+3sqrq7j75qifASkxa6u6tqFZ3/BB8EvK4pD/DXVbWq+fxmVZ3clPdzY627Tvf4+wGozrDnfwDcCJye5KU99nPXDPvdsY84eplp7P/ueO/j14ftD3Bl15/Nv6uq7mGkp8d817S201VLPS0DJggtCs1vzG8A3tL8Vn8O8IokUzew90pnXovzgBcmeVRTvmuzi+/QGWUT4MXABTMdL8njgFuq6mPAycBTm03/2hy/l5uT/E6S7YDnd5WfB7ym2feKJA8H7gR27rGf85s4SfJbwGPpXDrrxzXAo5M8o2m/QzrzDfSj+7gHA7dW1R19ttUSY4LQolFVl9AZffboqjqXzhj5301yOZ1pHHeuqiuB9wLfTnIp8MGm+RuAlye5jM7EK2/cyuEOBiaTXELnksuHmvJ1wGVJPtWj3XF0rul/gwde1nkjcEgT60bgiVX1U+Afm5vg75+2n48AK5r6ZwIvqwfONNZTdaa9PIrOTedL6YwQurqftnTuNYw3f07v4/4hqrUM+ZirJKmVZxCSpFYmCElSKxOEJKmVCUKS1MoEIUlqZYKQJLUyQUiSWv1/6eQO5/9asrIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=plot_diff_random, bins=20, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('My Very Own Histogram')\n",
    "# plt.text(23, 45, r'$mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=22)\n",
    "plt.savefig('./plots/error_histo_random_t600.png',dpi=300, bbox_inches = \"tight\")\n",
    "# plt.savefig('./plots/error_histo_random.eps', format='eps',dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
